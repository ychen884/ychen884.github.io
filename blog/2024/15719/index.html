<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Advanced Cloud Computing Notes - Currently Upating... | Yizhou Chen</title> <meta name="author" content="Yizhou Chen"> <meta name="description" content="Advanced Cloud Computing Notes - SCS"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ychen884.github.io/blog/2024/15719/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yizhou </span>Chen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Advanced Cloud Computing Notes - Currently Upating...</h1> <p class="post-meta">January 16, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/cmu"> <i class="fas fa-hashtag fa-sm"></i> CMU</a>     ·   <a href="/blog/category/study"> <i class="fas fa-tag fa-sm"></i> Study</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 1: Overview of Cloud Computing</a></li> <li><a href="#topic-2">Chapter 2: Building a Cloud</a></li> <li><a href="#topic-3">Chapter 3: Encapsulation</a></li> <li><a href="#topic-4">Chapter 4: Programming Models and Frameworks</a></li> <li><a href="#topic-5">Chapter 5: </a></li> <li><a href="#topic-6">Chapter 6: </a></li> <li><a href="#topic-7">Chapter 7: </a></li> </ol> <h2 id="-chapter-1-overview-of-cloud-computing--">** Chapter 1: Overview of Cloud Computing ** <a name="topic-1"></a> </h2> <h4 id="definitions">Definitions</h4> <h5 id="properties">Properties:</h5> <ul> <li>Computing utility, always available, accessible through the networks</li> <li>Simplified interface</li> <li>Statistical multiplexing, sharing resources</li> <li>Economies of scale from consolidation, costs lower</li> <li>Capital costs converted to operating costs</li> <li>Rapid and easy variation of usage</li> <li>Appearance of infinite resources with small users</li> <li>Pay only for what you use</li> <li>Cost conservation: 1 unit for 1000 hours == 1000 units for 1 hour</li> </ul> <h5 id="consolidation-sharing-elasticity">Consolidation, sharing, elasticity</h5> <ul> <li>CLT theory</li> <li>users with widely varying needs apply a considerably less variable load on a huge provider, allowing providers to do less overprovisioning.</li> <li> <ul> <li>Because of CLT, it is predictable for the overall load which causes less overprovisioning.</li> </ul> </li> <li>Users perceive exactly what they need all the time, if their needs are “small”(so the accessed resources are appearing as infinite)</li> </ul> <h5 id="saas-paas-iaas">SaaS, PaaS, IaaS</h5> <ul> <li>SaaS: service as application (Salesforce)</li> <li> <ul> <li>consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage &amp; minimum deployed applications configurations settings.</li> </ul> </li> <li>PaaS: high-level programming model for cloud computer, Turing complete but resource management hidden. (Google AppEngine)</li> <li> <ul> <li>Only App and data are controlled by user</li> </ul> </li> <li> <ul> <li>consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage but with control over the deployed applications and possibly configuration settings for the application-hosting environment.</li> </ul> </li> <li>IaaS: low-level computing model for cloud computer (AWS)</li> <li> <ul> <li>The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed application</li> </ul> </li> <li> <ul> <li>Can manage App, Data, Runtime, Middleware, OS, Cannot manage Virtualization, servers, storage, networking</li> </ul> </li> </ul> <h5 id="xxx-as-a-service">XXX as a Service</h5> <ul> <li>Data as a Service, Network as a Service, Communication as a Service(No hardware private VOIP switching), IT as a Service(IT providing services)..</li> </ul> <h5 id="deployment-models">Deployment models</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>public cloud: provider sells computing to unrelated consumers
private cloud: largely unrelated components and divisions as consumers
community cloud: providers and consumers are different organizations with strong shared concerns
Hybrid cloud: private plus public resources combined by same consumer. Better availability, overflow from private to public, load balancing to increase elasticity
</code></pre></div></div> <h5 id="larry-ellisons-objection">Larry Ellison’s objection</h5> <ul> <li>definition is including too much </li> </ul> <h5 id="obstacles-of-cloud-computing">Obstacles of cloud computing</h5> <ul> <li>Privacy &amp; security</li> <li>Privacy in the world tends to rely on regulation</li> <li>Utility issues</li> <li>Physical utilities tend to rely on regulation</li> <li>High cost of networking combined with always remote</li> <li>Performance unpredictability &amp; in situ development/debugging</li> <li>Software licensing – $/yr/CPU is not elastic and pay as you go</li> </ul> <h5 id="load-balancing-approach">load balancing approach</h5> <h5 id="1-dns-load-balancing">1. DNS load balancing</h5> <ul> <li>DNS reorder the list for each client asking for translation of name</li> <li>PRO: easy to scale, unrelated to actual TCP/HTTP requests</li> <li>CON: new server may get less resources if scheduling more servers, dynamic changing is hard because it has to tell client for a binding(existing binding exists due to TTL, caching in network middleboxes).</li> </ul> <h5 id="2-router-distribute-tcp-connections">2. Router distribute TCP connections</h5> <ul> <li>Router do the mapping: (client_ip+client_port) &lt;-&gt; (server_ip+server_port)</li> <li> <ul> <li>for SYN packets</li> </ul> </li> <li> <ul> <li>not exposed to client about the ip address(like NAT)</li> </ul> </li> <li>PRO: router doesn’t have to think or remember too much, *it just selected the server with least connnection to schedule for the new connections.</li> <li>CON: traffic all go through the router(more difficult to scale), and decision takes time cuz it’s for the entire session</li> </ul> <h4 id="3-router-distribute-individual-quests-embedded-in-connections">3. Router distribute individual quests embedded in connections</h4> <ul> <li>PRO: most dynamic</li> <li>CON: requires the most processing and state in the router, CPU load and memory goes up due to intelligent routing decisions. <h4 id="elasticit-how-elasticity-controller">Elasticit: How? Elasticity controller</h4> </li> <li>Elasticity controller to adjust load capability based on current load status</li> <li>Monitoring: resource usage, request sequence(patterns)</li> <li>Triggering: (simple conditions like thresholds), schedule, complex model based on monitored instances</li> </ul> <h4 id="elasticity-scale-out-or-scale-up">Elasticity: Scale-out or Scale up</h4> <ul> <li>Horizontal scaling: adding more instances (Common approach)</li> <li>Vertical scaling: Resizing the resources(bdw, cpu cores, memory) allocated to an existing instances, challenging(different OS.) More challenging.</li> </ul> <h4 id="two-tier-services">Two-tier services</h4> <ul> <li>web-database</li> <li>web server easy to be in cloud. At beginning, order-taking is not in cloud.</li> <li>Elasticity in IaaS: database scaling is more difficult with state(consistency)</li> <li>PassS, P=Web Service. Built-in elastic load balancing and scheduled actions for containers, persistent key-value store (datastore) &amp; non-persistent memcache for simple database tier, Users can instantiate Backends, user code can request (actuate) horizontal scaling, running traditional database services, whose scaling is still hard.</li> </ul> <h5 id="load-balancing-method-affect-how-much-statistics-we-can-get">Load-balancing method affect how much statistics we can get.</h5> <ul> <li>Router-based load balancing: firewall, intrusion detection, accelerator</li> <li>scaling middleboxes: CPU intensive tasks. (OpenFlow, split flows)</li> <li>bdw allocation by sw/rt</li> </ul> <h5 id="service-parallelization-load-balancer">Service parallelization: Load Balancer</h5> <ul> <li>aws cloud watch</li> <li>Load balancer is not necessarily elastic</li> </ul> <h5 id="scalable-relational-database">Scalable relational database</h5> <ul> <li>Separate data at rest(distributed pay-for-use storage (HDFS)) from ongoing or recent access &amp; mutation</li> <li>Recent access &amp; mutation servers are elastic (called Owning Transaction Managers)</li> <li>Partitioned but all transactions restricted to one partition: transactions block on locks and bottleneck performance scaling</li> <li>Fault-tolerance of Elastic controller. Controller itself, reliability provided by replication. Can re-assigns partitions while server is down/start up.</li> </ul> <h5 id="elastras-architecture-scales-otm-machines">ElasTraS architecture scales OTM machines</h5> <ul> <li>Transactions are limited to interacting with data from only one partition to avoid the complexity of distributed transactions.</li> <li>TODO</li> </ul> <h3 id="paper-reading-notes">Paper reading notes</h3> <h5 id="armbrust2010">Armbrust2010</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Referring to http://doi.acm.org/10.1145/1721654.1721672
Cloud computing: what brings it? large capital outlays, overprovisioning/underprovisioning...

Definition: Refers to both the applications as services over the internet and hardware and systems software in the data center that provide those services. The services themselves: SaaS. Services being sold is utility computing. Cloud computing = SaaS + utility computing. It has to be large enough to be called cloud. Hardware provisioning/pricing: 1. inifinite computing resources available on demand; 3. elimination of an up-front commitment by cloud users, add resources when needed; 4. pay for use of computing resources temporarily. *Construction and operaition of extremely large-scale, commodify-computer data centers at low-cost locations was the key necessary enabler of cloud computing. It could offer services below the costs of a medium-sized data center and make profits.

Utility Computing classes: EC2 with low level control but less automatic scalability and failover(application may need to control the replication...). Google AppEngine(domain specific platforms) on the other hand. Azure is in between. 

Economics: Favor cloud computing over conventional: 1. demand of services changes over time 2. demand is unknown
usage based pricing economically benefits the buyer. 
Elasticity helps reduce the costs. Underprovisioning has a cost that is difficult to measure: the users may never come back. Scale-up elasticity is an operational requirement, and scale-down elasticity allowed the steady state expenditure to more closely match the steady-state workload. 


Obstacles for Cloud computing:

1-3(adoption):
1. Business Continuity and Service Availability (hard to ensure availability, single failure still exists for a service provider)
2. Data Lock-In (public+private sharing by sharing API)
3. Data Confidentiality/Auditability: from other user/provider 

4-8(growth):
4. Data Transfer Bottlenecks： Applications continue to become more data-intensive
5. Performance Unpredictability： I/O interference between virtual machines, concerns scheduling of virtual machines for some classes of batch processing programs, specifically for highperformance computing
6.Scalable Storage
7. Bugs in Large Scale Distributed Systems: bugs cannot be reproduced in smaller configurations
8. Scaling Quickly

9-10(policy and business):
9. Reputation Fate Sharing, legal liability(customer responsible-&gt;unexpected down)
10. Software Licensing



Opportunities: 
- improve architectures and operating systems to efficiently virtualize interrupts and I/O channels
- flash memory will decrease I/O interference.
- offer something like “gang scheduling” for cloud computing
- create a storage system that would not only meet existing programmer expectations,but combine them with the cloud advantages of scaling arbitrarily up and down on demand. 
- reliance on virtual machines in cloud computing.(7)
- automatically scale quickly up and down in response to load in order to save money
- create reputation-guarding services similar to the “trusted email” services 
</code></pre></div></div> <h5 id="referring-to-nistdef2011">Referring to NISTdef2011</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Characteristics:
- On-demand self-service
- Broad network access
- Rapid elasticity
- Resource pooling (multi-tenant)
</code></pre></div></div> <h5 id="vaquero11">Vaquero11</h5> <h5 id="dynamically-scaling-applications-in-the-cloud">Dynamically Scaling Applications in the Cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vertical scaling is hard: rebooting..
load balancers need to support

Server scalability: a per-tier controller, a single controller for the whole application (for all tiers)
How and When to Scale: Feeding Controller with Rules and Policies
- adding load balancers
- LB scalability requires the total time taken to forward each request to the corresponding server to be negligible for small loads and should grow no faster than O(p) 
-  CPU-intensive web app: a LB to split computation among many instances.
-  network intensive: CPU-powerful standalone instance to maximize throughput
-  more network intensive applications, it may be necessary to use DNS load balancing

Scaling the network:
- Virtualization: VLAN(L2)
-  periodically measure actual network usage per application use other applications’ - increase the utilization of the network by virtually “slicing”: “network as a service”
- statistical multiplexing


Scaling the platform:
PaaS cloud platforms
- Container Scalability: execution environment for user applications.scalability of the container layer is crucial as it must efficiently manage and distribute resources to meet the demands of potentially numerous applications running concurrently.

Multitenant containers and Isolation: requires strong isolation to prevent security issues
Individual Containers per User: simplifies isolation but requires effective management of numerous containers.
Horizontal Scaling via Container Replication: achieved through automatic scaling in IaaS systems or by inherent capabilities of the platform itself
Components should ideally be stateless to cope with the dynamic nature of container instantiation and disposal, support for stateful components is also necessary, requiring sophisticated load balancing (LB) and container management to handle session data.(soft state replication, distributed caching systems)

- Database Scalability: for example, implementing horizontal scaling strategies, such as replicating the database across multiple nodes to handle increased load and ensure data availability. (demands on PaaS platforms can often surpass the capacity of any single machine)
NoSQL Databases: These databases provide high scalability and availability, fitting well in cloud environments with high demand. However, they offer eventual consistency rather than immediate consistency, leading to limitations in transaction support and SQL functionalities.
Replication Mechanisms: In-core Solutions, Middleware Solutions

Ideal Elastic Cloud: scale through VM or container replication, reconfiguration, and dynamic load balancing, possibly using DNS for the latter. allow dynamic allocation of network resources. 
Ideal PaaS Platform Features: capable of instantiating or releasing instances of user components based on demand changes and distributing the load transparently among them. Implementing session concepts and supporting transparent data replication are essential. Access to traditional relational databases with ACID transaction support is crucial. However, the system must address the increased latency due to consistency maintenance across replicas, especially under high demand.
</code></pre></div></div> <ul> <li>Infrastructure-as-Code tools provide a high-level software interface (e.g., Python / Ruby or JSON / YAML) that allows developers to specify their infrastructure requirements, software dependencies, and the process for building the infrastructure and deploying it to the cloud</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="-chapter-2-building-a-cmu-cloud--">** Chapter 2: Building a CMU Cloud ** <a name="topic-2"></a> </h2> <h4 id="model">Model</h4> <h5 id="aim-for-less-costs">aim for less costs</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Client -&gt;
App
OS
Instance(s)
--------------------
Hypervisor (shared)
Building Blocks
Usage Monitor
Hardware (shared)
(shared Amazon EC2)

</code></pre></div></div> <h5 id="problem-used-mostly-during-ddl-2-methods-renting">Problem: used mostly during ddl? 2 methods, renting..</h5> <h5 id="build-a-cloud">Build a cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. user would rent, so provisioner(resources,monitoring,... -&gt; assignment of users to machines)
- Bin-packing, NP-hard. Can do with assumptions.. -&gt; less complexity
- Migration, also costs
2. Scheduler: which user jobs/processes to run: Prioritization(pay more), Oversubscription, Workload constraints
3. Encapsulation: compute, storage, networking and data
4. Virtualization
5. Fault tolerance: 
    why scale of data center matters? Economy of scale, cost of operations
    state replication, logging, storage replic

Storage services - Scalable, fault tolerant
Tools - Programming models, frameworks
Automation - Reactive systems &amp; elastic scaling

Elastic scaling - based on monitoring and diagnosis
traditional: provision for peaks

</code></pre></div></div> <ul> <li>Cloud users &amp; Services</li> <li> <ul> <li>App user: availability, performance, no interfaces exposed by cloud service provider (app provider did that)</li> </ul> </li> <li> <ul> <li>Application Deployment User: Dashboard, management interfaces exposed</li> </ul> </li> <li> <ul> <li>Admin: Management…</li> </ul> </li> <li>Orchestration: automatic deployment for user</li> </ul> <h5 id="openstack">OpenStack</h5> <ul> <li>independent parts,6 core services</li> <li>communicate through public and well-defined APIs</li> <li>Identity -&gt; Dashboard -&gt; Compute(?)/Network -&gt; Image -&gt; Object storage(get the image for VM) -&gt; Block Storage(volume) -&gt; initiate VM</li> <li>Distributed storage to accelerate image initialization takes Azure many years to fix</li> </ul> <h5 id="referring-to-sotomayor2009">Referring to sotomayor2009</h5> <h5 id="virtual-infrastructure-management-in-private--hybrid-clouds">Virtual Infrastructure Management in Private &amp; Hybrid Clouds</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Virtual Infrastructure (VI) management in the context of private/hybrid cloud environments: Uniform Resource View, Full Lifecycle Management of VMs, Configurable Resource Allocation Policies, Adaptability to Changing Resource Needs. provides primitives to schedule and manage VMs across multiple physical hosts


OpenNebula: This is a VI manager that allows organizations to deploy and manage VMs, either individually or in groups. It automates the VM setup process (including preparing disk images and setting up networking) and is compatible with various virtualization layers (Xen, KVM, VMware) and external clouds (EC2, ElasticHosts).

Haizea: This acts as a lease manager and can serve as a scheduling backend for OpenNebula. It introduces leasing capabilities not present in other cloud systems, such as advance reservations and resource preemption, which are particularly valuable for private cloud environments.



Traditional VI Management Tools: lack certain features necessary for building IaaS clouds, such as public cloud-like interfaces and the ability to deploy VMs on external clouds.
Cloud toolkits can help transform existing infrastructure into an IaaS cloud with cloudlike interfaces. VI management capabilities are not as robust.
    -: Gap: There's a noticeable gap between cloud management and VI management. Cloud toolkits attempt to cover both areas but often fall short in delivering comprehensive VI management functionalities. Integrating cloud management solutions with VI managers is complex due to the lack of open, standard interfaces and certain key features in existing VI managers.
    
    OpenNebula: overcome these challenges, Scaling to external clouds. A flexible and open architecture for easy extension and integration with other software. A variety of placement policies and support for scheduling, deploying, and configuring groups of VMs.
    
    Integration of Haizea with OpenNebula: work as a scheduler for OpenNebula, This integration allows OpenNebula to offer resource leases as a fundamental provisioning abstraction, and Haizea to operate with real hardware through OpenNebula. This combination provides advanced features like advance reservation of capacity, which is not offered by other VI managers.

    The integration is particularly beneficial for private clouds with limited resources, enabling sophisticated VM placement strategies that support queues, priorities, and advance reservations (ARs).

OpenNebula: 
core:  image and storage technologies for preparing disk images for VMs, network fabric (such as Dynamic Host Configuration Protocol [DHCP] servers, firewalls, or switches) for providing VMs with a virtual network environment, hypervisors for creating and controlling VMs. Performs operations through pluggable drivers. 
A separate scheduler component makes VM placement decisions.
Management interfaces: libvirt API intergrate within other data center management tools, cloud interface exposed to external users.
Cloud drivers to interface with external clouds

The Haizea Lease Manager: Haizea is an open source resource lease manager and can act as a VM scheduler for OpenNebula.
Advance Reservation (AR) Leases: Resources are guaranteed to be available at a specific future time. This is beneficial for scenarios requiring resource certainty.
Best-Effort Leases: Resources are provisioned as soon as possible, with requests queued if immediate provisioning isn't possible.
Immediate Leases: Resources are provided immediately upon request or not at all, suitable for urgent needs without flexibility.
    
Haizea addresses the challenge of resource underutilization, often a downside of AR, by leveraging VMs. It allows for efficient support of ARs through resource preemption - suspending lower-priority VMs to free up resources for higher-priority needs and resuming them later.
It uses optimizations such as reusing disk images across leases to minimize the impact of preparation overhead and schedules runtime overheads (like suspending, resuming, and migrating VMs) efficiently.
Scheduling is based on a resource slot table, representing all physical nodes managed by Haizea over time.
Best-effort leases are managed using a first-come-first-serve queue with backfilling, optimizing queue-based systems.
AR leases utilize a greedy algorithm for selecting physical resources to minimize preemptions.

There are ongoing efforts to extend OpenNebula's capabilities, including the implementation of the libvirt interface, VM consolidation schedulers for minimizing energy consumption, and tools for service elasticity management, VM placement, public cloud interface support, and policy-driven dynamic placement optimization.

Haizea shows that VM-based approaches with suspend/resume capabilities can address utilization issues typically associated with advance reservation (AR) use.

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h5 id="xen-and-the-art-of-virtualization">Xen and the Art of Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Xen, a high-performance, resource-managed VMM that effectively allows multiple commodity operating systems to share hardware resources safely and efficiently

Advantages: isolation of virtual machines to prevent adverse performance effects, accommodation of diverse operating systems, and minimization of performance overhead introduced by virtualization

- targeting existing application binary interfaces (ABIs) and supporting the virtualization of complex server configurations within a single guest OS instance. full multi-application operating systems, and the necessity of paravirtualization for high performance on complex architectures like x86.

 resource virtualization: ensures both correctness and performance by allowing guest operating systems visibility of real as well as virtual resources.  better support for time-sensitive tasks and optimal use of physical resources like superpages or page coloring. 

Memory Management:
Segmentation and Paging: Guest OSes are responsible for managing hardware page tables with minimal Xen involvement for safety. Xen resides in a 64MB section at the top of every address space to avoid TLB flush when switching to and from the hypervisor. Segmentation is virtualized by validating updates to hardware segment descriptor tables.
Guest OS Restrictions: Guest OSes cannot install fully-privileged segment descriptors and must not overlap with the top end of the linear address space. They have direct read access to hardware page tables but updates are batched and validated by Xen.


CPU:
Privilege Levels: Guest OSes run at a lower privilege level than Xen to protect the hypervisor from OS misbehavior and ensure domain isolation. x86's four privilege levels are leveraged, with guest OSes typically moved from ring 0 to ring 1.
Exceptions and System Calls: Guest OSes must register descriptor tables for exception handlers with Xen. They may install a ‘fast’ handler for system calls to avoid indirecting through Xen for every call. Page faults and system calls are the most performance-critical exceptions.

Device I/O:
Virtual Devices: Instead of emulating hardware devices, Xen introduces efficient and simple device abstractions. Data transfer is done using shared-memory, asynchronous buffer-descriptor rings, allowing Xen to perform validation checks.
Event System: Hardware interrupts are replaced with a lightweight event system. Xen supports a mechanism for sending asynchronous notifications to a domain, similar to hardware interrupts, but with more control and efficiency.

Porting Operating Systems to Xen: Code Modifications in architecture-independent sections, virtual network drivers, block-device drivers, and Xen-specific (non-driver) areas. Both operating systems required substantial alterations in their architecture-specific sections. 

Role of Domain0: A domain created at boot time, termed Domain0, is responsible for hosting application-level management software. It uses the control interface to manage the system, including creating and terminating other domains and controlling their resources.

Control Transfer: Hypercalls and Events:

Hypercalls: Synchronous calls from a domain to Xen, analogous to system calls in conventional OSes. Used for operations like requesting page-table updates.
Event Mechanism: Asynchronous notifications from Xen to domains, replacing traditional interrupt delivery mechanisms. Events are used for lightweight notifications like domain-termination requests or indicating that new data has been received over the network.
Data Transfer: I/O Rings:

Efficient data transfer mechanism that allows data to move vertically through the system with minimal overhead. The design focuses on resource management and event notification.
I/O rings are circular queues of descriptors allowing asynchronous, out-of-band data transfer between Xen and guest OSes. They support a variety of device paradigms and enable efficient zero-copy transfer.

Subsystem Virtualization:
CPU Scheduling: Xen employs the Borrowed Virtual Time (BVT) scheduling algorithm for domain scheduling, ensuring low-latency wake-up and fair resource sharing.
Time and Timers: Xen provides notions of real time, virtual time, and wall-clock time to guest OSes. Domains can program alarm timers for real and virtual time, with timeouts delivered using Xen's event mechanism.

Virtual Address Translation: Xen avoids the overhead of shadow page tables by allowing guest OSes to manage hardware page tables directly, with Xen involved only in updates validation. This approach minimizes the number of hypercalls required for page table management.

Physical Memory: Memory is statically partitioned between domains, providing strong isolation. Guest OSes can adjust their memory usage dynamically, with mechanisms like the balloon driver in XenoLinux facilitating this interaction.

Network: Xen introduces the concept of a virtual firewall-router (VFR), with each domain having virtual network interfaces (VIFs) attached to the VFR. Domains transmit packets via enqueueing buffer descriptors on I/O rings, and Xen handles packet reception efficiently by exchanging packets directly with page frames.
Disk: Only Domain0 has direct access to physical disks. Other domains access persistent storage through virtual block devices (VBDs). Disk requests are batched and serviced in a round-robin fashion by Xen, ensuring fair access and good throughput.

Advantages of Delegating Domain Construction to Domain0:
Reduced Hypervisor Complexity: Building a domain within Domain0 rather than entirely within Xen simplifies the hypervisor's design, focusing its functionality on core tasks and leaving the domain setup process to a more specialized and capable component.

Improved Robustness: The process of setting up a new domain involves numerous delicate operations. Performing these operations within Domain0 allows for better error checking and handling.

Metrics:
Performance Comparison with Other Virtualization Technologies
Efficient Data Transfer and Subsystem Virtualization
Performance Isolation
Scalability
Network Performance
Concurrent Virtual Machines
Microbenchmarks 

Key points:
Efficient Paravirtualization
Resource Management and Performance Isolation
Scalability
Minimal Performance Overhead
Generality of the Interface
Facilitation of Network-Centric Services
</code></pre></div></div> <h2 id="-chapter-3-encapsulation--">** Chapter 3: Encapsulation ** <a name="topic-3"></a> </h2> <h5 id="options-available-for-encap">Options available for encap</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Isolation: others can't read/write your data
- - can't impact your performance (DOS)
- Processes may need memory sharing, get resource alloc, has own address space
</code></pre></div></div> <ol> <li>Bare metal <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>+: good isolation, performance, very good software freedom
-: granularity limitation
</code></pre></div> </div> </li> <li> <p>Process: just application</p> </li> <li>Containers: with applicaiton &amp; library/fs/etc that looks like an OS <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>+: performance, decent software freedom
-: security issues
</code></pre></div> </div> </li> <li>Virtual Machines: physical machine-like software container ``` HW &lt;-&gt; VMM &lt;-&gt; (OS, Library/fs/etc/, App) +: isolation properties, good software freedom -: performance overhead, imperfect performance isolation</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Comparison
- Process, Container, VM, BM
- Lower management &lt;-&gt; Better isolation/fidelity


##### interface is the key
- between APP/Library/OS/HW

##### Linux namespaces
- pid, gid, ...
##### resouece alloc: linux groups

##### Limitation of containers
- share the same host OS, security + flexibility(have to work with this OS)


##### Virtualization
- VM: vm OS &lt;--virtual machine interface--&gt; VMM &lt;--machine interface--&gt; HW
- The interfaces can be different

##### HW Virtualization Principles
- Fidelity, software operation keeps identical
- Isolation, guest cannot affect other guest/VMM
- Performance, most operations execute natively

#### Ecap Principle


</code></pre></div></div> <ul> <li>Basic: Execute VM softare in de-privileged mode</li> <li> <ul> <li>prevent privileged instr from escaping containment Privilege Level App User/Ring 3 Lib User/Ring 3 OS Supervisor/Ring-o HW</li> </ul> </li> </ul> <p>Option 1 Privilege Level App User/Ring 3 Lib User/Ring 3 OS User/Ring-3 VMM Supervisor/Ring-0 HW Overhead - Isolation, VMM has to deal with operations from OS.. not ideal</p> <p>Option 2 Privilege Level App User/Ring 3 Lib User/Ring 3 OS Ring-1 VMM Supervisor/Ring-0 HW</p> <p>—-&gt; later Option 3 Privilege Level App User/Ring 3* Lib User/Ring 3* OS Ring-0* (don’t have all power, cannot change fundamental page tables..) VMM Supervisor/Ring-0 HW</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##### Priv Ops


</code></pre></div></div> <ul> <li>Most executions run directly</li> <li>VNM needs to handle OS attemps that execute priv ops, Non-CPU devices..</li> <li> <ul> <li>popf, pushf…</li> </ul> </li> </ul> <p>Handle: Trap &amp; emulate: VMM handles Static software re-writing/paravirtualization: rewrite guest OS to leverage VMM hypercalls (performance, sacrificing transparancy) Dynamic software re-writing: VMM re-write guest’s privileged code - coalescing traps, VMM complex, good performance</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Handling memory: process
</code></pre></div></div> <ul> <li>OS: page mapping from virtual page to physical page</li> <li>User process: typically continuous address space</li> </ul> <p>Mutiple OS: Guest OSes manage: Mapping guest virtual to guest physical VMM manages guest physical to host physical</p> <p>other split: cores, time sharing..</p> <p>SW: Shadow page tables HW: Extended page tables(EPT)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Virtualization Devices

</code></pre></div></div> <ul> <li>Could be all virtualized (trap accesses, and emulate), but worse performance</li> </ul> <p>For performance</p> <ol> <li>Mapping, control given to a host (hw support)</li> <li>Partition, (disk..)</li> <li>Guest enhancement (special VM -&gt; VMM calls)</li> <li>Virtualized-enhanced devices (NICs with VMDq)</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Security issues of virtualization/containerization
- meltdown
- bitflip(row hammer)



[Back to Blog Chapters](#blog-chapters)

##### Web servers
- Online retail stores
- parallel programming: contention, failures, consistency(shared resources), load balancing,..

##### HPC
- scaling for N processes (lower costs, faster computation)
- strong/weak: same process finishes faster on N processors?
- Bulk Synchronous Processing..
- MPI, resource allocators, schedulers, ft
- Very manual, deep learning curve, few commercial runaway successes
##### Grid Computing
- easier to use
- emphasized geographical sharing
- jobs selected from batch queue, take over cluster
- workload diversity
##### Cloud Computing
- low initial cost
- workload diversity +
- may cost efficiency for easier programming

##### Motivation for CC programming framework
- using many commodity* machines
- tolerate failures
- locality(reduce cost of communication), parallelism

##### Batch processing of large datasets on a cluster: framework offers..
- ...
- Job orchestration
- Data staging and movement

##### MapReduce

</code></pre></div></div> <ul> <li>read large data set</li> <li>ind process the input data in chunks</li> <li>shuffle and sort data (by key)</li> <li>reduce(merge)</li> <li>locality optimization for racks</li> <li>shuffle begins after first round of map (master responsible for scheduling this)</li> </ul> <p>Split RR(record reader) -&gt;(K,V) Map -&gt; (K’, V’) -&gt; Partitioner: hash(key) mod R -&gt; reducer Sort (may need to wait for reduce finish) Reuce</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

##### Map data size vs shuffle data size
- ngram(small-&gt;large)

##### Spark

</code></pre></div></div> <p>Less disk operations for iterative apps RDD as memory storage(immutable), rerun instead of storing in persistent storage transformations create new ones, can be lazy action create value, recomputed when action comes</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## ** Chapter 4: Programming Models and Frameworks ** &lt;a name="topic-4"&gt;&lt;/a&gt;



##### Map Reduce
</code></pre></div></div> <p>Core Concepts of MapReduce:</p> <p>The MapReduce model involves two primary functions: the map function and the reduce function. The map function takes key/value pairs as input and produces a set of intermediate key/value pairs. The reduce function then merges all intermediate values associated with the same intermediate key.</p> <p>Automatic Parallelization and Distribution: MapReduce abstracts the complexity of parallelization, fault-tolerance, data distribution, and load balancing</p> <p>Scalability and Ease of Use: The MapReduce implementation can process vast amounts of data (many terabytes) across thousands of machines. It has a user-friendly nature, evidenced by the extensive use within Google, with hundreds of MapReduce programs implemented and thousands of jobs executed daily.</p> <p>Motivation and Design Philosophy: The motivation behind MapReduce stemmed from the need to process large sets of raw data (like web documents, request logs) into computed forms (like inverted indices, summaries) efficiently. MapReduce was designed to simplify the computations by abstracting the complex code required for parallelization, data distribution, and fault tolerance into a library.</p> <p>Programming Model: The computation in MapReduce takes input key/value pairs and produces output key/value pairs, expressed through Map and Reduce functions defined by the user. It handles the grouping of intermediate values by keys and provides these to the Reduce function, which then merges the values per key.</p> <p>Examples and Applications: The abstract and subsequent sections provide practical examples, like counting the number of occurrences of each word in a document collection, showcasing the model’s applicability to real-world tasks. It also highlights the versatility of MapReduce in tasks such as distributed grep, counting URL access frequency, reversing web-link graphs, and computing term-vectors per host. Inverted Index: A fundamental operation in document searching where the map function processes documents, emitting word and document ID pairs, and the reduce function groups these by word, creating a list of document IDs for each word. Distributed Sort: A sorting operation where the map function extracts keys from records and the reduce function sorts these keys, relying on the system’s partitioning and ordering capabilities.</p> <p>Implementation and Execution Overview: designed for large clusters of commodity PCs, considering the typical hardware specifications and the nature of the network and storage systems. The execution process involves dividing the input data into splits, assigning map and reduce tasks to workers, processing the data through the user-defined map and reduce functions, handling intermediate data storage and retrieval, and finally producing the output after all tasks are completed.</p> <p>Fault Tolerance: Fault tolerance is a critical aspect, given the scale of operation and the likelihood of machine failures. The system handles worker failures by reassigning tasks and redoing work if necessary. The master node monitors worker status and orchestrates the reassignment of tasks as needed. The system is designed to ensure that, despite failures, the output is consistent with what would be produced by a faultless, sequential execution, as long as the map and reduce functions are deterministic.</p> <p>Master Data Structures: The master node maintains data structures to keep track of the status of each task and the locations of intermediate data. This information is crucial for coordinating the work of map and reduce workers and for ensuring that data is correctly routed through the system.</p> <p>Locality Network Bandwidth Optimization: importance of conserving network bandwidth, a relatively scarce resource in large computing environments. Data Locality: The approach involves scheduling map tasks on machines that contain a replica of the input data, or at least are network-local to the data. This strategy significantly reduces network bandwidth usage as most data is read locally.</p> <p>Granularity of Map and Reduce Phases: The map phase is subdivided into M pieces, and the reduce phase into R pieces. Ideally, M and R should be much larger than the number of worker machines. Dynamic Load Balancing and Recovery: This setup improves dynamic load balancing and expedites recovery from worker failures, as the tasks a worker has completed can be redistributed across other workers. Handling Stragglers: machines that take unusually long to complete tasks. Causes for stragglers include hardware issues, resource contention, or bugs.</p> <p>Custom Partitioning: While a default partitioning function based on hashing is provided, users have the option to specify a custom partitioning function, allowing more control over how data is partitioned across reduce tasks.</p> <p>Intermediate Key/Value Pair Ordering: The framework ensures that within a partition, intermediate key/value pairs are processed in increasing key order. This ordering guarantee is particularly beneficial when the output needs to be sorted or efficiently accessible by key.</p> <p>Data Aggregation at Mapper Nodes: The Combiner function allows for partial merging of intermediate data on the mapper nodes before sending it over the network. This feature is especially useful when there’s significant repetition in the intermediate keys, and the reduce function is commutative and associative.</p> <p>Flexible Data Formats: The MapReduce library supports various formats for input data, providing flexibility to handle different types of data sources.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

##### Spark

</code></pre></div></div> <p>Spark, a new cluster computing framework designed to address the limitations of MapReduce and its variants in handling certain types of applications, particularly those involving iterative operations and interactive data analysis.</p> <ul> <li>Limitations of MapReduce and Similar Systems: they are primarily built around an acyclic data flow model. This model is not suitable for applications that need to reuse a working set of data across multiple parallel operations.</li> </ul> <p>Iterative Jobs: Common in machine learning, these jobs require applying a function repeatedly to the same dataset. MapReduce’s need to reload data from disk for each iteration causes significant performance penalties.</p> <p>Interactive Analytics: Users often run ad-hoc queries on large datasets using SQL interfaces. With MapReduce, each query incurs high latency because it operates as a separate job and needs to read data from disk.</p> <p>Spark aims to support applications with working sets while retaining the scalability and fault tolerance characteristics of MapReduce. The key abstraction in Spark is the resilient distributed dataset (RDD), a read-only collection of objects partitioned across a set of machines. RDDs can be explicitly cached in memory and reused in multiple parallel operations, significantly improving performance for certain types of applications.</p> <p>Fault Tolerance through Lineage: RDDs are fault-tolerant, using a concept called lineage. If a partition of an RDD is lost, the system has sufficient information on how the RDD was derived from other RDDs to rebuild just the lost partition.</p> <p>Implementation and Usability of Spark: Spark is implemented in Scala, allowing for a high-level, statically typed, and functional programming interface. It can also be used interactively, which is a novel feature for a system of this kind, enabling users to process large datasets on a cluster interactively.</p> <p>Spark can significantly outperform Hadoop in iterative machine learning workloads and provide interactive querying capabilities with sub-second latency for large datasets.</p> <ul> <li>Programming Model Spark introduces two primary abstractions for parallel programming: resilient distributed datasets (RDDs) and parallel operations on these datasets, and shared variables.</li> </ul> <p>RDDs are read-only collections of objects partitioned across machines, capable of being rebuilt if a partition is lost. RDDs can be created directly from a file in a shared filesystem like HDFS, by parallelizing a collection in the driver program, or by transforming an existing RDD. RDDs are lazy and ephemeral by default, meaning they are computed on demand and not stored persistently after use. However, users can modify this behavior using two actions: The cache action suggests keeping the dataset in memory after its initial computation for future reuse. The save action evaluates and persists the dataset to a distributed filesystem, like HDFS. This approach to persistence is a design choice in Spark to ensure continued operation under memory constraints or node failures, drawing a parallel to the concept of virtual memory.</p> <p>Parallel Operations Operations on RDDs: Spark supports various parallel operations on RDDs, such as reduce (combining elements using a function), collect (sending all elements to the driver program), and foreach (applying a function to each element for its side effects).</p> <p>Shared Variables Handling Variables in Parallel Operations: When parallel operations like map and filter are performed, the closures (functions) used can refer to variables in their creation scope. By default, these variables are copied to each worker node.</p> <p>Types of Shared Variables: Spark introduces two types of shared variables for common usage patterns: Broadcast Variables: Used for distributing large, read-only pieces of data efficiently across workers. Accumulators: These are “add-only” variables for workers and readable only by the driver, suitable for implementing counters and parallel sums.</p> <p>-Text Search The first example is a simple text search to count the number of lines containing “ERROR” in a large log file. The process involves creating an RDD from the file, filtering lines containing “ERROR”, mapping each line to 1, and then reducing by summing these ones. This example illustrates Spark’s lazy evaluation and in-memory data sharing capabilities, which allows for efficient data processing without materializing intermediate datasets.</p> <ul> <li> <p>Logistic Regression This program demonstrates an iterative machine learning algorithm, logistic regression, which benefits significantly from Spark’s ability to cache data in memory across iterations. The program reads points from a file, caches them, and then iteratively updates a vector w using a gradient computed in parallel across the points. The use of accumulators for summing the gradient and the syntax of Spark make the code resemble an imperative serial program while being executed in parallel.</p> </li> <li> <p>Alternating Least Squares (ALS)</p> </li> </ul> <p>Implementation The section details Spark’s implementation, including its reliance on Mesos for cluster management, the structure of RDDs, task scheduling for parallel operations, handling of shared variables, and integration with the Scala interpreter.</p> <p>Resilient Distributed Datasets (RDDs): RDDs are implemented as a chain of objects capturing their lineage, allowing efficient recomputation in case of node failures. Different types of RDDs (e.g., for files or transformed datasets) implement a standard interface for partitioning, iteration, and task scheduling. Parallel Operations and Task Scheduling: Spark creates tasks for each RDD partition and tries to schedule them based on data locality. It uses a technique called delay scheduling for efficiency. Handling of Shared Variables: Broadcast variables and accumulators are implemented with custom serialization formats to ensure efficient distribution and fault tolerance. Interpreter Integration: Spark integrates with the Scala interpreter, allowing interactive processing of large datasets. Modifications were made to ensure that closures and state are correctly serialized and distributed to worker nodes.</p> <p>5 Results Logistic Regression Performance: Spark significantly outperforms Hadoop in iterative machine learning workloads, with up to 10x faster performance due to data caching. Alternating Least Squares (ALS) Performance: The use of broadcast variables for distributing the ratings matrix results in substantial performance improvements in the ALS job. Interactive Spark Usage: Spark enables interactive querying of a large dataset with sub-second response times after initial data loading, providing a much faster and more interactive experience than Hadoop.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### MR ML model training
- it may scale great, but overhead is high: ML training just not well suited for stateless, deterministic functions
##### Spark better

##### Parameter Servers
1. Shared Memory for Parameters
2. Atomic Operations on Parameters
3. Efficient Communication via RPCs
4. Avoid Repartitioning Input Data

###### Sync? 
1. MR Approach: Requires strict synchronization (barrier synchronization) at the end of each stage, ensuring consistency but potentially leading to idle time as all nodes must wait for the slowest one.
2. Parameter Server Approach: Offers flexibility. Synchronous mode ensures consistency but might slow down the training due to waiting times. Asynchronous mode can significantly speed up training as it allows nodes to compute and update independently, but it introduces some level of noise in the updates, which the training process must tolerate.

</code></pre></div></div> <p>Parameter Servers</p> <p>Sequential (BSP - Bulk Synchronous Parallel):</p> <p>Traditional distributed computing, like in Spark, utilizes synchronous communication. Each iteration requires all tasks to be completed before moving to the next. Advantages: Broad applicability and high convergence quality per iteration. Disadvantages: Each iteration waits for the slowest task, leading to longer overall task computation times. Bounded Delay (SSP - Staleness Synchronous Parallel):</p> <p>Sets a maximum delay time, known as the staleness value, allowing a certain degree of inconsistency in task progress. When staleness = 0, it equates to the Sequential consistency model; when staleness = ∞, it becomes the Eventual consistency model. Advantages: Reduces waiting time between tasks to a certain extent, offering faster computation speed and allowing developers to balance between algorithm convergence rate and system performance. Disadvantages: The convergence quality per iteration may not be as high as BSP, potentially requiring more iterations to achieve similar convergence; not as broadly applicable as BSP, with some algorithms not suitable. Eventual (ASP - Asynchronous Parallel):</p> <p>Asynchronous communication, where tasks do not need to wait for each other. Tasks that complete first can proceed to the next iteration immediately. Advantages: No waiting for other tasks, leading to fast computation speed. Disadvantages: Poor applicability, potential decrease in convergence rate, or even non-convergence. The synchronization restrictions of the three consistency models progressively relax to pursue faster computation speeds. In practice, the consistency model and related parameters should be adjusted based on the changes in metrics to balance convergence and computational speed.</p> <p>User-defined Filters:</p> <p>The system supports user-defined filters to filter out certain entries, thus reducing network bandwidth. Common filters include the significantly modified filter, which only pushes entries that have changed beyond a certain threshold, and the KKT filter, which uses conditions from optimization problems to filter out entries that have minimal impact on weights.</p> <p>Vector Clock: The parameter server uses a range vector clock to record the timestamp of each node’s parameters. This helps in tracking the state of data and avoiding the resending of data. Since parameters are pushed and pulled in ranges, parameters within the same key range can share a timestamp. This approach compresses the traditional vector clock, reducing memory and network bandwidth overhead.</p> <p>Messages: Messages sent between nodes consist of a range vector clock and (key, value) pairs. Message Compression: Due to frequent updates of model parameters, the parameter server employs two methods to compress messages to reduce network bandwidth overhead:</p> <p>Key Compression: Since training data typically doesn’t change during iterations, it’s unnecessary for workers to send the same key lists each time. The server can cache the key lists upon the first reception. Subsequently, only the hash values of the key lists need to be sent for matching.</p> <p>Value Compression: Some parameter updates are not significant for final optimization, so users can define filter rules to discard unnecessary parameters. For instance, a large number of values being 0 or very small gradients can be inefficient in gradient descent and can be filtered out.</p> <p>Consistency and Replication: The parameter server uses consistent hashing to map keys and servers onto a ring according to a certain hash algorithm. Each server manages the key range from its insertion point counter-clockwise to another server. The server closest in the clockwise direction to a key on the ring is known as the primary server for that key range. Each server also backs up key ranges counter-clockwise, and these servers are known as backup servers for that key range. A physical server is often represented as multiple virtual servers to improve load balancing and fault tolerance.</p> <p>Chain Replication vs. Replication after Aggregation: Chain Replication: As shown in the left diagram, worker 1 updates x, server 1 processes the data with a custom function f(x), and then backs up f(x) to server 2. The push is completed only after worker 1 receives an acknowledgment. This backup method can cause significant network bandwidth overhead for algorithms requiring frequent parameter updates.</p> <p>Replication after Aggregation: As shown in the right diagram, the server aggregates updates from all workers before backing up and then sends acknowledgments to workers. Waiting for aggregation can introduce latency in pulling updates, but this can be mitigated by relaxing the consistency model.</p> <p>Server Management: Adding a Server: The server manager assigns a key range to the new server, and other servers adjust their key ranges accordingly. The new server acquires the key range it will maintain as the primary server and the key ranges it will back up as a backup server. The server manager broadcasts the changes to nodes. Removing a Server: When the server manager detects a server failure through heartbeat signals, it assigns the key range of the failed server to a new server and removes the failed server. Worker Management: Adding a Worker: The task scheduler assigns data to the new worker. The new worker loads training data and then fetches parameters from the server. The task scheduler broadcasts node changes, which may cause other workers to release some training data. Removing a Worker: Losing a small portion of training data typically doesn’t affect the training results. Moreover, restoring a worker requires more overhead than restoring a server. Therefore, removing a worker is usually done by simply disregarding the node. This can be used to terminate the slowest worker, mitigating the performance impact of stragglers. However, users can also choose to replace the lost worker with a new one.</p> <p>``` <a href="#blog-chapters">Back to Blog Chapters</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/15661/">Compiler Design Notes - Currently Upating...</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/15640/">Distributed Systems Notes - Currently Upating...</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/15513/">Computer Systems - Currently Updating...</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/14741/">Information Security - Currently Updating...</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/14740/">Fundamentals of Telecommunication Networks - Currently Upating...</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Yizhou Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>