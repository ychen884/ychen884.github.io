---
layout: post
title: Distributed Systems Notes - Currently Upating...
date: 2024-01-18 11:00:00
description: Distributed Systems Notes - SCS
tags: CMU
categories: Study
featured: true
---

### Course Evaluation (Final grade: N/A)
I will add my course evaluation at the end of this semester.
### Paper reviews
I will upload my paper reviews here later.

## **Blog Chapters**
1. [Chapter 1: Remote Procedure Call (RPC)](#topic-1)
2. [Chapter 2: Caching](#topic-2)
3. [Chapter 3: ](#topic-3)
4. [Chapter 4: ](#topic-4)
5. [Chapter 5: ](#topic-5)
6. [Chapter 6: ](#topic-6)
7. [Chapter 7: ](#topic-7)



## ** Chapter 1: Remote Procedure Call (RPC) ** <a name="topic-1"></a>
- Try to fake procedure call to local programming
- Why? bring down programming complexity for distributed systems 
- client-server model, per interface
- two aspects: control flow, invocation syntax
- with network delays (theoretically best at speed of light)

##### limitations of RPC 
- No address space sharing between client and server, can't sharing pointers(call by reference), can't share global data..
- Delayed binding in RPC

##### Failure independence
- caller and callee live and die together in local setup
- we can witness failure case but hard to do in local
- failure handling consider visibility of failure
- Security: different domains

##### Typical RPC
- client: makerpc(request_packet, &reply_packet): blocks until reply or failure
- server: getrequest(&request_packet) blocks until receives request, sendresponse(reply_packet)

##### Stub routines
- generated by stub generator
- sit between high level purpose and low level network packing/unpacking send/recv..

##### procedure:
```

--: network communication

        Client                                             Server
App -> Stub -> Transport --Network Com--- Transport -> Stub -> App

App calls Stub
Stub pack/unpack
Transport transmit/receive
Server App do actual work then return
Packing and unpacking is usually not elastic in dev env
Correctness and API design is more important

```
- Marshalling/Unmarshalling, Serialization/De-serialization

##### RPC packet format
- Network transport header(Ethernet, IP, Transport(TCP/UDP))
- RPC header
```
RPC Version ID
Opcode (Stub)
Flags
parameters + Len
```

##### Stub with dynamically allocated buffer
- variable to indicate how many bytes in coming
- malloc for new memories


##### Failure independence of clients & servers adds complexity
###### Outsourcing (for example, relying on TCP). TCP guarantees reliable, in-order, unlimited delivery.
```
Pain:
- no preservation of write() boundaries
- data is re-framed in transit
- read may return fewer than number of bytes requested

```
2. The buck has to stop somewhere. Do it yourself.
- Retransmission
- duplicate delivery/execution violates RPC semantics, sequence numbering to eliminate
- based on UDP



##### Timeout values in distributed systems
- statistics -> reasonable
- no matter what, could be too soon

##### What does server do when receiving duplication happens?
- indicates one of these happening: 
1. reply lost
2. reply crossed retransmitted request
3. compute time was excessive
4. client too impatient.
5. ...


##### Knowlege at server is always stale relative to client, and vice versa
- processing time/network transmit time, indifferent to client..
- best for server to do is retrans
- should preserve reply, not re-compute, because computation can be substantial
- *my question: what if replies too large? maybe best effort

##### Exactly-once semantics
- How long to keep old replies and sequence numbers
- Rigorous interpretation of "RPC" -> forever
- Server crashes: 
- - saved in non-volatile
- - server response has to be after non-volatile write
- - disk/flash latency per RPC
- - clean undo of partial computations before crash
- exactly-once RPC: 
- - success return -> call exected exactly once
- - call blocks indefinitely, no failure return



##### In practice for RPC package: At-most-once semantics 
```
Avoid indefinite blocking
Declare timeout beyond certain delay: success or not? 
many timeout reasons...
```

##### Slow Servers & Long-Running Calls
```
Solution: 
probes to check server health during long calls
server responds busy while working
essentially a keepalive mechanism
```

##### Orphaned Computations
```
network failure, then server continues, unaware its work is useless
Orphan detection and extermination are difficult - but important
```

##### At-least-once semantics (strongest)
```
Even simpler to implement
Requires operation idempotency
Idempotency is a property of certain operations or API requests, which guarantees that performing the operation multiple times will yield the same result as if it was executed only once.
- for example, read request on locked object or read-only object, current_stock_price(MSFT)
```

###### Choice of semantics (less strong)
- Achieving exactly-once semantics
```
not provided in any real RPC package
requrie application-level dup elim
built on top of at-most-once RPC
- have to write on disk before replying
```
- At-most-once (mostly provided by RPC packages)
```
avoids: 
- transactional storage
- non-volatile storageo of replies and sequence #s
- indefinite storage of replies

if crashed, just crash without executing anything
exactly once: have to undo, and do again using instruction in non-vol
```

##### Safety and liveness properties
```
safety: correct functionalty ("At most one entity can execute in a critical section")
- bad things never happen

liveness: characterizes timely execution progress (e.g., "This code is deadlock-free")
- good things will eventually happen
```
- "Exactly-once semantics" -> safety property
- Existence of timeout in at-most-once RPC -> liveness property



##### Placement of Functionality - what are you promising vs deliver
- Protocol layering

##### TCP for RPC
```
TCP timeout -> reconnect
- new connection is unaware of old
- server need to do dup elim
- orphans still possible
- exactly-once RPC no easier with TCP

TCO simplifies at-most-once RPC
TCP hurts since it has independent acks in each direction


User TCP rather UDP:
- not simplify exactly-once imple
- worse performance in best case
- simplify at-most once impl

End-to-end argument:
For a given functionality:
- correctness is expressed relative to two endpoints (safety)
- implementation requires support of those two end points
- support below end points cannot suffice (may improve performance(liveness))

```


##### Critical question: Where to place function in a distributed system?
```
What guarantee promising
end point 
package
simplified implementation
guarantee properties
```


##### Performance of distributed system: delay
1. Processing delay
2. Queueing delay <- dominate
3. Tranmission <- usuallly small unless very large files


##### Queueing
```
Any shared resources
Arrival pattern: uniform, poisson, ...

Service time also varies

---- without considering real world complications:----
1. Util
2. Latency
3. Freedom (how constrained of arrival discipline..) 
We can optimize at most 2 out of 3




```


##### Latency is the killer

## Chapter 2: Caching <a name="topic-2"></a>

##### Metrics
```
Miss Ratio = Misses/References
Hit Ratio = Hits/References = (1 − Miss Ratio)
Expected cost of a reference = (Miss Ratio * cost of miss) + (Hit Ratio * cost of hit)
Cache Advantage = (Cost of Miss / Cost of Hit)
```
##### Fopcus: distributed file systems
###### Fetch policy
```
Full Replication?
-Storage for entire subtree consumed on every replica
-Significant update traffic on hot spots
-Machines receive updates whether they care or not
Coarse-grain, non-selective management of data 

A Much Better Approach:
on-demand caching 
- requires operating system modifications
+ total application transparency
+ enable demand caching

Multi-OS On-Demand Caching 


```
2. Update propagation policy
3. Cache replacement policy ((prefetching))

##### Caching in the Real World
1.  Cost of remote data access often not uniform
2.  spatial locality
3.  Remote data more coarsely addressable than local
Fetch more than you need on miss

##### Spatial & Temporal Locality


##### Update Propagation
```
One-copy semantics
• there are no externally observable functional differences
• relative to same system without caching (or replication)

This model aims to provide the illusion that although data may be replicated across multiple servers or nodes (to improve reliability, performance, and fault tolerance), users interact with the data as if there is only one copy.

Challenges:
Physical master copy may not exist
Network may break between some users and master copy
Intense read- and write-sharing across sites
(The benefits of caching (reducing access time, decreasing bandwidth usage) are undermined in this scenario. The caches might spend more time synchronizing than serving the actual read and write requests, making them "effectively useless")
```

##### Cache Consistency Strategies
###### Broadcast Invalidations
```
Notification to All Caching Sites:every caching site in the network is notified, regardless of whether they actually have the cached object.

Handling at Cache Sites:
Upon receiving the invalidation notice, if a cache has the invalidated object, it will mark it as invalid.

Strengths
- Strict One-Copy Semantics:
- Race Condition Prevention:
If the updating process is blocked until all caches have invalidated the item, it prevents race conditions, ensuring that no stale reads occur.
- Simplicity



Limitations
- Wasted Traffic:
- Blocking Updating Process:
- Scalability Issues:
As the number of nodes in the system increases, the overhead of sending invalidations to every node and the corresponding acknowledgments becomes impractical. 
```

###### Check on Use:
```
Reader checks master copy before each use
Has to be done at coarse granularity (e.g. entire file or large block)
Whole file granularity-> “session semantics”

Advantages
• strict consistency at coarse granularity
• easy to implement, no server state
• servers don’t need to know of caching sites

"session semantics at open-close granularity," where changes made to a file during a session (from open to close) are not visible to other clients until the session ends (the file is closed).
principled weakening of strict one-copy semantics


Strict One-Copy Semantics With Write-Sharing
any write operation performed by any client or process in a distributed system is immediately visible to all other clients or processes. 
```
##### Sharing Taxonomy

```
Multiple concurrent read-only sessions 
 “read sharing”

Multiple concurrent read-only sessions + one read-write session
 “read-write sharing”

Multiple concurrent read-write sessions “write-write sharing”
1. “Last Close After Write Wins”: AFS
2. “Raise Conflict Exception”: Coda File System
3. “Live Happily and Be Blissfully Unaware”:  NFS, Google Docs, DropBox, etc.
None of these approaches prevents concurrent updates on server


3: How to do?


“Last Close After Write Wins”
Mechanism: This approach resolves write-write conflicts by accepting the changes made by the process that closes the file last. All previous writes to the file during the conflict period are overwritten by the last writer's data.
Pros: Simplifies conflict resolution by enforcing a clear rule.
Cons: Can lead to data loss for earlier writes, as changes made by all but the last writer are discarded.
Use Case: AFS uses this method, prioritizing simplicity and predictability over the preservation of every change


“Raise Conflict Exception”
Mechanism: When a write-write conflict is detected, the system raises an exception, alerting the involved parties (applications or users) of the conflict.
Pros: Prevents data loss by not automatically overwriting any data. It requires intervention to resolve the conflict, which can ensure that important data isn't inadvertently lost or overwritten.
Cons: Requires additional mechanisms for conflict resolution and may interrupt the user workflow.
Use Case: The Coda File System employs this strategy to maintain high data integrity, especially in environments where data consistency is critical.


“Live Happily and Be Blissfully Unaware”
Mechanism: This approach allows concurrent writes to proceed without immediate conflict resolution. Systems employing this strategy might merge changes automatically, keep all versions of the file, or simply ignore the conflict entirely.
Pros: Enhances user experience by avoiding interruptions. Systems like Google Docs merge changes in real-time, allowing seamless collaboration.
Cons: Can lead to inconsistencies or unexpected results if automatic merging is not possible or if changes are incompatible.
Use Case: NFS (Network File System) traditionally does not handle write-write conflicts explicitly at the file system level. Google Docs and Dropbox provide user-friendly collaboration features, allowing multiple users to edit documents simultaneously, with changes reflected in real-time or through version history.

```
##### Check-on-open diadv
```
slows read access on loaded servers & high-latency networks
check is almost always success: frivolous traffic
load on network and server
```
##### Callback
```
targeted notification of caching sites
on update, all sites with cached data notified (“callback”)

Advantages:
        excellent scalability for Unix workloads (often involve a mix of read and write operations on files)
        zero network traffic for read of cached-valid objects 
        precursor to caching for disconnected operation
        biases read performance in favor of write-performance

Disadvantages:
        sizable state on server
        complexity of tracking cached state on clients
        NAT networks with masquerading firewalls
        
        Clients may not be able to distinguish between a network failure that prevents 
        callbacks from reaching them and a situation where no data changes have occurred


        Periodic “Keepalive” Probes: To mitigate some of the issues with lost callbacks and ambiguous silence, systems might implement periodic "keepalive" probes. These probes allow clients to verify their connection to the server and the validity of their cached data. However, this approach introduces additional network traffic and can only partially address the problem, as data could still become stale between probes.


```
#####  Prevents concurrent updates on server

```
Method 4:
Caching site obtains finite-duration control from master copy
few seconds
multiple sites can obtain read lease; only one can get write lease

lease duration = 0: Check on Use
lease duration = ∞: (Targeted Notification)

Advantages
• generalizes the check on use and callback schemes
• lease duration can be tuned to adapt to mutation rate
Lease duration is a clean tuning knob for design flexibility
For data that changes frequently, shorter leases ensure that caches are updated or invalidated promptly, maintaining data consistency. For more stable data, longer leases can reduce the overhead of lease renewals and data checks, improving system efficiency. 
• conceptually simple yet flexible

Key Assumption
Clocks tick at the same rate everywhere
• clocks do NOT have to be synchronized
• absolute time does not matter
• only relative time (i.e., clock tick rate) matters
Time becomes a hidden communication channel



Disadvantages
• lease-holder has total autonomy during lease; revocation?
• writers delayed while read lease holders complete their leases
• more traffic than callback (but less than check on use)
keepalives for callback only one per server, not per lease


```
##### What To Do When There is Intense Write-Sharing?

```
5. Skip Scary Parts
• When write-sharing detected, turn off caching everywhere
All references go directly master copy
• Resume caching when write-sharing ends

Original Use
• Sprite (circa 1987) (in conjunction with check on use)

Advantages
• Precise single-copy semantics (even at byte-level consistency)
• Excellent fallback position
• Good adaptation of caching aggressiveness to workload
Disadvantages
• Server maintains state
• Server aware of every use of data (open)

```

##### Even weaker safety property?
```

6. Faith-Based Caching
Basic Idea
• blindly assume cached data is valid for a while
• periodically check (based on time since last check)
• no communication needed during trust period
Original use
• Sun NFSv3 file system
cached file blocks assumed current for X seconds
X = 3 for files, 30 for directories
• small variant is a TTL field for each object
used in web caching, gives content creator modicum of control

Imprecise and weak approximation to one-copy semantics
• not just in the presence of network failures (partitions)
• no clear basis for reasoning about state of system
Methods 1-5 used controlled and precise approximations
• e.g., session semantics at open-close granularity
• offered clear basis for reasoning

Advantages
• Simple implementation
• Server is stateless

Disadvantages
• User-visible inconsistencies sometimes seen (make)
• Blind faith sometimes misplaced!
• Not as efficient as callback-based schemes



7. Pass the Buck
Basic Idea
• Let the user trigger cache revalidation (hit “reload”)
• otherwise, all cached copies assumed valid forever
• Equivalent to infinite-TTL faith-based caching
• Arose in the context of the Web

Advantages
• trivial to implement, no server changes
• avoids frivolous cache maintenance traffic

Disadvantages
• places burden on user
user may be clueless about level of consistency needed
• assumes existence of user
• pain for write scripts/programs



```
##### What old data do you throw out to free up space?
###### Cache replacement policy
```

Ideal victim: large object that is not needed for a long time, and cheap to refetch

Non-Serviceable Miss: The pain of evicting an object that cannot be easily refetched due to failures or disconnected operation is also a vital factor. This aspect is particularly relevant in environments with unreliable connectivity or when dealing with unique or hard-to-replace data.

Uniform vs. Non-Uniform Fetch Cost: While the assumption of uniform fetch cost might hold near the hardware level (e.g., when all data resides on the same disk or in the same data center), it becomes less tenable at higher system layers. At these layers, factors such as variable object sizes, the physical characteristics of storage media (like rotational and seek delays in disks), and differing network qualities to different servers introduce non-uniform fetch costs.

Abstract Problem Formulation: 
The problem can be abstracted to managing a set of 
equal-sized data containers (frames) and 
a large set of equal-sized, equal-importance data objects (pages), 
with access patterns represented by a sequence of integers (reference string). 
The primary metric of interest in this model is the miss ratio.

```

##### Predict distance to next ref
```
optimal replacement algorithm
predict is hard
• LRU is often a good approximation to OPT (not always)
assumes recent past is a good predictor of the near future

```

##### LRU
```
Loop (good for LRU)
- while (some test)
Sequential scan (bad for LRU)
- memset (start address, 0, many bytes)


Stack Property: “Adding cache space never hurts”
LRU has this property
Other cache replacement algorithms may not
FIFO exhibits “Belady’s anomaly


When is LRU Ineffective?
• purely sequential access (aka “scan”)
caching cannot help at all; only adds overhead

• purely random access
ratio of cache size to total data size is all that matters

Examples:
sequential scan of large files & databases in data mining

video/audio playback (“streaming data”)

hash-based data structures cause accesses to be “spread out”


Example: small tight loop accessing huge array sequentially
• code shows high locality, but data does not
• the sequential data access will “pollute” an LRU-based cache
• even code (which shows locality) will be flushed out of the cache
```
##### Working Set

```
Given a time interval T,
WorkingSet(T) is the set of distinct data objects accessed in T
This captures adequacy of cache size relative to program behavior
• small working set, small cache is enough,high locality
• large working set, poor locality
size and pages in working set may change over time
```

##### queueing theory
```
It improves utilization by reducing the load on critical resources, 
thereby preventing bottlenecks and allowing for more efficient resource usage. 
Caching also directly reduces latency
it provides greater flexibility in handling different arrival disciplines, 
ensuring the system can cope with varying patterns of demand.

utilization vs peak

```


##### The Curse of Uncacheability

```
Businesses want to know your every click
Client caching hides this knowledge from the server
hurts response time, network load & server load


Can businesses benefit from caching without giving up control?

Content Distribution Networks
A Weak Solution
Effectively third-party caching sites trusted by businesses
Late binding of parasitic content by caching site
Pioneered by Akamai in the late 1990s
• many examples now
• e.g., CloudFront (Google), Windows Azure CDN, Streamzilla

Not as effective as client caching
But better than no caching at all, and better monetizes cached resources
```
[Back to Blog Chapters](#blog-chapters)





[Back to Blog Chapters](#blog-chapters)

