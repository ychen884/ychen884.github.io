---
layout: post
title: Storage Systems - Currently Upating...
date: 2024-08-27 11:00:00
description: Storage Systems Notes - SCS
tags: CMU
categories: Study
featured: true
---

### Course Evaluation (Final grade: N/A)

### Reading summary
#### Remzi - SSD
```
https://pages.cs.wisc.edu/~remzi/OSTEP/file-ssd.pdf

Hence, flash chips are organized into banks or
planes which consist of a large number of cells.

A bank is accessed in two different sized units: blocks (sometimes
called erase blocks), which are typically of size 128 KB or 256 KB, and
pages, which are a few KB in size (e.g., 4KB). 

Each page has a state associated with it: Invalid, Erased, Valid
Writing a page is trickier; the entire block must first be erased

Perf read > program -> erase, 


Reliability program: 
1. wear out!
increasingly difficult to differentiate between a 0 and a 1 -> block becomes unusable
2. disturbance
When accessing a particular page within a flash, it is possible that some bits get flipped in neighboring pages

Internally, an SSD consists of some number of flash chips (for persistent storage). 
An SSD also contains some amount of volatile (i.e., nonpersistent) memory (e.g., SRAM); such memory is useful for caching and buffering of data as well as for mapping tables. 
Finally, an SSD contains control logic to orchestrate device operation (satisfy client
reads and writes, turning them into internal flash operations as need be. The flash translation layer, or FTL, goal: performance and high reliability)

Techniques: 
utilize multiple flash chips in parallel, 
reduce write amplification(total write traffic (in bytes) issued to the flash chips by the FTL divided by the total write traffic (in bytes)),
wear leveling(o spread writes across the blocks of the flash as evenly as possible)


FTL Organization: 
Why direct map bad (pref + reliability): severe write amplification (proportional to the number of pages in a block); If file system metadata or user file data is repeatedly overwritten, the same block is erased and programmed, over and over.

Log-Structured FTL
Upon a write to logical block N, the device appends the write to the next free spot in the currently-beingwritten-to block; we call this style of writing logging. 

To allow for subsequent reads of block N, the device keeps a mapping table (in its memory, and persistent, in some form, on the device)

    -: old versions of data around the drive and taking up space. The device has to periodically perform garbage collection (GC). Example: same logical blocks written again, written to a new block. Old block has garbage data. find a block that contains one or more garbage pages, read in the live (non-garbage) pages from that block, write out those live pages to the log, and (finally) reclaim the entire block for use in writing.

    high cost of in-memory mapping tables. page-level FTL scheme is impractical due to
    large size. Issue for block-based mapping: small write (read a large amount of live
    data from the old block and copy it into a new one). 

hybrid mapping:
per-page mappings for these log blocks.The FTL thus logically has two types of mapping table in its memory: a small set of per-page mappings in what weâ€™ll call the log table, and a larger set of per-block mappings in the data table. 

+ Caching: 
+ Wear leveling: long-lived data that does not get over-written; in this case, garbage collection will never reclaim the block, and thus it does not receive its fair share of the write load. 

```

#### Design Tradeoffs for SSD Performance (Agrawal2008)
```
SSD performance and lifetime is highly workloadsensitive, and that complex systems problems that normally appear higher in the storage stack, or even in distributed systems, are relevant to device firmware

Interleaving: 
Interleaving can provide considerable speedups when the operation latency is greater than the serial access latency. 
For example, a costly erase command can in some cases proceed in parallel with other commands.
constraints: operations on the same flash plane cannot be interleaved.

SSD Basics:  host interface logic to support some form of physical host
interface connection. (flash translation layer)

SSD Controller contains processor, buffer manager (holds pending and satisfied requests along the primary data path),  A multiplexer(Flash Demux/Mux)emits com
mands and handles transport of data along the serial connections to the flash packages.
They are typically ASIC/FPGA.


each write of a single logical-disk block address (LBA) corresponds to a write
of a different flash page.

Variants(allocation pools): Static map, Dynamicmap, Logical page size, Pagespan
Bounded by constraints: LB, Parallel access, Block erasure.

if a large portion of the LBA space is statically mapped, then there is little
scope for load-balancing. 
If a contiguous range of LBAs is mapped to the same physical die, performance for se
quential access in large chunks will suffer. 
With a small logical page size, more work will be required to eliminate valid pages from erasure candidates. 
If the logical page size (with unit span) is equal to the block size, then
erasure is simplified because the write unit and erase unit are the same,
however all writes smaller than the logical page size result in a 
read-modify-write operation involving the portions of the logical page 
not being modified.


Flash blocks are the basic units for allocation, and a garbage collector is used to recycle used blocks by erasing and preparing them for reuse. Cleaning involves moving valid data from blocks before they are erased. The goal is to maximize cleaning efficiency, which is the ratio of outdated pages to total pages during block cleaning.

Wear-leveling is crucial to ensure that all blocks age evenly, preventing premature failure of any single block. SSDs are always full relative to their advertised capacity, requiring spare blocks to facilitate ongoing cleaning and block replacement. Unlike log-cleaning in Log-Structured File Systems, SSDs face constant cleaning pressure due to the absence of free sectors, making efficient block management essential.


Parallel Requests: Each flash element operates independently, handling separate I/O requests. This requires complex logic to manage queues for each element, which might be challenging in systems with limited processing power.

Ganging: Multiple flash packages operate in synchrony to optimize multi-page requests, reducing complexity. However, idle elements can occur if the request doesn't span all packages.

The choice of parallelism technique depends on the workload:

Sequential Workloads: Benefit from ganging.
Parallel Workloads: Require deep parallel request queuing.
Workloads with Poor Cleaning Efficiency: Need a cleaning strategy compatible with the foreground load.

 Technique Positives Negatives
 Large allocation pool 
 Load balancing, but Few intra-chipops

 Large page size 
 Small pagetable, but Read-modify-writes
 Overprovisioning Less cleaning, but Reduced capacity
 Ganging Sparser wiring, but Reduced parallelism
 Striping Concurrency, but Loss of locality


Even wear:  algorithm tracks the erase count of each block and proposes managing blocks based on their remaining lifetime relative to the average. Blocks that fall below certain thresholds are either recycled at a lower rate or have cold data migrated into them to balance wear.Rate-limiting Worn Blocks, Cold data is migrated into worn-out blocks before they hit critical wear thresholds.this approach incurs a small performance overhead due to the additional operations involved in data migration.
```


## **Blog Chapters**
1. [Chapter 1: Introduction ](#topic-1)



## ** Chapter 1: Introduction ** <a name="topic-1"></a>
#### Definitions


[Back to Blog Chapters](#blog-chapters)