---
layout: post
title: Advanced Cloud Computing Notes - Currently Upating...
date: 2024-01-16 11:00:00
description: Advanced Cloud Computing Notes - SCS
tags: CMU
categories: Study
featured: true
---

### Course Evaluation (Final grade: N/A)
I will add my course evaluation at the end of this semester.


## **Blog Chapters**
1. [Chapter 1: Overview of Cloud Computing](#topic-1)
2. [Chapter 2: Building a Cloud](#topic-2)
3. [Chapter 3: Encapsulation](#topic-3)
4. [Chapter 4: ](#topic-4)
5. [Chapter 5: ](#topic-5)
6. [Chapter 6: ](#topic-6)
7. [Chapter 7: ](#topic-7)



## ** Chapter 1: Overview of Cloud Computing ** <a name="topic-1"></a>
#### Definitions

##### Properties:
- Computing utility, always available, accessible through the networks
- Simplified interface
- Statistical multiplexing, sharing resources
- Economies of scale from consolidation, costs lower
- Capital costs converted to operating costs
- Rapid and easy variation of usage
- Appearance of infinite resources with small users
- Pay only for what you use
- Cost conservation: 1 unit for 1000 hours == 1000 units for 1 hour

##### Consolidation, sharing, elasticity
- CLT theory
- users with widely varying needs apply a considerably less variable load on a huge provider, allowing providers to do less overprovisioning. 
- - Because of CLT, it is predictable for the overall load which causes less overprovisioning.
- Users perceive exactly what they need all the time, if their needs are “small”(so the accessed resources are appearing as infinite)

##### SaaS, PaaS, IaaS
- SaaS: service as application (Salesforce)
- - consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage & minimum deployed applications configurations settings.
- PaaS: high-level programming model for cloud computer, Turing complete but resource management hidden. (Google AppEngine)
- - consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage but with control over the deployed applications and possibly configuration settings for the application-hosting environment.
- IaaS:  low-level computing model for cloud computer (AWS)
- - The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications

##### XXX as a Service
- Data as a Service, Network as a Service, Communication as a Service(No hardware private VOIP switching), IT as a Service(IT providing services)..

##### Deployment models
```
public cloud: provider sells computing to unrelated consumers
private cloud: largely unrelated components and divisions as consumers
community cloud: providers and consumers are different organizations with strong shared concerns
Hybrid cloud: private plus public resources combined by same consumer. Better availability, overflow from private to public, load balancing to increase elasticity
```
##### Larry Ellison’s objection
- definition is including too much
<!-- ● E.g., timesharing: many users slice up a single big computer
○ Virtualize the hardware as protected processes with shared storage
○ Accumulate usage accounting for billing/cost-sharing
● E.g., client-server computing: program API for communication
○ Relieve clients from burden of operating, provisioning server
○ Set abstraction of server function at a natural/constrained level
● E.g., leasing companies – spread the cost over computer lifetime
○ Rates depend on commitment: 100% -> prime rate loan,
lower commitment to pay leads to high “interest” rates -->

##### Obstacles of cloud computing
- Privacy & security
- Privacy in the world tends to rely on regulation
- Utility issues
- Physical utilities tend to rely on regulation 
- High cost of networking combined with always remote
- Performance unpredictability & in situ development/debugging
- Software licensing – $/yr/CPU is not elastic and pay as you go


##### load balancing approach
##### 1. DNS load balancing
- DNS reorder the list for each client asking for translation of name
- PRO: easy to scale, unrelated to actual TCP/HTTP requests
- CON: new server may get less resources if scheduling more servers, dynamic changing is hard because it has to tell client for a binding(existing binding exists due to TTL, caching in network middleboxes). 


##### 2. Router distribute TCP connections 
- Router do the mapping: (client_ip+client_port) <-> (server_ip+server_port)
- - for SYN packets
- - not exposed to client about the ip address(like NAT)


- PRO: router doesn't have to think or remember too much, *it just selected the server with least connnection to schedule for the new connections.
- CON: traffic all go through the router(more difficult to scale), and decision takes time cuz it's for the entire session


#### 3. Router distribute individual quests embedded in connections
<!--  -->
- PRO: most dynamic
- CON: requires the most processing and state in the router, CPU load and memory goes up due to intelligent routing decisions.
#### Elasticit: How? Elasticity controller
- Elasticity controller to adjust load capability based on current load status
- Monitoring: resource usage, request sequence(patterns)
- Triggering: (simple conditions like thresholds), schedule, complex model based on monitored instances

#### Elasticity: Scale-out or Scale up
- Horizontal scaling: adding more instances (Common approach)
- Vertical scaling: Resizing the resources(bdw, cpu cores, memory) allocated to an existing instances, challenging(different OS.) More challenging.

#### Two-tier services
- web-database
- web server easy to be in cloud. At beginning, order-taking is not in cloud.
- Elasticity in IaaS: database scaling is more difficult with state(consistency)
- PassS, P=Web Service. Built-in elastic load balancing and scheduled actions for containers, persistent key-value store (datastore) & non-persistent memcache
for simple database tier, Users can instantiate Backends, user code can request (actuate) horizontal scaling, running traditional database services, whose scaling is still hard.

##### Load-balancing method affect how much statistics we can get. 
- Router-based load balancing: firewall, intrusion detection, accelerator
- scaling middleboxes: CPU intensive tasks. (OpenFlow, split flows)
- bdw allocation by sw/rt

##### Service parallelization: Load Balancer
- aws cloud watch
- Load balancer is not necessarily elastic

##### Scalable relational database
- Separate data at rest(distributed pay-for-use storage (HDFS)) from ongoing or recent access & mutation
- Recent access & mutation servers are elastic (called Owning Transaction Managers)
- Partitioned but all transactions restricted to one partition: transactions block on locks and bottleneck performance scaling
- Fault-tolerance of Elastic controller.  Controller itself, reliability provided by replication. Can re-assigns partitions while server is down/start up.

##### ElasTraS architecture scales OTM machines
- Transactions are limited to interacting with data from only one partition to avoid the complexity of distributed transactions.
- TODO


### Paper reading notes
##### Armbrust2010

```
Referring to http://doi.acm.org/10.1145/1721654.1721672
Cloud computing: what brings it? large capital outlays, overprovisioning/underprovisioning...

Definition: Refers to both the applications as services over the internet and hardware and systems software in the data center that provide those services. The services themselves: SaaS. Services being sold is utility computing. Cloud computing = SaaS + utility computing. It has to be large enough to be called cloud. Hardware provisioning/pricing: 1. inifinite computing resources available on demand; 3. elimination of an up-front commitment by cloud users, add resources when needed; 4. pay for use of computing resources temporarily. *Construction and operaition of extremely large-scale, commodify-computer data centers at low-cost locations was the key necessary enabler of cloud computing. It could offer services below the costs of a medium-sized data center and make profits.

Utility Computing classes: EC2 with low level control but less automatic scalability and failover(application may need to control the replication...). Google AppEngine(domain specific platforms) on the other hand. Azure is in between. 

Economics: Favor cloud computing over conventional: 1. demand of services changes over time 2. demand is unknown
usage based pricing economically benefits the buyer. 
Elasticity helps reduce the costs. Underprovisioning has a cost that is difficult to measure: the users may never come back. Scale-up elasticity is an operational requirement, and scale-down elasticity allowed the steady state expenditure to more closely match the steady-state workload. 


Obstacles for Cloud computing:

1-3(adoption):
1. Business Continuity and Service Availability (hard to ensure availability, single failure still exists for a service provider)
2. Data Lock-In (public+private sharing by sharing API)
3. Data Confidentiality/Auditability: from other user/provider 

4-8(growth):
4. Data Transfer Bottlenecks： Applications continue to become more data-intensive
5. Performance Unpredictability： I/O interference between virtual machines, concerns scheduling of virtual machines for some classes of batch processing programs, specifically for highperformance computing
6.Scalable Storage
7. Bugs in Large Scale Distributed Systems: bugs cannot be reproduced in smaller configurations
8. Scaling Quickly

9-10(policy and business):
9. Reputation Fate Sharing, legal liability(customer responsible->unexpected down)
10. Software Licensing



Opportunities: 
- improve architectures and operating systems to efficiently virtualize interrupts and I/O channels
- flash memory will decrease I/O interference.
- offer something like “gang scheduling” for cloud computing
- create a storage system that would not only meet existing programmer expectations,but combine them with the cloud advantages of scaling arbitrarily up and down on demand. 
- reliance on virtual machines in cloud computing.(7)
- automatically scale quickly up and down in response to load in order to save money
- create reputation-guarding services similar to the “trusted email” services 
```


##### Referring to NISTdef2011
```
Characteristics:
- On-demand self-service
- Broad network access
- Rapid elasticity
- Resource pooling (multi-tenant)
```

##### Vaquero11
##### Dynamically Scaling Applications in the Cloud
```
vertical scaling is hard: rebooting..
load balancers need to support

Server scalability: a per-tier controller, a single controller for the whole application (for all tiers)
How and When to Scale: Feeding Controller with Rules and Policies
- adding load balancers
- LB scalability requires the total time taken to forward each request to the corresponding server to be negligible for small loads and should grow no faster than O(p) 
-  CPU-intensive web app: a LB to split computation among many instances.
-  network intensive: CPU-powerful standalone instance to maximize throughput
-  more network intensive applications, it may be necessary to use DNS load balancing

Scaling the network:
- Virtualization: VLAN(L2)
-  periodically measure actual network usage per application use other applications’ - increase the utilization of the network by virtually “slicing”: “network as a service”
- statistical multiplexing


Scaling the platform:
PaaS cloud platforms
- Container Scalability: execution environment for user applications.scalability of the container layer is crucial as it must efficiently manage and distribute resources to meet the demands of potentially numerous applications running concurrently.

Multitenant containers and Isolation: requires strong isolation to prevent security issues
Individual Containers per User: simplifies isolation but requires effective management of numerous containers.
Horizontal Scaling via Container Replication: achieved through automatic scaling in IaaS systems or by inherent capabilities of the platform itself
Components should ideally be stateless to cope with the dynamic nature of container instantiation and disposal, support for stateful components is also necessary, requiring sophisticated load balancing (LB) and container management to handle session data.(soft state replication, distributed caching systems)

- Database Scalability: for example, implementing horizontal scaling strategies, such as replicating the database across multiple nodes to handle increased load and ensure data availability. (demands on PaaS platforms can often surpass the capacity of any single machine)
NoSQL Databases: These databases provide high scalability and availability, fitting well in cloud environments with high demand. However, they offer eventual consistency rather than immediate consistency, leading to limitations in transaction support and SQL functionalities.
Replication Mechanisms: In-core Solutions, Middleware Solutions

Ideal Elastic Cloud: scale through VM or container replication, reconfiguration, and dynamic load balancing, possibly using DNS for the latter. allow dynamic allocation of network resources. 
Ideal PaaS Platform Features: capable of instantiating or releasing instances of user components based on demand changes and distributing the load transparently among them. Implementing session concepts and supporting transparent data replication are essential. Access to traditional relational databases with ACID transaction support is crucial. However, the system must address the increased latency due to consistency maintenance across replicas, especially under high demand.
```
-  Infrastructure-as-Code tools provide a high-level software interface (e.g., Python / Ruby or JSON / YAML) that allows developers to specify their infrastructure requirements, software dependencies, and the process for building the infrastructure and deploying it to the cloud 

<!--  -->

[Back to Blog Chapters](#blog-chapters)




## ** Chapter 2: Building a CMU Cloud ** <a name="topic-2"></a>
#### Model
##### aim for less costs

```
Client ->
App
OS
Instance(s)
--------------------
Hypervisor (shared)
Building Blocks
Usage Monitor
Hardware (shared)
(shared Amazon EC2)

```
##### Problem: used mostly during ddl? 2 methods, renting..
##### Build a cloud
```
1. user would rent, so provisioner(resources,monitoring,... -> assignment of users to machines)
- Bin-packing, NP-hard. Can do with assumptions.. -> less complexity
- Migration, also costs
2. Scheduler: which user jobs/processes to run: Prioritization(pay more), Oversubscription, Workload constraints
3. Encapsulation: compute, storage, networking and data
4. Virtualization
5. Fault tolerance: 
    why scale of data center matters? Economy of scale, cost of operations
    state replication, logging, storage replic

Storage services - Scalable, fault tolerant
Tools - Programming models, frameworks
Automation - Reactive systems & elastic scaling

Elastic scaling - based on monitoring and diagnosis
traditional: provision for peaks

```


- Cloud users & Services
- - App user: availability, performance, no interfaces exposed by cloud service provider (app provider did that)
- - Application Deployment User: Dashboard, management interfaces exposed
- - Admin: Management...

- Orchestration: automatic deployment for user

##### OpenStack
- independent parts,6 core services
- communicate through public and well-defined APIs
- Identity -> Dashboard -> Compute(?)/Network -> Image -> Object storage(get the image for VM) -> Block Storage(volume) -> initiate VM
- Distributed storage to accelerate image initialization takes Azure many years to fix


##### Referring to sotomayor2009
##### Virtual Infrastructure Management in Private & Hybrid Clouds
```
Virtual Infrastructure (VI) management in the context of private/hybrid cloud environments: Uniform Resource View, Full Lifecycle Management of VMs, Configurable Resource Allocation Policies, Adaptability to Changing Resource Needs. provides primitives to schedule and manage VMs across multiple physical hosts


OpenNebula: This is a VI manager that allows organizations to deploy and manage VMs, either individually or in groups. It automates the VM setup process (including preparing disk images and setting up networking) and is compatible with various virtualization layers (Xen, KVM, VMware) and external clouds (EC2, ElasticHosts).

Haizea: This acts as a lease manager and can serve as a scheduling backend for OpenNebula. It introduces leasing capabilities not present in other cloud systems, such as advance reservations and resource preemption, which are particularly valuable for private cloud environments.



Traditional VI Management Tools: lack certain features necessary for building IaaS clouds, such as public cloud-like interfaces and the ability to deploy VMs on external clouds.
Cloud toolkits can help transform existing infrastructure into an IaaS cloud with cloudlike interfaces. VI management capabilities are not as robust.
    -: Gap: There's a noticeable gap between cloud management and VI management. Cloud toolkits attempt to cover both areas but often fall short in delivering comprehensive VI management functionalities. Integrating cloud management solutions with VI managers is complex due to the lack of open, standard interfaces and certain key features in existing VI managers.
    
    OpenNebula: overcome these challenges, Scaling to external clouds. A flexible and open architecture for easy extension and integration with other software. A variety of placement policies and support for scheduling, deploying, and configuring groups of VMs.
    
    Integration of Haizea with OpenNebula: work as a scheduler for OpenNebula, This integration allows OpenNebula to offer resource leases as a fundamental provisioning abstraction, and Haizea to operate with real hardware through OpenNebula. This combination provides advanced features like advance reservation of capacity, which is not offered by other VI managers.

    The integration is particularly beneficial for private clouds with limited resources, enabling sophisticated VM placement strategies that support queues, priorities, and advance reservations (ARs).

OpenNebula: 
core:  image and storage technologies for preparing disk images for VMs, network fabric (such as Dynamic Host Configuration Protocol [DHCP] servers, firewalls, or switches) for providing VMs with a virtual network environment, hypervisors for creating and controlling VMs. Performs operations through pluggable drivers. 
A separate scheduler component makes VM placement decisions.
Management interfaces: libvirt API intergrate within other data center management tools, cloud interface exposed to external users.
Cloud drivers to interface with external clouds

The Haizea Lease Manager: Haizea is an open source resource lease manager and can act as a VM scheduler for OpenNebula.
Advance Reservation (AR) Leases: Resources are guaranteed to be available at a specific future time. This is beneficial for scenarios requiring resource certainty.
Best-Effort Leases: Resources are provisioned as soon as possible, with requests queued if immediate provisioning isn't possible.
Immediate Leases: Resources are provided immediately upon request or not at all, suitable for urgent needs without flexibility.
    
Haizea addresses the challenge of resource underutilization, often a downside of AR, by leveraging VMs. It allows for efficient support of ARs through resource preemption - suspending lower-priority VMs to free up resources for higher-priority needs and resuming them later.
It uses optimizations such as reusing disk images across leases to minimize the impact of preparation overhead and schedules runtime overheads (like suspending, resuming, and migrating VMs) efficiently.
Scheduling is based on a resource slot table, representing all physical nodes managed by Haizea over time.
Best-effort leases are managed using a first-come-first-serve queue with backfilling, optimizing queue-based systems.
AR leases utilize a greedy algorithm for selecting physical resources to minimize preemptions.

There are ongoing efforts to extend OpenNebula's capabilities, including the implementation of the libvirt interface, VM consolidation schedulers for minimizing energy consumption, and tools for service elasticity management, VM placement, public cloud interface support, and policy-driven dynamic placement optimization.

Haizea shows that VM-based approaches with suspend/resume capabilities can address utilization issues typically associated with advance reservation (AR) use.

```

[Back to Blog Chapters](#blog-chapters)

##### Xen and the Art of Virtualization
```
Xen, a high-performance, resource-managed VMM that effectively allows multiple commodity operating systems to share hardware resources safely and efficiently

Advantages: isolation of virtual machines to prevent adverse performance effects, accommodation of diverse operating systems, and minimization of performance overhead introduced by virtualization

- targeting existing application binary interfaces (ABIs) and supporting the virtualization of complex server configurations within a single guest OS instance. full multi-application operating systems, and the necessity of paravirtualization for high performance on complex architectures like x86.

 resource virtualization: ensures both correctness and performance by allowing guest operating systems visibility of real as well as virtual resources.  better support for time-sensitive tasks and optimal use of physical resources like superpages or page coloring. 

Memory Management:
Segmentation and Paging: Guest OSes are responsible for managing hardware page tables with minimal Xen involvement for safety. Xen resides in a 64MB section at the top of every address space to avoid TLB flush when switching to and from the hypervisor. Segmentation is virtualized by validating updates to hardware segment descriptor tables.
Guest OS Restrictions: Guest OSes cannot install fully-privileged segment descriptors and must not overlap with the top end of the linear address space. They have direct read access to hardware page tables but updates are batched and validated by Xen.


CPU:
Privilege Levels: Guest OSes run at a lower privilege level than Xen to protect the hypervisor from OS misbehavior and ensure domain isolation. x86's four privilege levels are leveraged, with guest OSes typically moved from ring 0 to ring 1.
Exceptions and System Calls: Guest OSes must register descriptor tables for exception handlers with Xen. They may install a ‘fast’ handler for system calls to avoid indirecting through Xen for every call. Page faults and system calls are the most performance-critical exceptions.

Device I/O:
Virtual Devices: Instead of emulating hardware devices, Xen introduces efficient and simple device abstractions. Data transfer is done using shared-memory, asynchronous buffer-descriptor rings, allowing Xen to perform validation checks.
Event System: Hardware interrupts are replaced with a lightweight event system. Xen supports a mechanism for sending asynchronous notifications to a domain, similar to hardware interrupts, but with more control and efficiency.

Porting Operating Systems to Xen: Code Modifications in architecture-independent sections, virtual network drivers, block-device drivers, and Xen-specific (non-driver) areas. Both operating systems required substantial alterations in their architecture-specific sections. 

Role of Domain0: A domain created at boot time, termed Domain0, is responsible for hosting application-level management software. It uses the control interface to manage the system, including creating and terminating other domains and controlling their resources.

Control Transfer: Hypercalls and Events:

Hypercalls: Synchronous calls from a domain to Xen, analogous to system calls in conventional OSes. Used for operations like requesting page-table updates.
Event Mechanism: Asynchronous notifications from Xen to domains, replacing traditional interrupt delivery mechanisms. Events are used for lightweight notifications like domain-termination requests or indicating that new data has been received over the network.
Data Transfer: I/O Rings:

Efficient data transfer mechanism that allows data to move vertically through the system with minimal overhead. The design focuses on resource management and event notification.
I/O rings are circular queues of descriptors allowing asynchronous, out-of-band data transfer between Xen and guest OSes. They support a variety of device paradigms and enable efficient zero-copy transfer.

Subsystem Virtualization:
CPU Scheduling: Xen employs the Borrowed Virtual Time (BVT) scheduling algorithm for domain scheduling, ensuring low-latency wake-up and fair resource sharing.
Time and Timers: Xen provides notions of real time, virtual time, and wall-clock time to guest OSes. Domains can program alarm timers for real and virtual time, with timeouts delivered using Xen's event mechanism.

Virtual Address Translation: Xen avoids the overhead of shadow page tables by allowing guest OSes to manage hardware page tables directly, with Xen involved only in updates validation. This approach minimizes the number of hypercalls required for page table management.

Physical Memory: Memory is statically partitioned between domains, providing strong isolation. Guest OSes can adjust their memory usage dynamically, with mechanisms like the balloon driver in XenoLinux facilitating this interaction.

Network: Xen introduces the concept of a virtual firewall-router (VFR), with each domain having virtual network interfaces (VIFs) attached to the VFR. Domains transmit packets via enqueueing buffer descriptors on I/O rings, and Xen handles packet reception efficiently by exchanging packets directly with page frames.
Disk: Only Domain0 has direct access to physical disks. Other domains access persistent storage through virtual block devices (VBDs). Disk requests are batched and serviced in a round-robin fashion by Xen, ensuring fair access and good throughput.

Advantages of Delegating Domain Construction to Domain0:
Reduced Hypervisor Complexity: Building a domain within Domain0 rather than entirely within Xen simplifies the hypervisor's design, focusing its functionality on core tasks and leaving the domain setup process to a more specialized and capable component.

Improved Robustness: The process of setting up a new domain involves numerous delicate operations. Performing these operations within Domain0 allows for better error checking and handling.

Metrics:
Performance Comparison with Other Virtualization Technologies
Efficient Data Transfer and Subsystem Virtualization
Performance Isolation
Scalability
Network Performance
Concurrent Virtual Machines
Microbenchmarks 

Key points:
Efficient Paravirtualization
Resource Management and Performance Isolation
Scalability
Minimal Performance Overhead
Generality of the Interface
Facilitation of Network-Centric Services
```

## ** Chapter 3: Encapsulation ** <a name="topic-3"></a>
##### Options available for encap
- Isolation: others can't read/write your data
- - can't impact your performance (DOS)
- Processes may need memory sharing, get resource alloc, has own address space
1. Bare metal
```
+: good isolation, performance, very good software freedom
-: granularity limitation
```
2. Process: just application

3. Containers: with applicaiton & library/fs/etc that looks like an OS
```
+: performance, decent software freedom
-: security issues
```

4. Virtual Machines: physical machine-like software container
```
HW <-> VMM <-> (OS, Library/fs/etc/, App)
+: isolation properties, good software freedom
-: performance overhead, imperfect performance isolation

```

##### Comparison
- Process, Container, VM, BM
- Lower management <-> Better isolation/fidelity


##### interface is the key
- between APP/Library/OS/HW

##### Linux namespaces
- pid, gid, ...
##### resouece alloc: linux groups

##### Limitation of containers
- share the same host OS, security + flexibility(have to work with this OS)


##### Virtualization
- VM: vm OS <--virtual machine interface--> VMM <--machine interface--> HW
- The interfaces can be different

##### HW Virtualization Principles
- Fidelity, software operation keeps identical
- Isolation, guest cannot affect other guest/VMM
- Performance, most operations execute natively

#### Ecap Principle
- Basic: Execute VM softare in de-privileged mode
- - prevent privileged instr from escaping containment

```
Privilege Level
App User/Ring 3
Lib User/Ring 3
OS  Supervisor/Ring-o
HW


Option 1
Privilege Level
App User/Ring 3
Lib User/Ring 3
OS  User/Ring-3
VMM Supervisor/Ring-0
HW
Overhead - Isolation, VMM has to deal with operations from OS.. not ideal

Option 2
Privilege Level
App User/Ring 3
Lib User/Ring 3
OS  Ring-1
VMM Supervisor/Ring-0
HW

----> later
Option 3
Privilege Level
App User/Ring 3*
Lib User/Ring 3*
OS  Ring-0* (don't have all power, cannot change fundamental page tables..)
VMM Supervisor/Ring-0
HW
```
##### Priv Ops
- Most executions run directly
- VNM needs to handle OS attemps that execute priv ops, Non-CPU devices..
- - popf, pushf...

```
Handle: 
Trap & emulate: VMM handles
Static software re-writing/paravirtualization: rewrite guest OS to leverage VMM hypercalls (performance, sacrificing transparancy)
Dynamic software re-writing: VMM re-write guest's privileged code - coalescing traps, VMM complex, good performance

```

##### Handling memory: process
- OS: page mapping from virtual page to physical page
- User process: typically continuous address space
```
Mutiple OS:
Guest OSes manage: Mapping guest virtual to guest physical
VMM manages guest physical to host physical

other split: cores, time sharing..

SW: Shadow page tables
HW: Extended page tables(EPT)
```

##### Virtualization Devices
- Could be all virtualized (trap accesses, and emulate), but worse performance
```
For performance
1. Mapping, control given to a host (hw support)
2. Partition, (disk..)
3. Guest enhancement (special VM -> VMM calls)
4. Virtualized-enhanced devices (NICs with VMDq)

```

##### Security issues of virtualization/containerization
- meltdown
- bitflip(row hammer)



[Back to Blog Chapters](#blog-chapters)


## ** Chapter 4: Encapsulation ** <a name="topic-4"></a>



##### Map Reduce
```
Core Concepts of MapReduce:

The MapReduce model involves two primary functions: the map function and the reduce function.
The map function takes key/value pairs as input and produces a set of intermediate key/value pairs.
The reduce function then merges all intermediate values associated with the same intermediate key.

Automatic Parallelization and Distribution:
MapReduce abstracts the complexity of parallelization, fault-tolerance, data distribution, and load balancing

Scalability and Ease of Use:
The MapReduce implementation can process vast amounts of data (many terabytes) across thousands of machines.
It has a user-friendly nature, evidenced by the extensive use within Google, with hundreds of MapReduce programs implemented and thousands of jobs executed daily.

Motivation and Design Philosophy:
The motivation behind MapReduce stemmed from the need to process large sets of raw data (like web documents, request logs) into computed forms (like inverted indices, summaries) efficiently.
MapReduce was designed to simplify the computations by abstracting the complex code required for parallelization, data distribution, and fault tolerance into a library.

Programming Model:
The computation in MapReduce takes input key/value pairs and produces output key/value pairs, expressed through Map and Reduce functions defined by the user.
It handles the grouping of intermediate values by keys and provides these to the Reduce function, which then merges the values per key.

Examples and Applications:
The abstract and subsequent sections provide practical examples, like counting the number of occurrences of each word in a document collection, showcasing the model's applicability to real-world tasks.
It also highlights the versatility of MapReduce in tasks such as distributed grep, counting URL access frequency, reversing web-link graphs, and computing term-vectors per host.
Inverted Index: A fundamental operation in document searching where the map function processes documents, emitting word and document ID pairs, and the reduce function groups these by word, creating a list of document IDs for each word.
Distributed Sort: A sorting operation where the map function extracts keys from records and the reduce function sorts these keys, relying on the system's partitioning and ordering capabilities.

Implementation and Execution Overview:
designed for large clusters of commodity PCs, considering the typical hardware specifications and the nature of the network and storage systems.
The execution process involves dividing the input data into splits, assigning map and reduce tasks to workers, processing the data through the user-defined map and reduce functions, handling intermediate data storage and retrieval, and finally producing the output after all tasks are completed.

Fault Tolerance:
Fault tolerance is a critical aspect, given the scale of operation and the likelihood of machine failures. The system handles worker failures by reassigning tasks and redoing work if necessary. The master node monitors worker status and orchestrates the reassignment of tasks as needed.
The system is designed to ensure that, despite failures, the output is consistent with what would be produced by a faultless, sequential execution, as long as the map and reduce functions are deterministic.

Master Data Structures:
The master node maintains data structures to keep track of the status of each task and the locations of intermediate data. This information is crucial for coordinating the work of map and reduce workers and for ensuring that data is correctly routed through the system.


Locality
Network Bandwidth Optimization:  importance of 
conserving network bandwidth, a relatively scarce resource in large computing environments.
Data Locality: The approach involves scheduling map tasks on machines that contain a replica of the input data, or at least are network-local to the data. This strategy significantly reduces network bandwidth usage as most data is read locally.

Granularity of Map and Reduce Phases: The map phase is subdivided into M pieces, and the reduce phase into R pieces. Ideally, M and R should be much larger than the number of worker machines.
Dynamic Load Balancing and Recovery: This setup improves dynamic load balancing and expedites recovery from worker failures, as the tasks a worker has completed can be redistributed across other workers.
Handling Stragglers: machines that take unusually long to complete tasks. Causes for stragglers include hardware issues, resource contention, or bugs.

Custom Partitioning: While a default partitioning function based on hashing is provided, users have the option to specify a custom partitioning function, allowing more control over how data is partitioned across reduce tasks.

Intermediate Key/Value Pair Ordering: The framework ensures that within a partition, intermediate key/value pairs are processed in increasing key order. This ordering guarantee is particularly beneficial when the output needs to be sorted or efficiently accessible by key.

Data Aggregation at Mapper Nodes: The Combiner function allows for partial merging of intermediate data on the mapper nodes before sending it over the network. This feature is especially useful when there's significant repetition in the intermediate keys, and the reduce function is commutative and associative.

Flexible Data Formats: The MapReduce library supports various formats for input data, providing flexibility to handle different types of data sources.
```


##### Spark

```
Spark, a new cluster computing framework designed to address the limitations of MapReduce and its variants in handling certain types of applications, particularly those involving iterative operations and interactive data analysis. 

- Limitations of MapReduce and Similar Systems:
they are primarily built around an acyclic data flow model. This model is not suitable for applications that need to reuse a working set of data across multiple parallel operations.

Iterative Jobs: Common in machine learning, these jobs require applying a function repeatedly to the same dataset. MapReduce's need to reload data from disk for each iteration causes significant performance penalties.

Interactive Analytics: Users often run ad-hoc queries on large datasets using SQL interfaces. With MapReduce, each query incurs high latency because it operates as a separate job and needs to read data from disk.

Spark aims to support applications with working sets while retaining the scalability and fault tolerance characteristics of MapReduce.
The key abstraction in Spark is the resilient distributed dataset (RDD), a read-only collection of objects partitioned across a set of machines. RDDs can be explicitly cached in memory and reused in multiple parallel operations, significantly improving performance for certain types of applications.


Fault Tolerance through Lineage:
RDDs are fault-tolerant, using a concept called lineage. If a partition of an RDD is lost, the system has sufficient information on how the RDD was derived from other RDDs to rebuild just the lost partition.

Implementation and Usability of Spark:
Spark is implemented in Scala, allowing for a high-level, statically typed, and functional programming interface. It can also be used interactively, which is a novel feature for a system of this kind, enabling users to process large datasets on a cluster interactively.

Spark can significantly outperform Hadoop in iterative machine learning workloads and provide interactive querying capabilities with sub-second latency for large datasets.



- Programming Model
Spark introduces two primary abstractions for parallel programming: resilient distributed datasets (RDDs) and parallel operations on these datasets, and shared variables.

RDDs are read-only collections of objects partitioned across machines, capable of being rebuilt if a partition is lost.
RDDs can be created directly from a file in a shared filesystem like HDFS, by parallelizing a collection in the driver program, or by transforming an existing RDD.
RDDs are lazy and ephemeral by default, meaning they are computed on demand and not stored persistently after use.
However, users can modify this behavior using two actions:
The cache action suggests keeping the dataset in memory after its initial computation for future reuse.
The save action evaluates and persists the dataset to a distributed filesystem, like HDFS.
This approach to persistence is a design choice in Spark to ensure continued operation under memory constraints or node failures, drawing a parallel to the concept of virtual memory.

Parallel Operations
Operations on RDDs:
Spark supports various parallel operations on RDDs, such as reduce (combining elements using a function), collect (sending all elements to the driver program), and foreach (applying a function to each element for its side effects).

Shared Variables
Handling Variables in Parallel Operations:
When parallel operations like map and filter are performed, the closures (functions) used can refer to variables in their creation scope. By default, these variables are copied to each worker node.

Types of Shared Variables:
Spark introduces two types of shared variables for common usage patterns:
Broadcast Variables: Used for distributing large, read-only pieces of data efficiently across workers.
Accumulators: These are "add-only" variables for workers and readable only by the driver, suitable for implementing counters and parallel sums.


-Text Search
The first example is a simple text search to count the number of lines containing "ERROR" in a large log file.
The process involves creating an RDD from the file, filtering lines containing "ERROR", mapping each line to 1, and then reducing by summing these ones.
This example illustrates Spark's lazy evaluation and in-memory data sharing capabilities, which allows for efficient data processing without materializing intermediate datasets.


- Logistic Regression
This program demonstrates an iterative machine learning algorithm, logistic regression, which benefits significantly from Spark's ability to cache data in memory across iterations.
The program reads points from a file, caches them, and then iteratively updates a vector w using a gradient computed in parallel across the points.
The use of accumulators for summing the gradient and the syntax of Spark make the code resemble an imperative serial program while being executed in parallel.

- Alternating Least Squares (ALS)

Implementation
The section details Spark's implementation, including its reliance on Mesos for cluster management, the structure of RDDs, task scheduling for parallel operations, handling of shared variables, and integration with the Scala interpreter.

Resilient Distributed Datasets (RDDs): RDDs are implemented as a chain of objects capturing their lineage, allowing efficient recomputation in case of node failures. Different types of RDDs (e.g., for files or transformed datasets) implement a standard interface for partitioning, iteration, and task scheduling.
Parallel Operations and Task Scheduling: Spark creates tasks for each RDD partition and tries to schedule them based on data locality. It uses a technique called delay scheduling for efficiency.
Handling of Shared Variables: Broadcast variables and accumulators are implemented with custom serialization formats to ensure efficient distribution and fault tolerance.
Interpreter Integration: Spark integrates with the Scala interpreter, allowing interactive processing of large datasets. Modifications were made to ensure that closures and state are correctly serialized and distributed to worker nodes.

5 Results
Logistic Regression Performance: Spark significantly outperforms Hadoop in iterative machine learning workloads, with up to 10x faster performance due to data caching.
Alternating Least Squares (ALS) Performance: The use of broadcast variables for distributing the ratings matrix results in substantial performance improvements in the ALS job.
Interactive Spark Usage: Spark enables interactive querying of a large dataset with sub-second response times after initial data loading, providing a much faster and more interactive experience than Hadoop.


```

[Back to Blog Chapters](#blog-chapters)
