<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ychen884.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ychen884.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-13T08:11:05+00:00</updated><id>https://ychen884.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Compiler Design Notes - Currently Upating…</title><link href="https://ychen884.github.io/blog/2024/15661/" rel="alternate" type="text/html" title="Compiler Design Notes - Currently Upating…"/><published>2024-01-30T11:00:00+00:00</published><updated>2024-01-30T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2024/15661</id><content type="html" xml:base="https://ychen884.github.io/blog/2024/15661/"><![CDATA[ <h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h3 id="paper-reviews">Paper reviews</h3> <p>I will upload my paper reviews here later.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 0: Overview of Compiler Design</a></li> <li><a href="#topic-1.1">Chapter 1: Instruction Selection</a></li> <li><a href="#topic-2">Chapter 2: Register Allocation</a></li> <li><a href="#topic-3">Chapter 3: Elaboration(after par/lex..)</a></li> <li><a href="#topic-4">Chapter 4: Static Semantics</a></li> </ol> <h2 id="chapter-1-overview-of-compiler-design--">**Chapter 1: Overview of Compiler Design ** <a name="topic-1"></a></h2> <h3 id="what-makes-a-good-compiler-metrics">What makes a good Compiler: metrics</h3> <ul> <li>correctness</li> <li>code quality: compiled code runs fast</li> <li>efficiency: compilation runs fast</li> <li>usability: provides errors/warnings, …</li> </ul> <h3 id="compiler-design">Compiler Design</h3> <ul> <li>structure compilers</li> <li>applied alg &amp; data structures</li> <li>focus on sequential imperative programming languages</li> <li> <ul> <li>not functional, parallel, distributed, OOP…</li> </ul> </li> <li>code generation and optimization</li> </ul> <h3 id="organizing-a-compiler">Organizing a compiler</h3> <h4 id="front">Front</h4> <ul> <li>split work into different phases</li> <li>Lexical analysis -&gt; Token stream</li> <li>Parsing -&gt; Abstract syntax tree (mark body of while loop…)</li> <li>Sementic analysis (type check, variable initialization)</li> </ul> <h4 id="middle">Middle</h4> <ul> <li>IR(intermediate representation) Generation -&gt; Intermediate representations</li> <li>Optimize (most challenging)</li> </ul> <h4 id="back">Back</h4> <ul> <li>Instruction selection -&gt; Abstract assembly</li> <li>Register allocation -&gt; ASM Middle and Back has unclear distinctions</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-11-instruction-selection--">**Chapter 1.1: Instruction Selection ** <a name="topic-1.1"></a></h2> <ul> <li>Compiler phase</li> <li>IR tree -&gt; abstract assembly</li> </ul> <p>Example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = 5
return x+x+x*2

-&gt;&gt;&gt; Instruction selection
x = 5
temp1 = x + x 
temp2 = x * 2
ret_reg = t1 + t2
ret
</code></pre></div></div> <h5 id="ir-tree-more-expressions-statements">IR tree (more expressions, statements..)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Programs p ::= s1,...sn (sequence of statements)
statements s ::= x = e 
                return e

Expressions:
e ::= c int const
      x variable
      e1 ⊕ e2 binary OP (nested)
      ⊕ ::= +1 * 1 / 1 ...
</code></pre></div></div> <h5 id="abstract-assembly-flat">Abstract Assembly (flat)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Program: p ::= i1, ... in (instructions)
Instructions i::= d &lt;- s move
                = d &lt;- s1 ⊕ s2 bin op (sometimes one of the source works as dst)
                = ret return
Operands:
    d,s ::= r register (usually* finite numbers as defined)
          = c int const
          = t temps (variables)
          = x var

</code></pre></div></div> <h5 id="translations-expr">Translations Expr</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>translate(e1 ⊕ e2) = translate(e1); translate(e2);
res1 ⊕ res2?
Better: 
trans(d,e): seq of instructions that stores value of e in destination d. 


e           trans(d,e)
x           d &lt;- x
c           d &lt;- c
e1 ⊕ e2     trans(t1, e1), trans(t2, e2), d&lt;-t1⊕t2, (t1 and t2 are fresh temps)


</code></pre></div></div> <h5 id="translate-statements">Translate statements</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>trans'(s): seq of instru that inlements s
s           trans'(s)
x = e       trans(x,e)
return e    trans(ret,e) return (ret: return register)
</code></pre></div></div> <h5 id="example">Example</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IR prog: 
z = (x + 1) * (y * 4)
return z

trans'(p) 
= trans'(z = (x + 1) * (y * 4)), trans'(return z)
= trans(z,(x + 1) * (y * 4)),trans(ret, z), return
= trans(t1,x+1), trans(t2,y * 4), z&lt;- t1 * t2, ret&lt;-z, return
= t3 &lt;- x, t4 &lt;- 1, t1 &lt;- t3 + t4, t5 &lt;- y, t6 &lt;- 4, t2 &lt;- t5 * t6, z &lt;- t1*t2, ret&lt;-z, return
Optimize? directly use x and y instead of moving them to temps

</code></pre></div></div> <h5 id="how-to-improve">How to improve</h5> <ol> <li>Add special cases: for example c ⊕ e2</li> <li>Optimization pass after the first pass of translation (common approach)</li> <li>Different translation</li> </ol> <h5 id="constant-propagation">Constant propagation</h5> <ul> <li>goal: eliminate move t &lt;- c, p by replacing t with c in p</li> <li>But: stop replacing t if it’s written again <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example: 
t &lt;- 4
x &lt;- t+1   ---&gt; x &lt;- 4+1 
t &lt;- 5
ret &lt;- t   --NO--&gt; ret &lt;- 4
return 
</code></pre></div> </div> </li> </ul> <h5 id="copy-propagation">Copy propagation</h5> <ul> <li>goal: elim move d &lt;- t,p by replacing d with t in p, But: step replacing if d is written or if t is written</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

t &lt;- 5+1
d &lt;- t 
x &lt;- d+1 ----&gt; x &lt;- t+1
t &lt;- 5+2
ret &lt;- d+1 ---No---&gt; ret &lt;- t + 1
ret
</code></pre></div></div> <h5 id="static-single-assignment-form">Static Single Assignment Form</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- every temp is assigned at most once
- don't have to check "writes" but can replace all occurances in propagations
- Conversion to SSA -&gt; user version nums

t &lt;- 5+1
d &lt;- t 
x &lt;- d+1 ----&gt; x &lt;- t+1
t &lt;- 5+2
ret &lt;- d+1 ----No---&gt; ret &lt;- t + 1
ret

-----&gt;&gt;

t0 &lt;- 5+1
d0 &lt;- t0
x0 &lt;- d0+1
t1 &lt;- 5+2
...
</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-2-register-allocation--">**Chapter 2: Register Allocation ** <a name="topic-2"></a></h2> <ul> <li>Goal: assign registers and stack locations to temps <h5 id="x86-64-16-registers-no-temps">X86-64: 16 registers, no temps</h5> </li> <li>stack locations, when keeping track of more variables than registers <h5 id="strategy">Strategy</h5> <ol> <li>Store all temps on the stack (CON: inefficient, still need registers for efficiency)</li> </ol> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IR trees(simplified syntax tree) 
--&gt; Instruction selection --&gt; ASM 
--&gt; reg alloc --&gt; ASM
--&gt; x86 asm


Example: 
d &lt;- s1 ⊕ s2
-&gt; reg alloc

rlld &lt;- exd * 4(rsp)
</code></pre></div></div> <h5 id="difficulty-x86-has-15-gen-purpose-registers">Difficulty: x86 has 15 gen purpose registers</h5> <ul> <li>Goal: assign each variable a register</li> <li>may have to use stack locations and clever use of registers for variables</li> </ul> <h5 id="interference">Interference:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x &lt;- 14
y &lt;- 15 
z &lt;- x+y

x &lt;- 14
y &lt;- 15 + x 
ret &lt;- 4+y
ret
if x is not used again, we can use overwrite the register for y
</code></pre></div></div> <h5 id="rigth-ir-for-reg-alloc">Rigth IR for reg alloc?</h5> <h5 id="3-addr-abs-asm">3 addr abs asm</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d &lt;- s1 + s2
d &lt;- s1 / s2
...
</code></pre></div></div> <h5 id="2-addr-abs-asm">2 addr abs asm</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d &lt;- s1
d &lt;- d + s2

d &lt;- s1
d &lt;- d / s2
...

</code></pre></div></div> <h5 id="abstract-x86">abstract x86</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MOVL S1,d
ADDL S2,d
...
IDIVL
edx:eax / s2 -&gt; eax

MOVL s1, %eax
CLTD (sign-extends eax into edx:eax.)
----
https://stackoverflow.com/questions/17170388/trying-to-understand-the-assembly-instruction-cltd-on-x86
What this means in practice is that edx is filled with the most significant bit of eax (the sign bit). For example, if eax is 0x7F000000 edx would become 0x00000000 after cdq. And if eax is 0x80000000 edx would become 0xFFFFFFFF.
----
IDIVL s2 (edx:eax / s2)
MOVL %eax, d

</code></pre></div></div> <h5 id="reg-alloc-at-3-addr-assem">Reg Alloc at 3-Addr Assem</h5> <ul> <li>leave one register unassigned for later conversion (r11d)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For example

d &lt;- s1 ⊕ s2
--&gt; 
MOVL s1, %r11d
ADDL s2, %r11d
MOVL %r11d, d
(one of them has to be a register
this will always work, but may not be the optimzied solution)

</code></pre></div></div> <h5 id="reg-alloc-at-2-addr-assem">Reg Alloc at 2-Addr Assem</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For example

d &lt;- s1 + s2
--&gt; 
d &lt;- s1
d &lt;- s2 + d
</code></pre></div></div> <h5 id="how-to-allocate-registers">How to allocate registers?</h5> <ul> <li>Goal: minimize spilling to the stack</li> <li>One method: graph-based, greedy register allocation <ol> <li>Build interference graph ``` Vertices are registers and temps Edges indicate interference (need diff registers)</li> </ol> </li> </ul> <p>Example: t1 – t2 – t3 – eax | | ———–</p> <p>Complexity pf deciding if two temps interfere? Undecidable Why? Reduction to the halting problem x &lt;- 5 y &lt;- 6 complex code (doesn’t use x and y) ret&lt;- x + y return</p> <p>x and y interfere iff complex code terminates</p> <p>Approximate Interference with liveness Liveness: temp t is live at line l if t might be used in the following computation</p> <p>For L1: work backwards temp t is live at l if either of them is valid:</p> <ul> <li>t is read at l</li> <li>t is live at l+1 and not written at line l</li> </ul> <p>Example: x1 &lt;- 1 - x2 &lt;- 1 x1 x3 &lt;- x1 + x2 x2,x1 x4 &lt;- x3 + x2 x3,x2 x5 &lt;- x4 + x3 x3,x4 ret &lt;- x5 x5 return ret ret register</p> <p>life range of t1: 2-3 life range of x2: 3-4 ..</p> <p>t1 &lt;- 1 - t1 &lt;- t1+1 t1 t2 &lt;- t1 t1</p> <p>Construct interf graph: Option 1: Add edge between t1 and t2 if they have overlapping live ranges Option 2: rule 1: For every instruction d &lt;- s1 ⊕ s2 add an edge {d,t} if t is live at the next instruction rule 2: For every move d &lt;- s, add an edge {d,t} that t not in {s,d}, and is live at the next instruction</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>


2. Find coloring with as few colors as possible: Adjacent vertices have different colors
</code></pre></div></div> <p>Given: Graph G = (V,E) and k colors Question: Is there a coloring of V with k colors such that adjacent verticies have different colors Complexity: NP complete for k &gt;= 3</p> <p>What folllows for reg allocation?</p> <p>For a Turing-comp lang It’s undecidable for a prog if there’s an equiv prog that uses k registers (and no tem)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
3. Assign colors to registers and stack locations
</code></pre></div></div> <p>Greedy graph coloring: get a minimal coloring for most programs</p> <p>Assume colors are number 1,2,3…</p> <p>Let N(v) be the nbs of v Input: G(V,E) and ordering V=v1,…,vn Output: Coloring of V: col:V-&gt;{1,..k}</p> <ul> <li>Order matters</li> </ul> <p>x5</p> <p>x4-x3-x2-x1</p> <p>Order 1: x5,x4,x3,x2,x1,ret</p> <p>Order 2: x1,x4,x3,x2,x5,ret</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##### Th: There exists an ordering that produces an optimal coloring
</code></pre></div></div> <p>How to find optimal ordering for chordal graphs?</p> <p>Def: A graph is chordal if every cycle with &gt;= 4 vertices has a chord (edge between two verticies in the cycle that is not in the cycle)</p> <p>Example</p> <p>a - b | | c - d not chordal</p> <p>a - b | / | c - d chordal</p> <p>a - b | x | c - d chordal</p> <p>Intuition: how to get long cycle w/o chord? a over lap with b and c we want d to be not overlapping with a</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#####  Maximum cardinality search
</code></pre></div></div> <p>Input: G, Op: ordering of v for each vertex in V, set wt(v) &lt;- 0</p> <p>For all v ∈ V set wt(v) ← 0 Let W ← V For i ← 1 to n do Let v be a node of maximal weight in W Set vi ← v For all u ∈ W ∩ N(v) set wt(u) ← wt(u) + 1 Set W ← W \ {v}</p> <p>MCS Ordering returns: simplicial elimination ordering if G is chordal Def: v is simplicial in G in N(v) is a clique A simplicial vertex is one whose neighbors form a clique: every two neighbors are adjacent.</p> <p>Def: A simpl elim ordering is an ord v1, … vn st vi is simplicial in Gv1,..vi &lt;- subgraph induced by v1, .. vi (picked)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Theorem 1: The graph is chordal iff it has an simplicial elim ordering

</code></pre></div></div> <p>Proof..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##### Theorem 2: The greedy coloring alg finds an opt coloring if run with a simpl elim ordering
</code></pre></div></div> <p>Proof: Let k be the number of colors used Observations:</p> <ol> <li> <table> <tbody> <tr> <td>k &lt;= max</td> <td>Ni(vi)</td> <td>+ 1</td> </tr> </tbody> </table> </li> <li>#min colors &gt;= max|Ni(vi)| + 1 ```</li> </ol> <h5 id="--theorem-mcs-returns-a-sompl-elem-ordering-iff-g-is-chordal">-&gt; Theorem: MCS returns a sompl elem ordering iff G is chordal</h5> <ol> <li>Spill: assign remaining colors to stack loc.</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>strategies: 
1. number of uses of temps in code, higher use freq get the registers
2. incorp loop nestings
3. highest colors (high color may be used in less time, approx for 1)

</code></pre></div></div> <h4 id="summary">Summary</h4> <ol> <li>Build the interference graph (different ways)</li> <li>Order the vertices with MCS</li> <li>Color with greedy alg</li> <li>Spill if # colors &gt; 13</li> </ol> <h4 id="liveness-analysis--interence-rules">Liveness analysis &amp; interence rules</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>live(l,x) ~ x i live at line l
d != u
l: d &lt;- x ⊕ y live(l+1, u)
--------------------------
      live(l, u)


l: d&lt;- x ⊕ y
------------------
    live(l, x)
    live(l, y)

l: return
------------------
   live(l, ret)


l: x &lt;- c
live(l+1, u), x!=u
-------------------
      live(l,u)

l: x&lt;-y, u!=x
live(l+1, u)
--------------
live(l,u)


l:x&lt;-y
-------
live(l,y)

Using derivation tree to prove if x live at line l

</code></pre></div></div> <h5 id="general-saturation-alg">General Saturation Alg</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Can be used for all predicates
1: start with empty set of facts &lt;- derived predicates
2: pick arg (here: l and t) and a rule with live(l,t) in the conclusion
so that the premises are already facts
3: Repeat until no facts can be derived

Will always stop
</code></pre></div></div> <h5 id="refactoring-liveness-rules">Refactoring liveness rules</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use(l,x)
---------
live(l,x)

*: l' is a possible succesor
live(l'x) succ(l,l') ~def(l,x) 
---------------------------------
      live(l,x)

</code></pre></div></div> <h5 id="need-to-define">Need to define:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use(l,x) ~ x is used at l 
def(l,x) ~ x is def at l
succ(l,l') ~ l' can be a succ of l
</code></pre></div></div> <h5 id="example-1">Example</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>l: d &lt;- x ⊕ y
-------------
use(l,x)
use(l,y)
def(l,d)
succ(l,l+1)
</code></pre></div></div> <h5 id="adding-loops-and-conditionals">Adding loops and conditionals:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>i = l: d &lt;- x ⊕ y
.
.
.
l: goto l' (uncond jump)
l: if(x?c) then lt then lt else lf(cond jump)

New rules:

l: goto l'
-----------
succ(l,l')

l: if(x'c) then lt
else lf
-------------------
succ(l,lt)
succ(l,lf)
use(l,x)


Keep doing iterations until we cannot add more new to the liveness sets
pass 1, pass 2, ...

</code></pre></div></div> <h5 id="interference-graph">Interference graph</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Overlapping live ranges

live(l,x) live(l,y)
-------------------
inter(x,y)

But doesn't work in the presence of that code dead code

Example: 
a &lt;- 1
b &lt;- 2
ret &lt;- a
return
</code></pre></div></div> <ol> <li>Assignment based More sparse ``` l: x &lt;- y ⊕ z live(l+1, u), u!=x ——————————— inter(x,u)</li> </ol> <p>l:x&lt;-y, y!=u live(l+1, u) x!=u —————— inter(x,u)</p> <p>l: x&lt;-c, u!=x live(l+1, u) ————– inter(x,u)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## Chapter 3: Elaboration &lt;a name="topic-3"&gt;&lt;/a&gt;

##### Goal: minimal repr and clear of progr
1. remove syntactic sugar
2. make scope explicit


</code></pre></div></div> <p>Parsing -&gt; Parse tree -&gt; elaboration -&gt; AST(&lt;-Semantic analysis)</p> <p>Example: turn for lops into while loops</p> <p>for(int x=4, x&lt;8128, x++){ y = y+x } ——&gt; elab use declaration(x,int,s) {int x=4; while(x&lt;8128)..} decl(x, int, while(x&lt;8218){..})</p> <p>// Before elaboration for (int x = 4; x &lt; 30; x++) { y = y + x }</p> <p>// After elaboration { int x = 4; while (x &lt; 30) { y = y + x; x += 1 } }</p> <p>abstract syntax</p> <p>declare(x, int,seq(assign(x, 4), while(x &lt; 30, seq(assign(y, y + x), assign(x, x + 1)))))</p> <p>The extra scope is necessary Rather than continuing to perform manipulations on surface syntax, we introduce the BNF of an elaborated abstract syntax</p> <table> <tbody> <tr> <td>Expressions e ::= n</td> <td>x</td> <td>e1 ⊕ e2</td> <td>e1 /o e2</td> <td>f(e1, . . . , en)</td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td>e1 ? e2</td> <td>!e</td> <td>e1 &amp;&amp; e2</td> <td>e1</td> <td> </td> <td>e2</td> </tr> </tbody> </table> <p>/0: potentially effectful operators (such as division or shift, which could raise an exception)</p> <p>⊕ for effect-free operators ? for comparison operators returning a boolean, !, &amp;&amp;, and || for logical negation, conjunction, and disjunction, respectively</p> <p>Statements s ::= declare(x, τ, s) | assign(x, e) | if(e, s1, s2) | while(e, s)| return(e) | nop | seq(s1, s2)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##### Inf rule
</code></pre></div></div> <tp> τ <ident> x <exp1> e1 <exp2> e2 <stmt1> s1 <stmt2> s2 ----------------------------------------------------- for (<tp> <ident> = <exp1>; <exp2>; <stmt1>) <stmt2> declare(x, τ,while(e1,seq(s2, s1))) When we want to translate a for loop that matches the pattern at the bottom, we have to do six things: elaborate the type, elaborate the identifier, elaborate both expressions, elaborate the afterthought, and then elaborate the loop body. x++ -&gt; assign(x, x + 1) make sure any nested for loops in <stmt2>, the body of our for loop, are elaborated ``` ##### WHy IR tree 1. isolate potentially effectful expressions, making their order of execution explicit.simplifies instruction selection and also means that the remaining pure expressions can be optimized much more effectively. 2. make the control flow explicit in the form of conditional or unconditional branches, which is closer to the assembly language target and allows us to apply standard program analyses based on an explicit control flow graph. ``` Pure Expressions p ::= n | x | p1 ⊕ p2 Commands c ::= x ← p | x ← p1 /0 p2 | x ← f(p1, . . . , pn) | if (p1 ? p2) then lt else lf | goto l | l : | return(p) Programs r ::= c1 ; . . . ; cn ``` ##### Translating Expressions ``` tr(e) = &lt;ˇe, eˆ&gt; where eˇ is a sequence of commands r that we need to write down to compute the effects of e and eˆ is a pure expression p that we can use to compute the value of e back up. tr(n) = &lt;·, n&gt; tr(x) = &lt;·, x&gt; tr(e1 ⊕ e2) = &lt;(ˇe1 ; ˇe2), eˆ1 ⊕ eˆ2&gt;? ``` ##### Translating Statements ``` tr(assign(x, e)) = ˇe ; x ← eˆ tr(return(e)) = ˇe ; return(ˆe) tr(nop) = · tr(seq(s1, s2)) = ˇs1 ; ˇs2 tr(if(e, s1, s2)) = ˇe ; if (ˆe != 0) then l1 else l2 ; l1 : ˇs1 ; goto l3 ; l2 : ˇs2 ; l3 : tr(while(e, s)) = l1 : ˇe; if (ˆe != 0) then l2 else l3 ; l2 : ˇs ; goto l1; l3 : ``` ##### Translating Boolean Expressions ``` cp(e1 ? e2, l, l') = ˇe1 ; ˇe2 ; if (ˆe1 ? eˆ2) then l else l' cp(!e, l, l') = cp(e, l', l) cp(e1 &amp;&amp; e2, l, l') = cp(e1, l2, l'); l2 : cp(e2, l, l') cp(e1 || e2, l, l') = left to the reader cp(0, l, l') = goto l' cp(1, l, l') = goto l cp(e, l, l') = ˇe ; if (ˆe != 0) then l else l' tr(if(b, s1, s2)) = cp(b, l1, l2) l1 : tr(s1) ; goto l3 l2 : tr(s2) ; goto l3 l3 : tr(e) = &lt;cp(e, l1, l2); l1 : t ← 1 ; goto l3 l2 : t ← 0 ; goto l3 l3 : , t&gt; ``` ##### Extended basic blocks ``` instead of basic blocks being sequences of commands ending in a jump, they are trees of commands that branch at conditional statements and have unconditional jumps (goto or return) as their leaves ``` ## Chapter 4: Static Semantics <a name="topic-4"></a> ##### abstract syntax ``` After lexing and parsing, a compiler will usually apply elaboration to translate the parse tree to a high-level intermediate form often called abstract syntax. Then we verify that the abstract syntax satisfies the requirements of the static semantics. C0: • Initialization: variables must be defined before they are used. • Proper returns: functions that return a value must have an explicit return statement on every control flow path starting at the beginning of the function. • Types: the program must be well-typed. ``` ##### Abstract Syntax as defined in last chap ##### Def ``` we would like to raise an error if there is a possibility that an unitialized variable may be used. may-property use(e1/2, x) ----------- use(e1 &amp;&amp; e2, x) use(e1/2, x) ----------- use(e1 ⊕ e2, x) must-property def(s, x) if the execution of statement s will define x. -------------------- def(assign(x, e), x) def(s1, x) def(s2, x) ----------------------- def(if(e, s1, s2), x) A conditional only defines a variable if is it defined along both branches, and a while loop does not define any variable (since the body may never be executed). def(s1/2, x) ----------------- def(seq(s1, s2), x) def(s, x) y != x --------------------- def(decl(y, τ, s), x) x is declared at most once on a control-flow path. -------------------- def(return(e), x) Since a return statement will never pass the control flow to the next instruction, any subsequent statements are unreachable. It is therefore permissible to claim that all variables currently in scope have been defined. ``` ###### Liveness ``` We observe that liveness is indeed a may-property, since a variable is live in a conditional if is used in the condition or live in one or more of the branches. use(e, x) ---------------- live(assign(y, e), x) use(e, x) ---------------- live(if(e, s1, s2), x) use(s1, x) ---------------- live(if(e, s1, s2), x) use(s2, x) ---------------- live(if(e, s1, s2), x) use(e, x) ---------------- live(while(e, s), x) live(s, x) ---------------- live(while(e, s), x) use(e, x) ---------------- live(return(e), x) no rule for live(nop, x) live(x, s) y != x ---------------- live(decl(y, τ, s), x) live(s1, x) ---------------- live(seq(s1, s2), x) ¬def(s1, x) live(s2, x) ---------------- live(seq(s1, s2), x) ``` ###### Initialization ``` captures the general condition. should be read from the premises to the conclusion. decl(x, τ, s) in p live(s, x) ------------------------------ error from the conclusion to the premises ------------------------------ init(nop) init(s1) init(s2) ------------------------------ init(seq(s1, s2)) init(s) ¬live(s, x) ------------------------------ init(decl(x, τ, s)) ``` ##### From Judgments to Functions ``` init : stm → bool init(nop) = T init(seq(s1, s2)) = init(s1) ∧ init(s2) init(decl(x, τ, s)) = init(s) ∧ ¬live(s, x) live(nop, x) = ⊥ live(seq(s1, s2), x) = live(s1, x) ∨ (¬def(s1, x) ∧ live(s2, x)) live(decl(y, τ, s), x) = y != x ∧ live(x, s) . . . ... init(δ, s, δ'): assuming all the variables in δ are defined when s is reached, no uninitialized variable will be referenced and after its execution all the variables in δ' will be defined. use(δ, e): e will only reference variables defined in δ. δ |- s ⇒ δ' for init(δ, s, δ'). δ |- e for use(δ, e). δ ⊢ s1 ⇒ δ1 δ1 ⊢ s2 ⇒ δ2 ______________ δ ⊢ seq(s1, s2) ⇒ δ2 δ ⊢ e _________ δ ⊢ assign(x, e) ⇒ δ ∪ {x} δ ⊢ e δ ⊢ s1 ⇒ δ1 δ ⊢ s2 ⇒ δ2 _________________ δ ⊢ if(e, s1, s2) ⇒ δ1 ∩ δ2 δ ⊢ e δ ⊢ s ⇒ δ' _________ δ ⊢ while(e, s) ⇒ δ In particular, declare(x, τ, s) means that the variable x is declared (only) within the statement s. δ ⊢ s ⇒ δ' ___________ δ ⊢ decl(y, τ, s) ⇒ δ' - {y} δ ⊢ e _________ δ ⊢ return(e) ⇒ {x | x in scope} Typing Judgment for Statements: The notation Γ ⊢ s : [τ] is a typing judgment for statements, where Γ is the type environment (context), s is a statement, and τ is the type. This notation signifies that the statement s is well-typed within the context Γ and ultimately returns a value of type τ ``` [Back to Blog Chapters](#blog-chapters) </stmt2></stmt2></stmt1></exp2></exp1></ident></tp></stmt2></stmt1></exp2></exp1></ident></tp>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Compiler Design Notes - SCS]]></summary></entry><entry><title type="html">Distributed Systems Notes - Currently Upating…</title><link href="https://ychen884.github.io/blog/2024/15640/" rel="alternate" type="text/html" title="Distributed Systems Notes - Currently Upating…"/><published>2024-01-18T11:00:00+00:00</published><updated>2024-01-18T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2024/15640</id><content type="html" xml:base="https://ychen884.github.io/blog/2024/15640/"><![CDATA[<h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h3 id="paper-reviews">Paper reviews</h3> <p>I will upload my paper reviews here later.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 1: Remote Procedure Call (RPC)</a></li> <li><a href="#topic-2">Chapter 2: Caching</a></li> <li><a href="#topic-3">Chapter 3: </a></li> <li><a href="#topic-4">Chapter 4: </a></li> <li><a href="#topic-5">Chapter 5: </a></li> <li><a href="#topic-6">Chapter 6: </a></li> <li><a href="#topic-7">Chapter 7: </a></li> </ol> <h2 id="-chapter-1-remote-procedure-call-rpc--">** Chapter 1: Remote Procedure Call (RPC) ** <a name="topic-1"></a></h2> <ul> <li>Try to fake procedure call to local programming</li> <li>Why? bring down programming complexity for distributed systems</li> <li>client-server model, per interface</li> <li>two aspects: control flow, invocation syntax</li> <li>with network delays (theoretically best at speed of light)</li> </ul> <h5 id="limitations-of-rpc">limitations of RPC</h5> <ol> <li>No address space sharing between client and server, can’t sharing pointers(call by reference), can’t share global data..</li> <li>Delayed binding in RPC</li> </ol> <h5 id="failure-independence">Failure independence</h5> <ul> <li>caller and callee live and die together in local setup</li> <li>we can witness failure case but hard to do in local</li> <li>failure handling consider visibility of failure</li> <li>Security: different domains</li> </ul> <h5 id="typical-rpc">Typical RPC</h5> <ul> <li>client: makerpc(request_packet, &amp;reply_packet): blocks until reply or failure</li> <li>server: getrequest(&amp;request_packet) blocks until receives request, sendresponse(reply_packet)</li> </ul> <h5 id="stub-routines">Stub routines</h5> <ul> <li>generated by stub generator</li> <li>sit between high level purpose and low level network packing/unpacking send/recv.. <h6 id="procedure">procedure:</h6> <p>``` -&gt; : local procedure call –: network communication</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Client                                             Server App -&gt; Stub -&gt; Transport --Network Com--- Transport -&gt; Stub -&gt; App
</code></pre></div> </div> </li> </ul> <p>App calls Stub Stub pack/unpack Transport transmit/receive Server App do actual work then return Packing and unpacking is usually not elastic in dev env Correctness and API design is more important</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Marshalling/Unmarshalling, Serialization/De-serialization

##### RPC packet format
1. Network transport header(Ethernet, IP, Transport(TCP/UDP))
2. RPC header
</code></pre></div></div> <p>RPC Version ID Opcode (Stub) Flags parameters + Len</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Stub with dynamically allocated buffer
- variable to indicate how many bytes in coming
- malloc for new memories


##### Failure independence of clients &amp; servers adds complexity
1. Outsourcing (for example, relying on TCP). TCP guarantees reliable, in-order, unlimited delivery.
</code></pre></div></div> <p>Pain:</p> <ul> <li>no preservation of write() boundaries</li> <li>data is re-framed in transit</li> <li>read may return fewer than number of bytes requested</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2. The buck has to stop somewhere. Do it yourself.
- Retransmission
- duplicate delivery/execution violates RPC semantics, sequence numbering to eliminate
- based on UDP



##### Timeout values in distributed systems
- statistics -&gt; reasonable
- no matter what, could be too soon

##### What does server do when receiving duplication happens?
- indicates one of these happening: 
1. reply lost
2. reply crossed retransmitted request
3. compute time was excessive
4. client too impatient.
5. ...


##### Knowlege at server is always stale relative to client, and vice versa
- processing time/network transmit time, indifferent to client..
- best for server to do is retrans
- should preserve reply, not re-compute, because computation can be substantial
- *my question: what if replies too large? maybe best effort

##### Exactly-once semantics
- How long to keep old replies and sequence numbers
- Rigorous interpretation of "RPC" -&gt; forever
- Server crashes: 
- - saved in non-volatile
- - server response has to be after non-volatile write
- - disk/flash latency per RPC
- - clean undo of partial computations before crash
- exactly-once RPC: 
- - success return -&gt; call exected exactly once
- - call blocks indefinitely, no failure return



##### In practice for RPC package: At-most-once semantics 
</code></pre></div></div> <p>Avoid indefinite blocking Declare timeout beyond certain delay: success or not? many timeout reasons…</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Slow Servers &amp; Long-Running Calls
</code></pre></div></div> <p>Solution: probes to check server health during long calls server responds busy while working essentially a keepalive mechanism</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Orphaned Computations
</code></pre></div></div> <p>network failure, then server continues, unaware its work is useless Orphan detection and extermination are difficult - but important</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### At-least-once semantics (strongest)
</code></pre></div></div> <p>Even simpler to implement Requires operation idempotency Idempotency is a property of certain operations or API requests, which guarantees that performing the operation multiple times will yield the same result as if it was executed only once.</p> <ul> <li>for example, read request on locked object or read-only object, current_stock_price(MSFT) ```</li> </ul> <h6 id="choice-of-semantics-less-strong">Choice of semantics (less strong)</h6> <ol> <li>Achieving exactly-once semantics <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>not provided in any real RPC package
requrie application-level dup elim
built on top of at-most-once RPC
- have to write on disk before replying
</code></pre></div> </div> </li> <li>At-most-once (mostly provided by RPC packages) ``` avoids: <ul> <li>transactional storage</li> <li>non-volatile storageo of replies and sequence #s</li> <li>indefinite storage of replies</li> </ul> </li> </ol> <p>if crashed, just crash without executing anything exactly once: have to undo, and do again using instruction in non-vol</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Safety and liveness properties
</code></pre></div></div> <p>safety: correct functionalty (“At most one entity can execute in a critical section”)</p> <ul> <li>bad things never happen</li> </ul> <p>liveness: characterizes timely execution progress (e.g., “This code is deadlock-free”)</p> <ul> <li>good things will eventually happen ```</li> <li>“Exactly-once semantics” -&gt; safety property</li> <li>Existence of timeout in at-most-once RPC -&gt; liveness property</li> </ul> <h5 id="placement-of-functionality---what-are-you-promising-vs-deliver">Placement of Functionality - what are you promising vs deliver</h5> <ul> <li>Protocol layering</li> </ul> <h5 id="tcp-for-rpc">TCP for RPC</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TCP timeout -&gt; reconnect
- new connection is unaware of old
- server need to do dup elim
- orphans still possible
- exactly-once RPC no easier with TCP

TCO simplifies at-most-once RPC
TCP hurts since it has independent acks in each direction


User TCP rather UDP:
- not simplify exactly-once imple
- worse performance in best case
- simplify at-most once impl

End-to-end argument:
For a given functionality:
- correctness is expressed relative to two endpoints (safety)
- implementation requires support of those two end points
- support below end points cannot suffice (may improve performance(liveness))

</code></pre></div></div> <h5 id="critical-question-where-to-place-function-in-a-distributed-system">Critical question: Where to place function in a distributed system?</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What guarantee promising
end point 
package
simplified implementation
guarantee properties
</code></pre></div></div> <h5 id="performance-of-distributed-system-delay">Performance of distributed system: delay</h5> <ol> <li>Processing delay</li> <li>Queueing delay &lt;- dominate</li> <li>Tranmission &lt;- usuallly small unless very large files</li> </ol> <h5 id="queueing">Queueing</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Any shared resources
Arrival pattern: uniform, poisson, ...

Service time also varies

---- without considering real world complications:----
1. Util
2. Latency
3. Freedom (how constrained of arrival discipline..) 
We can optimize at most 2 out of 3




</code></pre></div></div> <h5 id="latency-is-the-killer">Latency is the killer</h5> <h2 id="chapter-2-caching-">Chapter 2: Caching <a name="topic-2"></a></h2> <h5 id="metrics">Metrics</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Miss Ratio = Misses/References
Hit Ratio = Hits/References = (1 − Miss Ratio)
Expected cost of a reference = (Miss Ratio * cost of miss) + (Hit Ratio * cost of hit)
Cache Advantage = (Cost of Miss / Cost of Hit)
</code></pre></div></div> <h5 id="fopcus-distributed-file-systems">Fopcus: distributed file systems</h5> <ol> <li>Fetch policy ``` Full Replication? -Storage for entire subtree consumed on every replica -Significant update traffic on hot spots -Machines receive updates whether they care or not Coarse-grain, non-selective management of data</li> </ol> <p>A Much Better Approach: on-demand caching</p> <ul> <li>requires operating system modifications</li> <li>total application transparency</li> <li>enable demand caching</li> </ul> <p>Multi-OS On-Demand Caching</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2. Update propagation policy
3. Cache replacement policy ((prefetching))

##### Caching in the Real World
1.  Cost of remote data access often not uniform
2.  spatial locality
3.  Remote data more coarsely addressable than local
Fetch more than you need on miss

##### Spatial &amp; Temporal Locality


##### Update Propagation
</code></pre></div></div> <p>One-copy semantics • there are no externally observable functional differences • relative to same system without caching (or replication)</p> <p>This model aims to provide the illusion that although data may be replicated across multiple servers or nodes (to improve reliability, performance, and fault tolerance), users interact with the data as if there is only one copy.</p> <p>Challenges: Physical master copy may not exist Network may break between some users and master copy Intense read- and write-sharing across sites (The benefits of caching (reducing access time, decreasing bandwidth usage) are undermined in this scenario. The caches might spend more time synchronizing than serving the actual read and write requests, making them “effectively useless”)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Cache Consistency Strategies
1. Broadcast Invalidations
</code></pre></div></div> <p>Notification to All Caching Sites:every caching site in the network is notified, regardless of whether they actually have the cached object.</p> <p>Handling at Cache Sites: Upon receiving the invalidation notice, if a cache has the invalidated object, it will mark it as invalid.</p> <p>Strengths</p> <ul> <li>Strict One-Copy Semantics:</li> <li>Race Condition Prevention: If the updating process is blocked until all caches have invalidated the item, it prevents race conditions, ensuring that no stale reads occur.</li> <li>Simplicity</li> </ul> <p>Limitations</p> <ul> <li>Wasted Traffic:</li> <li>Blocking Updating Process:</li> <li>Scalability Issues: As the number of nodes in the system increases, the overhead of sending invalidations to every node and the corresponding acknowledgments becomes impractical. ```</li> </ul> <ol> <li>Check on Use: ``` Reader checks master copy before each use Has to be done at coarse granularity (e.g. entire file or large block) Whole file granularity-&gt; “session semantics”</li> </ol> <p>Advantages • strict consistency at coarse granularity • easy to implement, no server state • servers don’t need to know of caching sites</p> <p>“session semantics at open-close granularity,” where changes made to a file during a session (from open to close) are not visible to other clients until the session ends (the file is closed). principled weakening of strict one-copy semantics</p> <p>Strict One-Copy Semantics With Write-Sharing any write operation performed by any client or process in a distributed system is immediately visible to all other clients or processes.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##### Sharing Taxonomy

</code></pre></div></div> <p>Multiple concurrent read-only sessions “read sharing”</p> <p>Multiple concurrent read-only sessions + one read-write session “read-write sharing”</p> <p>Multiple concurrent read-write sessions “write-write sharing”</p> <ol> <li>“Last Close After Write Wins”: AFS</li> <li>“Raise Conflict Exception”: Coda File System</li> <li>“Live Happily and Be Blissfully Unaware”: NFS, Google Docs, DropBox, etc.</li> </ol> <p>3: How to do?</p> <p>“Last Close After Write Wins” Mechanism: This approach resolves write-write conflicts by accepting the changes made by the process that closes the file last. All previous writes to the file during the conflict period are overwritten by the last writer’s data. Pros: Simplifies conflict resolution by enforcing a clear rule. Cons: Can lead to data loss for earlier writes, as changes made by all but the last writer are discarded. Use Case: AFS uses this method, prioritizing simplicity and predictability over the preservation of every change</p> <p>“Raise Conflict Exception” Mechanism: When a write-write conflict is detected, the system raises an exception, alerting the involved parties (applications or users) of the conflict. Pros: Prevents data loss by not automatically overwriting any data. It requires intervention to resolve the conflict, which can ensure that important data isn’t inadvertently lost or overwritten. Cons: Requires additional mechanisms for conflict resolution and may interrupt the user workflow. Use Case: The Coda File System employs this strategy to maintain high data integrity, especially in environments where data consistency is critical.</p> <p>“Live Happily and Be Blissfully Unaware” Mechanism: This approach allows concurrent writes to proceed without immediate conflict resolution. Systems employing this strategy might merge changes automatically, keep all versions of the file, or simply ignore the conflict entirely. Pros: Enhances user experience by avoiding interruptions. Systems like Google Docs merge changes in real-time, allowing seamless collaboration. Cons: Can lead to inconsistencies or unexpected results if automatic merging is not possible or if changes are incompatible. Use Case: NFS (Network File System) traditionally does not handle write-write conflicts explicitly at the file system level. Google Docs and Dropbox provide user-friendly collaboration features, allowing multiple users to edit documents simultaneously, with changes reflected in real-time or through version history.</p> <p>```</p> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <p><a href="#blog-chapters">Back to Blog Chapters</a></p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Distributed Systems Notes - SCS]]></summary></entry><entry><title type="html">Advanced Cloud Computing Notes - Currently Upating…</title><link href="https://ychen884.github.io/blog/2024/15719/" rel="alternate" type="text/html" title="Advanced Cloud Computing Notes - Currently Upating…"/><published>2024-01-16T11:00:00+00:00</published><updated>2024-01-16T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2024/15719</id><content type="html" xml:base="https://ychen884.github.io/blog/2024/15719/"><![CDATA[<h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 1: Overview of Cloud Computing</a></li> <li><a href="#topic-2">Chapter 2: Building a Cloud</a></li> <li><a href="#topic-3">Chapter 3: Encapsulation</a></li> <li><a href="#topic-4">Chapter 4: Programming Models and Frameworks</a></li> <li><a href="#topic-5">Chapter 5: Cloud Storage</a></li> <li><a href="#topic-6">Chapter 6: </a></li> <li><a href="#topic-7">Chapter 7: </a></li> </ol> <h2 id="-chapter-1-overview-of-cloud-computing--">** Chapter 1: Overview of Cloud Computing ** <a name="topic-1"></a></h2> <h4 id="definitions">Definitions</h4> <h5 id="properties">Properties:</h5> <ul> <li>Computing utility, always available, accessible through the networks</li> <li>Simplified interface</li> <li>Statistical multiplexing, sharing resources</li> <li>Economies of scale from consolidation, costs lower</li> <li>Capital costs converted to operating costs</li> <li>Rapid and easy variation of usage</li> <li>Appearance of infinite resources with small users</li> <li>Pay only for what you use</li> <li>Cost conservation: 1 unit for 1000 hours == 1000 units for 1 hour</li> </ul> <h5 id="consolidation-sharing-elasticity">Consolidation, sharing, elasticity</h5> <ul> <li>CLT theory</li> <li>users with widely varying needs apply a considerably less variable load on a huge provider, allowing providers to do less overprovisioning.</li> <li> <ul> <li>Because of CLT, it is predictable for the overall load which causes less overprovisioning.</li> </ul> </li> <li>Users perceive exactly what they need all the time, if their needs are “small”(so the accessed resources are appearing as infinite)</li> </ul> <h5 id="saas-paas-iaas">SaaS, PaaS, IaaS</h5> <ul> <li>SaaS: service as application (Salesforce)</li> <li> <ul> <li>consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage &amp; minimum deployed applications configurations settings.</li> </ul> </li> <li>PaaS: high-level programming model for cloud computer, Turing complete but resource management hidden. (Google AppEngine)</li> <li> <ul> <li>Only App and data are controlled by user</li> </ul> </li> <li> <ul> <li>consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage but with control over the deployed applications and possibly configuration settings for the application-hosting environment.</li> </ul> </li> <li>IaaS: low-level computing model for cloud computer (AWS)</li> <li> <ul> <li>The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed application</li> </ul> </li> <li> <ul> <li>Can manage App, Data, Runtime, Middleware, OS, Cannot manage Virtualization, servers, storage, networking</li> </ul> </li> </ul> <h5 id="xxx-as-a-service">XXX as a Service</h5> <ul> <li>Data as a Service, Network as a Service, Communication as a Service(No hardware private VOIP switching), IT as a Service(IT providing services)..</li> </ul> <h5 id="deployment-models">Deployment models</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>public cloud: provider sells computing to unrelated consumers
private cloud: largely unrelated components and divisions as consumers
community cloud: providers and consumers are different organizations with strong shared concerns
Hybrid cloud: private plus public resources combined by same consumer. Better availability, overflow from private to public, load balancing to increase elasticity
</code></pre></div></div> <h5 id="larry-ellisons-objection">Larry Ellison’s objection</h5> <ul> <li>definition is including too much </li> </ul> <h5 id="obstacles-of-cloud-computing">Obstacles of cloud computing</h5> <ul> <li>Privacy &amp; security</li> <li>Privacy in the world tends to rely on regulation</li> <li>Utility issues</li> <li>Physical utilities tend to rely on regulation</li> <li>High cost of networking combined with always remote</li> <li>Performance unpredictability &amp; in situ development/debugging</li> <li>Software licensing – $/yr/CPU is not elastic and pay as you go</li> </ul> <h5 id="load-balancing-approach">load balancing approach</h5> <h5 id="1-dns-load-balancing">1. DNS load balancing</h5> <ul> <li>DNS reorder the list for each client asking for translation of name</li> <li>PRO: easy to scale, unrelated to actual TCP/HTTP requests</li> <li>CON: new server may get less resources if scheduling more servers, dynamic changing is hard because it has to tell client for a binding(existing binding exists due to TTL, caching in network middleboxes).</li> </ul> <h5 id="2-router-distribute-tcp-connections">2. Router distribute TCP connections</h5> <ul> <li>Router do the mapping: (client_ip+client_port) &lt;-&gt; (server_ip+server_port)</li> <li> <ul> <li>for SYN packets</li> </ul> </li> <li> <ul> <li>not exposed to client about the ip address(like NAT)</li> </ul> </li> <li>PRO: router doesn’t have to think or remember too much, *it just selected the server with least connnection to schedule for the new connections.</li> <li>CON: traffic all go through the router(more difficult to scale), and decision takes time cuz it’s for the entire session</li> </ul> <h4 id="3-router-distribute-individual-quests-embedded-in-connections">3. Router distribute individual quests embedded in connections</h4> <ul> <li>PRO: most dynamic</li> <li>CON: requires the most processing and state in the router, CPU load and memory goes up due to intelligent routing decisions. <h4 id="elasticit-how-elasticity-controller">Elasticit: How? Elasticity controller</h4> </li> <li>Elasticity controller to adjust load capability based on current load status</li> <li>Monitoring: resource usage, request sequence(patterns)</li> <li>Triggering: (simple conditions like thresholds), schedule, complex model based on monitored instances</li> </ul> <h4 id="elasticity-scale-out-or-scale-up">Elasticity: Scale-out or Scale up</h4> <ul> <li>Horizontal scaling: adding more instances (Common approach)</li> <li>Vertical scaling: Resizing the resources(bdw, cpu cores, memory) allocated to an existing instances, challenging(different OS.) More challenging.</li> </ul> <h4 id="two-tier-services">Two-tier services</h4> <ul> <li>web-database</li> <li>web server easy to be in cloud. At beginning, order-taking is not in cloud.</li> <li>Elasticity in IaaS: database scaling is more difficult with state(consistency)</li> <li>PassS, P=Web Service. Built-in elastic load balancing and scheduled actions for containers, persistent key-value store (datastore) &amp; non-persistent memcache for simple database tier, Users can instantiate Backends, user code can request (actuate) horizontal scaling, running traditional database services, whose scaling is still hard.</li> </ul> <h5 id="load-balancing-method-affect-how-much-statistics-we-can-get">Load-balancing method affect how much statistics we can get.</h5> <ul> <li>Router-based load balancing: firewall, intrusion detection, accelerator</li> <li>scaling middleboxes: CPU intensive tasks. (OpenFlow, split flows)</li> <li>bdw allocation by sw/rt</li> </ul> <h5 id="service-parallelization-load-balancer">Service parallelization: Load Balancer</h5> <ul> <li>aws cloud watch</li> <li>Load balancer is not necessarily elastic</li> </ul> <h5 id="scalable-relational-database">Scalable relational database</h5> <ul> <li>Separate data at rest(distributed pay-for-use storage (HDFS)) from ongoing or recent access &amp; mutation</li> <li>Recent access &amp; mutation servers are elastic (called Owning Transaction Managers)</li> <li>Partitioned but all transactions restricted to one partition: transactions block on locks and bottleneck performance scaling</li> <li>Fault-tolerance of Elastic controller. Controller itself, reliability provided by replication. Can re-assigns partitions while server is down/start up.</li> </ul> <h5 id="elastras-architecture-scales-otm-machines">ElasTraS architecture scales OTM machines</h5> <ul> <li>Transactions are limited to interacting with data from only one partition to avoid the complexity of distributed transactions.</li> <li>TODO</li> </ul> <h3 id="paper-reading-notes">Paper reading notes</h3> <h5 id="armbrust2010">Armbrust2010</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Referring to http://doi.acm.org/10.1145/1721654.1721672
Cloud computing: what brings it? large capital outlays, overprovisioning/underprovisioning...

Definition: Refers to both the applications as services over the internet and hardware and systems software in the data center that provide those services. The services themselves: SaaS. Services being sold is utility computing. Cloud computing = SaaS + utility computing. It has to be large enough to be called cloud. Hardware provisioning/pricing: 1. inifinite computing resources available on demand; 3. elimination of an up-front commitment by cloud users, add resources when needed; 4. pay for use of computing resources temporarily. *Construction and operaition of extremely large-scale, commodify-computer data centers at low-cost locations was the key necessary enabler of cloud computing. It could offer services below the costs of a medium-sized data center and make profits.

Utility Computing classes: EC2 with low level control but less automatic scalability and failover(application may need to control the replication...). Google AppEngine(domain specific platforms) on the other hand. Azure is in between. 

Economics: Favor cloud computing over conventional: 1. demand of services changes over time 2. demand is unknown
usage based pricing economically benefits the buyer. 
Elasticity helps reduce the costs. Underprovisioning has a cost that is difficult to measure: the users may never come back. Scale-up elasticity is an operational requirement, and scale-down elasticity allowed the steady state expenditure to more closely match the steady-state workload. 


Obstacles for Cloud computing:

1-3(adoption):
1. Business Continuity and Service Availability (hard to ensure availability, single failure still exists for a service provider)
2. Data Lock-In (public+private sharing by sharing API)
3. Data Confidentiality/Auditability: from other user/provider 

4-8(growth):
4. Data Transfer Bottlenecks： Applications continue to become more data-intensive
5. Performance Unpredictability： I/O interference between virtual machines, concerns scheduling of virtual machines for some classes of batch processing programs, specifically for highperformance computing
6.Scalable Storage
7. Bugs in Large Scale Distributed Systems: bugs cannot be reproduced in smaller configurations
8. Scaling Quickly

9-10(policy and business):
9. Reputation Fate Sharing, legal liability(customer responsible-&gt;unexpected down)
10. Software Licensing



Opportunities: 
- improve architectures and operating systems to efficiently virtualize interrupts and I/O channels
- flash memory will decrease I/O interference.
- offer something like “gang scheduling” for cloud computing
- create a storage system that would not only meet existing programmer expectations,but combine them with the cloud advantages of scaling arbitrarily up and down on demand. 
- reliance on virtual machines in cloud computing.(7)
- automatically scale quickly up and down in response to load in order to save money
- create reputation-guarding services similar to the “trusted email” services 
</code></pre></div></div> <h5 id="referring-to-nistdef2011">Referring to NISTdef2011</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Characteristics:
- On-demand self-service
- Broad network access
- Rapid elasticity
- Resource pooling (multi-tenant)
</code></pre></div></div> <h5 id="vaquero11">Vaquero11</h5> <h5 id="dynamically-scaling-applications-in-the-cloud">Dynamically Scaling Applications in the Cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vertical scaling is hard: rebooting..
load balancers need to support

Server scalability: a per-tier controller, a single controller for the whole application (for all tiers)
How and When to Scale: Feeding Controller with Rules and Policies
- adding load balancers
- LB scalability requires the total time taken to forward each request to the corresponding server to be negligible for small loads and should grow no faster than O(p) 
-  CPU-intensive web app: a LB to split computation among many instances.
-  network intensive: CPU-powerful standalone instance to maximize throughput
-  more network intensive applications, it may be necessary to use DNS load balancing

Scaling the network:
- Virtualization: VLAN(L2)
-  periodically measure actual network usage per application use other applications’ - increase the utilization of the network by virtually “slicing”: “network as a service”
- statistical multiplexing


Scaling the platform:
PaaS cloud platforms
- Container Scalability: execution environment for user applications.scalability of the container layer is crucial as it must efficiently manage and distribute resources to meet the demands of potentially numerous applications running concurrently.

Multitenant containers and Isolation: requires strong isolation to prevent security issues
Individual Containers per User: simplifies isolation but requires effective management of numerous containers.
Horizontal Scaling via Container Replication: achieved through automatic scaling in IaaS systems or by inherent capabilities of the platform itself
Components should ideally be stateless to cope with the dynamic nature of container instantiation and disposal, support for stateful components is also necessary, requiring sophisticated load balancing (LB) and container management to handle session data.(soft state replication, distributed caching systems)

- Database Scalability: for example, implementing horizontal scaling strategies, such as replicating the database across multiple nodes to handle increased load and ensure data availability. (demands on PaaS platforms can often surpass the capacity of any single machine)
NoSQL Databases: These databases provide high scalability and availability, fitting well in cloud environments with high demand. However, they offer eventual consistency rather than immediate consistency, leading to limitations in transaction support and SQL functionalities.
Replication Mechanisms: In-core Solutions, Middleware Solutions

Ideal Elastic Cloud: scale through VM or container replication, reconfiguration, and dynamic load balancing, possibly using DNS for the latter. allow dynamic allocation of network resources. 
Ideal PaaS Platform Features: capable of instantiating or releasing instances of user components based on demand changes and distributing the load transparently among them. Implementing session concepts and supporting transparent data replication are essential. Access to traditional relational databases with ACID transaction support is crucial. However, the system must address the increased latency due to consistency maintenance across replicas, especially under high demand.
</code></pre></div></div> <ul> <li>Infrastructure-as-Code tools provide a high-level software interface (e.g., Python / Ruby or JSON / YAML) that allows developers to specify their infrastructure requirements, software dependencies, and the process for building the infrastructure and deploying it to the cloud</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="-chapter-2-building-a-cmu-cloud--">** Chapter 2: Building a CMU Cloud ** <a name="topic-2"></a></h2> <h4 id="model">Model</h4> <h5 id="aim-for-less-costs">aim for less costs</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Client -&gt;
App
OS
Instance(s)
--------------------
Hypervisor (shared)
Building Blocks
Usage Monitor
Hardware (shared)
(shared Amazon EC2)

</code></pre></div></div> <h5 id="problem-used-mostly-during-ddl-2-methods-renting">Problem: used mostly during ddl? 2 methods, renting..</h5> <h5 id="build-a-cloud">Build a cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. user would rent, so provisioner(resources,monitoring,... -&gt; assignment of users to machines)
- Bin-packing, NP-hard. Can do with assumptions.. -&gt; less complexity
- Migration, also costs
2. Scheduler: which user jobs/processes to run: Prioritization(pay more), Oversubscription, Workload constraints
3. Encapsulation: compute, storage, networking and data
4. Virtualization
5. Fault tolerance: 
    why scale of data center matters? Economy of scale, cost of operations
    state replication, logging, storage replic

Storage services - Scalable, fault tolerant
Tools - Programming models, frameworks
Automation - Reactive systems &amp; elastic scaling

Elastic scaling - based on monitoring and diagnosis
traditional: provision for peaks

</code></pre></div></div> <ul> <li>Cloud users &amp; Services</li> <li> <ul> <li>App user: availability, performance, no interfaces exposed by cloud service provider (app provider did that)</li> </ul> </li> <li> <ul> <li>Application Deployment User: Dashboard, management interfaces exposed</li> </ul> </li> <li> <ul> <li>Admin: Management…</li> </ul> </li> <li>Orchestration: automatic deployment for user</li> </ul> <h5 id="openstack">OpenStack</h5> <ul> <li>independent parts,6 core services</li> <li>communicate through public and well-defined APIs</li> <li>Identity -&gt; Dashboard -&gt; Compute(?)/Network -&gt; Image -&gt; Object storage(get the image for VM) -&gt; Block Storage(volume) -&gt; initiate VM</li> <li>Distributed storage to accelerate image initialization takes Azure many years to fix</li> </ul> <h5 id="referring-to-sotomayor2009">Referring to sotomayor2009</h5> <h5 id="virtual-infrastructure-management-in-private--hybrid-clouds">Virtual Infrastructure Management in Private &amp; Hybrid Clouds</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Virtual Infrastructure (VI) management in the context of private/hybrid cloud environments: Uniform Resource View, Full Lifecycle Management of VMs, Configurable Resource Allocation Policies, Adaptability to Changing Resource Needs. provides primitives to schedule and manage VMs across multiple physical hosts


OpenNebula: This is a VI manager that allows organizations to deploy and manage VMs, either individually or in groups. It automates the VM setup process (including preparing disk images and setting up networking) and is compatible with various virtualization layers (Xen, KVM, VMware) and external clouds (EC2, ElasticHosts).

Haizea: This acts as a lease manager and can serve as a scheduling backend for OpenNebula. It introduces leasing capabilities not present in other cloud systems, such as advance reservations and resource preemption, which are particularly valuable for private cloud environments.



Traditional VI Management Tools: lack certain features necessary for building IaaS clouds, such as public cloud-like interfaces and the ability to deploy VMs on external clouds.
Cloud toolkits can help transform existing infrastructure into an IaaS cloud with cloudlike interfaces. VI management capabilities are not as robust.
    -: Gap: There's a noticeable gap between cloud management and VI management. Cloud toolkits attempt to cover both areas but often fall short in delivering comprehensive VI management functionalities. Integrating cloud management solutions with VI managers is complex due to the lack of open, standard interfaces and certain key features in existing VI managers.
    
    OpenNebula: overcome these challenges, Scaling to external clouds. A flexible and open architecture for easy extension and integration with other software. A variety of placement policies and support for scheduling, deploying, and configuring groups of VMs.
    
    Integration of Haizea with OpenNebula: work as a scheduler for OpenNebula, This integration allows OpenNebula to offer resource leases as a fundamental provisioning abstraction, and Haizea to operate with real hardware through OpenNebula. This combination provides advanced features like advance reservation of capacity, which is not offered by other VI managers.

    The integration is particularly beneficial for private clouds with limited resources, enabling sophisticated VM placement strategies that support queues, priorities, and advance reservations (ARs).

OpenNebula: 
core:  image and storage technologies for preparing disk images for VMs, network fabric (such as Dynamic Host Configuration Protocol [DHCP] servers, firewalls, or switches) for providing VMs with a virtual network environment, hypervisors for creating and controlling VMs. Performs operations through pluggable drivers. 
A separate scheduler component makes VM placement decisions.
Management interfaces: libvirt API intergrate within other data center management tools, cloud interface exposed to external users.
Cloud drivers to interface with external clouds

The Haizea Lease Manager: Haizea is an open source resource lease manager and can act as a VM scheduler for OpenNebula.
Advance Reservation (AR) Leases: Resources are guaranteed to be available at a specific future time. This is beneficial for scenarios requiring resource certainty.
Best-Effort Leases: Resources are provisioned as soon as possible, with requests queued if immediate provisioning isn't possible.
Immediate Leases: Resources are provided immediately upon request or not at all, suitable for urgent needs without flexibility.
    
Haizea addresses the challenge of resource underutilization, often a downside of AR, by leveraging VMs. It allows for efficient support of ARs through resource preemption - suspending lower-priority VMs to free up resources for higher-priority needs and resuming them later.
It uses optimizations such as reusing disk images across leases to minimize the impact of preparation overhead and schedules runtime overheads (like suspending, resuming, and migrating VMs) efficiently.
Scheduling is based on a resource slot table, representing all physical nodes managed by Haizea over time.
Best-effort leases are managed using a first-come-first-serve queue with backfilling, optimizing queue-based systems.
AR leases utilize a greedy algorithm for selecting physical resources to minimize preemptions.

There are ongoing efforts to extend OpenNebula's capabilities, including the implementation of the libvirt interface, VM consolidation schedulers for minimizing energy consumption, and tools for service elasticity management, VM placement, public cloud interface support, and policy-driven dynamic placement optimization.

Haizea shows that VM-based approaches with suspend/resume capabilities can address utilization issues typically associated with advance reservation (AR) use.

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h5 id="xen-and-the-art-of-virtualization">Xen and the Art of Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Xen, a high-performance, resource-managed VMM that effectively allows multiple commodity operating systems to share hardware resources safely and efficiently

Advantages: isolation of virtual machines to prevent adverse performance effects, accommodation of diverse operating systems, and minimization of performance overhead introduced by virtualization

- targeting existing application binary interfaces (ABIs) and supporting the virtualization of complex server configurations within a single guest OS instance. full multi-application operating systems, and the necessity of paravirtualization for high performance on complex architectures like x86.

 resource virtualization: ensures both correctness and performance by allowing guest operating systems visibility of real as well as virtual resources.  better support for time-sensitive tasks and optimal use of physical resources like superpages or page coloring. 

Memory Management:
Segmentation and Paging: Guest OSes are responsible for managing hardware page tables with minimal Xen involvement for safety. Xen resides in a 64MB section at the top of every address space to avoid TLB flush when switching to and from the hypervisor. Segmentation is virtualized by validating updates to hardware segment descriptor tables.
Guest OS Restrictions: Guest OSes cannot install fully-privileged segment descriptors and must not overlap with the top end of the linear address space. They have direct read access to hardware page tables but updates are batched and validated by Xen.


CPU:
Privilege Levels: Guest OSes run at a lower privilege level than Xen to protect the hypervisor from OS misbehavior and ensure domain isolation. x86's four privilege levels are leveraged, with guest OSes typically moved from ring 0 to ring 1.
Exceptions and System Calls: Guest OSes must register descriptor tables for exception handlers with Xen. They may install a ‘fast’ handler for system calls to avoid indirecting through Xen for every call. Page faults and system calls are the most performance-critical exceptions.

Device I/O:
Virtual Devices: Instead of emulating hardware devices, Xen introduces efficient and simple device abstractions. Data transfer is done using shared-memory, asynchronous buffer-descriptor rings, allowing Xen to perform validation checks.
Event System: Hardware interrupts are replaced with a lightweight event system. Xen supports a mechanism for sending asynchronous notifications to a domain, similar to hardware interrupts, but with more control and efficiency.

Porting Operating Systems to Xen: Code Modifications in architecture-independent sections, virtual network drivers, block-device drivers, and Xen-specific (non-driver) areas. Both operating systems required substantial alterations in their architecture-specific sections. 

Role of Domain0: A domain created at boot time, termed Domain0, is responsible for hosting application-level management software. It uses the control interface to manage the system, including creating and terminating other domains and controlling their resources.

Control Transfer: Hypercalls and Events:

Hypercalls: Synchronous calls from a domain to Xen, analogous to system calls in conventional OSes. Used for operations like requesting page-table updates.
Event Mechanism: Asynchronous notifications from Xen to domains, replacing traditional interrupt delivery mechanisms. Events are used for lightweight notifications like domain-termination requests or indicating that new data has been received over the network.
Data Transfer: I/O Rings:

Efficient data transfer mechanism that allows data to move vertically through the system with minimal overhead. The design focuses on resource management and event notification.
I/O rings are circular queues of descriptors allowing asynchronous, out-of-band data transfer between Xen and guest OSes. They support a variety of device paradigms and enable efficient zero-copy transfer.

Subsystem Virtualization:
CPU Scheduling: Xen employs the Borrowed Virtual Time (BVT) scheduling algorithm for domain scheduling, ensuring low-latency wake-up and fair resource sharing.
Time and Timers: Xen provides notions of real time, virtual time, and wall-clock time to guest OSes. Domains can program alarm timers for real and virtual time, with timeouts delivered using Xen's event mechanism.

Virtual Address Translation: Xen avoids the overhead of shadow page tables by allowing guest OSes to manage hardware page tables directly, with Xen involved only in updates validation. This approach minimizes the number of hypercalls required for page table management.

Physical Memory: Memory is statically partitioned between domains, providing strong isolation. Guest OSes can adjust their memory usage dynamically, with mechanisms like the balloon driver in XenoLinux facilitating this interaction.

Network: Xen introduces the concept of a virtual firewall-router (VFR), with each domain having virtual network interfaces (VIFs) attached to the VFR. Domains transmit packets via enqueueing buffer descriptors on I/O rings, and Xen handles packet reception efficiently by exchanging packets directly with page frames.
Disk: Only Domain0 has direct access to physical disks. Other domains access persistent storage through virtual block devices (VBDs). Disk requests are batched and serviced in a round-robin fashion by Xen, ensuring fair access and good throughput.

Advantages of Delegating Domain Construction to Domain0:
Reduced Hypervisor Complexity: Building a domain within Domain0 rather than entirely within Xen simplifies the hypervisor's design, focusing its functionality on core tasks and leaving the domain setup process to a more specialized and capable component.

Improved Robustness: The process of setting up a new domain involves numerous delicate operations. Performing these operations within Domain0 allows for better error checking and handling.

Metrics:
Performance Comparison with Other Virtualization Technologies
Efficient Data Transfer and Subsystem Virtualization
Performance Isolation
Scalability
Network Performance
Concurrent Virtual Machines
Microbenchmarks 

Key points:
Efficient Paravirtualization
Resource Management and Performance Isolation
Scalability
Minimal Performance Overhead
Generality of the Interface
Facilitation of Network-Centric Services
</code></pre></div></div> <h2 id="-chapter-3-encapsulation--">** Chapter 3: Encapsulation ** <a name="topic-3"></a></h2> <h5 id="options-available-for-encap">Options available for encap</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Isolation: others can't read/write your data
- - can't impact your performance (DOS)
- Processes may need memory sharing, get resource alloc, has own address space
</code></pre></div></div> <ol> <li>Bare metal <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+: good isolation, performance, very good software freedom
-: granularity limitation
</code></pre></div> </div> </li> <li> <p>Process: just application</p> </li> <li>Containers: with applicaiton &amp; library/fs/etc that looks like an OS <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+: performance, decent software freedom
-: security issues
</code></pre></div> </div> </li> <li>Virtual Machines: physical machine-like software container ``` HW &lt;-&gt; VMM &lt;-&gt; (OS, Library/fs/etc/, App) +: isolation properties, good software freedom -: performance overhead, imperfect performance isolation</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Comparison
- Process, Container, VM, BM
- Lower management &lt;-&gt; Better isolation/fidelity


##### interface is the key
- between APP/Library/OS/HW

##### Linux namespaces
- pid, gid, ...
##### resouece alloc: linux groups

##### Limitation of containers
- share the same host OS, security + flexibility(have to work with this OS)


##### Virtualization
- VM: vm OS &lt;--virtual machine interface--&gt; VMM &lt;--machine interface--&gt; HW
- The interfaces can be different

##### HW Virtualization Principles
- Fidelity, software operation keeps identical
- Isolation, guest cannot affect other guest/VMM
- Performance, most operations execute natively

#### Ecap Principle


</code></pre></div></div> <ul> <li>Basic: Execute VM softare in de-privileged mode</li> <li> <ul> <li>prevent privileged instr from escaping containment Privilege Level App User/Ring 3 Lib User/Ring 3 OS Supervisor/Ring-o HW</li> </ul> </li> </ul> <p>Option 1 Privilege Level App User/Ring 3 Lib User/Ring 3 OS User/Ring-3 VMM Supervisor/Ring-0 HW Overhead - Isolation, VMM has to deal with operations from OS.. not ideal</p> <p>Option 2 Privilege Level App User/Ring 3 Lib User/Ring 3 OS Ring-1 VMM Supervisor/Ring-0 HW</p> <p>—-&gt; later Option 3 Privilege Level App User/Ring 3* Lib User/Ring 3* OS Ring-0* (don’t have all power, cannot change fundamental page tables..) VMM Supervisor/Ring-0 HW</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##### Priv Ops


</code></pre></div></div> <ul> <li>Most executions run directly</li> <li>VNM needs to handle OS attemps that execute priv ops, Non-CPU devices..</li> <li> <ul> <li>popf, pushf…</li> </ul> </li> </ul> <p>https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/VMware_paravirtualization.pdf</p> <p>Handle: Trap &amp; emulate: VMM handles Static software re-writing/paravirtualization: rewrite guest OS to leverage VMM hypercalls (performance, sacrificing transparancy) Dynamic software re-writing: VMM re-write guest’s privileged code - coalescing traps, VMM complex, good performance</p> <p>Referring to stackoverflow.com/questions/21462581 Paravirtualization is virtualization in which the guest operating system (the one being virtualized) is aware that it is a guest and accordingly has drivers that, instead of issuing hardware commands, simply issue commands directly to the host operating system. This also includes memory and thread management as well, which usually require unavailable privileged instructions in the processor.</p> <p>Full Virtualization is virtualization in which the guest operating system is unaware that it is in a virtualized environment, and therefore hardware is virtualized by the host operating system so that the guest can issue commands to what it thinks is actual hardware, but really are just simulated hardware devices created by the host.</p> <p>Hardware Assisted Virtualization is a type of Full Virtualization where the microprocessor architecture has special instructions to aid the virtualization of hardware. These instructions might allow a virtual context to be setup so that the guest can execute privileged instructions directly on the processor without affecting the host. Such a feature set is often called a Hypervisor. If said instructions do not exist, Full Virtualization is still possible, however it must be done via software techniques such as Dynamic Recompilation where the host recompiles on the fly privileged instructions in the guest to be able to run in a non-privileged way on the host.</p> <p>There is also a combination of Para Virtualization and Full Virtualization called Hybrid Virtualization where parts of the guest operating system use paravirtualization for certain hardware drivers, and the host uses full virtualization for other features. This often produces superior performance on the guest without the need for the guest to be completely paravirtualized. An example of this: The guest uses full virtualization for privileged instructions in the kernel but paravirtualization for IO requests using a special driver in the guest. This way the guest operating system does not need to be fully paravirtualized, since this is sometimes not available, but can still enjoy some paravirtualized features by implementing special drivers for the guest.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Handling memory: process
</code></pre></div></div> <ul> <li>OS: page mapping from virtual page to physical page</li> <li>User process: typically continuous address space</li> </ul> <p>Mutiple OS: Guest OSes manage: Mapping guest virtual to guest physical VMM manages guest physical to host physical</p> <p>other split: cores, time sharing..</p> <p>SW: Shadow page tables HW: Extended page tables(EPT)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Virtualization Devices

</code></pre></div></div> <ul> <li>Could be all virtualized (trap accesses, and emulate), but worse performance</li> </ul> <p>For performance</p> <ol> <li>Mapping, control given to a host (hw support)</li> <li>Partition, (disk..)</li> <li>Guest enhancement (special VM -&gt; VMM calls)</li> <li>Virtualized-enhanced devices (NICs with VMDq)</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Security issues of virtualization/containerization
- meltdown
- bitflip(row hammer)



[Back to Blog Chapters](#blog-chapters)

##### Web servers
- Online retail stores
- parallel programming: contention, failures, consistency(shared resources), load balancing,..

##### HPC
- scaling for N processes (lower costs, faster computation)
- strong/weak: same process finishes faster on N processors?
- Bulk Synchronous Processing..
- MPI, resource allocators, schedulers, ft
- Very manual, deep learning curve, few commercial runaway successes
##### Grid Computing
- easier to use
- emphasized geographical sharing
- jobs selected from batch queue, take over cluster
- workload diversity
##### Cloud Computing
- low initial cost
- workload diversity +
- may cost efficiency for easier programming

##### Motivation for CC programming framework
- using many commodity* machines
- tolerate failures
- locality(reduce cost of communication), parallelism

##### Batch processing of large datasets on a cluster: framework offers..
- ...
- Job orchestration
- Data staging and movement

##### MapReduce

</code></pre></div></div> <ul> <li>read large data set</li> <li>ind process the input data in chunks</li> <li>shuffle and sort data (by key)</li> <li>reduce(merge)</li> <li>locality optimization for racks</li> <li>shuffle begins after first round of map (master responsible for scheduling this)</li> </ul> <p>Split RR(record reader) -&gt;(K,V) Map -&gt; (K’, V’) -&gt; Partitioner: hash(key) mod R -&gt; reducer Sort (may need to wait for reduce finish) Reuce</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

##### Map data size vs shuffle data size
- ngram(small-&gt;large)

##### Spark

</code></pre></div></div> <p>Less disk operations for iterative apps RDD as memory storage(immutable), rerun instead of storing in persistent storage transformations create new ones, can be lazy action create value, recomputed when action comes</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
## ** Chapter 4: Programming Models and Frameworks ** &lt;a name="topic-4"&gt;&lt;/a&gt;



##### Map Reduce
</code></pre></div></div> <p>Core Concepts of MapReduce:</p> <p>The MapReduce model involves two primary functions: the map function and the reduce function. The map function takes key/value pairs as input and produces a set of intermediate key/value pairs. The reduce function then merges all intermediate values associated with the same intermediate key.</p> <p>Automatic Parallelization and Distribution: MapReduce abstracts the complexity of parallelization, fault-tolerance, data distribution, and load balancing</p> <p>Scalability and Ease of Use: The MapReduce implementation can process vast amounts of data (many terabytes) across thousands of machines. It has a user-friendly nature, evidenced by the extensive use within Google, with hundreds of MapReduce programs implemented and thousands of jobs executed daily.</p> <p>Motivation and Design Philosophy: The motivation behind MapReduce stemmed from the need to process large sets of raw data (like web documents, request logs) into computed forms (like inverted indices, summaries) efficiently. MapReduce was designed to simplify the computations by abstracting the complex code required for parallelization, data distribution, and fault tolerance into a library.</p> <p>Programming Model: The computation in MapReduce takes input key/value pairs and produces output key/value pairs, expressed through Map and Reduce functions defined by the user. It handles the grouping of intermediate values by keys and provides these to the Reduce function, which then merges the values per key.</p> <p>Examples and Applications: The abstract and subsequent sections provide practical examples, like counting the number of occurrences of each word in a document collection, showcasing the model’s applicability to real-world tasks. It also highlights the versatility of MapReduce in tasks such as distributed grep, counting URL access frequency, reversing web-link graphs, and computing term-vectors per host. Inverted Index: A fundamental operation in document searching where the map function processes documents, emitting word and document ID pairs, and the reduce function groups these by word, creating a list of document IDs for each word. Distributed Sort: A sorting operation where the map function extracts keys from records and the reduce function sorts these keys, relying on the system’s partitioning and ordering capabilities.</p> <p>Implementation and Execution Overview: designed for large clusters of commodity PCs, considering the typical hardware specifications and the nature of the network and storage systems. The execution process involves dividing the input data into splits, assigning map and reduce tasks to workers, processing the data through the user-defined map and reduce functions, handling intermediate data storage and retrieval, and finally producing the output after all tasks are completed.</p> <p>Fault Tolerance: Fault tolerance is a critical aspect, given the scale of operation and the likelihood of machine failures. The system handles worker failures by reassigning tasks and redoing work if necessary. The master node monitors worker status and orchestrates the reassignment of tasks as needed. The system is designed to ensure that, despite failures, the output is consistent with what would be produced by a faultless, sequential execution, as long as the map and reduce functions are deterministic.</p> <p>Master Data Structures: The master node maintains data structures to keep track of the status of each task and the locations of intermediate data. This information is crucial for coordinating the work of map and reduce workers and for ensuring that data is correctly routed through the system.</p> <p>Locality Network Bandwidth Optimization: importance of conserving network bandwidth, a relatively scarce resource in large computing environments. Data Locality: The approach involves scheduling map tasks on machines that contain a replica of the input data, or at least are network-local to the data. This strategy significantly reduces network bandwidth usage as most data is read locally.</p> <p>Granularity of Map and Reduce Phases: The map phase is subdivided into M pieces, and the reduce phase into R pieces. Ideally, M and R should be much larger than the number of worker machines. Dynamic Load Balancing and Recovery: This setup improves dynamic load balancing and expedites recovery from worker failures, as the tasks a worker has completed can be redistributed across other workers. Handling Stragglers: machines that take unusually long to complete tasks. Causes for stragglers include hardware issues, resource contention, or bugs.</p> <p>Custom Partitioning: While a default partitioning function based on hashing is provided, users have the option to specify a custom partitioning function, allowing more control over how data is partitioned across reduce tasks.</p> <p>Intermediate Key/Value Pair Ordering: The framework ensures that within a partition, intermediate key/value pairs are processed in increasing key order. This ordering guarantee is particularly beneficial when the output needs to be sorted or efficiently accessible by key.</p> <p>Data Aggregation at Mapper Nodes: The Combiner function allows for partial merging of intermediate data on the mapper nodes before sending it over the network. This feature is especially useful when there’s significant repetition in the intermediate keys, and the reduce function is commutative and associative.</p> <p>Flexible Data Formats: The MapReduce library supports various formats for input data, providing flexibility to handle different types of data sources.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

##### Spark

</code></pre></div></div> <p>Spark, a new cluster computing framework designed to address the limitations of MapReduce and its variants in handling certain types of applications, particularly those involving iterative operations and interactive data analysis.</p> <ul> <li>Limitations of MapReduce and Similar Systems: they are primarily built around an acyclic data flow model. This model is not suitable for applications that need to reuse a working set of data across multiple parallel operations.</li> </ul> <p>Iterative Jobs: Common in machine learning, these jobs require applying a function repeatedly to the same dataset. MapReduce’s need to reload data from disk for each iteration causes significant performance penalties.</p> <p>Interactive Analytics: Users often run ad-hoc queries on large datasets using SQL interfaces. With MapReduce, each query incurs high latency because it operates as a separate job and needs to read data from disk.</p> <p>Spark aims to support applications with working sets while retaining the scalability and fault tolerance characteristics of MapReduce. The key abstraction in Spark is the resilient distributed dataset (RDD), a read-only collection of objects partitioned across a set of machines. RDDs can be explicitly cached in memory and reused in multiple parallel operations, significantly improving performance for certain types of applications.</p> <p>Fault Tolerance through Lineage: RDDs are fault-tolerant, using a concept called lineage. If a partition of an RDD is lost, the system has sufficient information on how the RDD was derived from other RDDs to rebuild just the lost partition.</p> <p>Implementation and Usability of Spark: Spark is implemented in Scala, allowing for a high-level, statically typed, and functional programming interface. It can also be used interactively, which is a novel feature for a system of this kind, enabling users to process large datasets on a cluster interactively.</p> <p>Spark can significantly outperform Hadoop in iterative machine learning workloads and provide interactive querying capabilities with sub-second latency for large datasets.</p> <ul> <li>Programming Model Spark introduces two primary abstractions for parallel programming: resilient distributed datasets (RDDs) and parallel operations on these datasets, and shared variables.</li> </ul> <p>RDDs are read-only collections of objects partitioned across machines, capable of being rebuilt if a partition is lost. RDDs can be created directly from a file in a shared filesystem like HDFS, by parallelizing a collection in the driver program, or by transforming an existing RDD. RDDs are lazy and ephemeral by default, meaning they are computed on demand and not stored persistently after use. However, users can modify this behavior using two actions: The cache action suggests keeping the dataset in memory after its initial computation for future reuse. The save action evaluates and persists the dataset to a distributed filesystem, like HDFS. This approach to persistence is a design choice in Spark to ensure continued operation under memory constraints or node failures, drawing a parallel to the concept of virtual memory.</p> <p>Parallel Operations Operations on RDDs: Spark supports various parallel operations on RDDs, such as reduce (combining elements using a function), collect (sending all elements to the driver program), and foreach (applying a function to each element for its side effects).</p> <p>Shared Variables Handling Variables in Parallel Operations: When parallel operations like map and filter are performed, the closures (functions) used can refer to variables in their creation scope. By default, these variables are copied to each worker node.</p> <p>Types of Shared Variables: Spark introduces two types of shared variables for common usage patterns: Broadcast Variables: Used for distributing large, read-only pieces of data efficiently across workers. Accumulators: These are “add-only” variables for workers and readable only by the driver, suitable for implementing counters and parallel sums.</p> <p>-Text Search The first example is a simple text search to count the number of lines containing “ERROR” in a large log file. The process involves creating an RDD from the file, filtering lines containing “ERROR”, mapping each line to 1, and then reducing by summing these ones. This example illustrates Spark’s lazy evaluation and in-memory data sharing capabilities, which allows for efficient data processing without materializing intermediate datasets.</p> <ul> <li> <p>Logistic Regression This program demonstrates an iterative machine learning algorithm, logistic regression, which benefits significantly from Spark’s ability to cache data in memory across iterations. The program reads points from a file, caches them, and then iteratively updates a vector w using a gradient computed in parallel across the points. The use of accumulators for summing the gradient and the syntax of Spark make the code resemble an imperative serial program while being executed in parallel.</p> </li> <li> <p>Alternating Least Squares (ALS)</p> </li> </ul> <p>Implementation The section details Spark’s implementation, including its reliance on Mesos for cluster management, the structure of RDDs, task scheduling for parallel operations, handling of shared variables, and integration with the Scala interpreter.</p> <p>Resilient Distributed Datasets (RDDs): RDDs are implemented as a chain of objects capturing their lineage, allowing efficient recomputation in case of node failures. Different types of RDDs (e.g., for files or transformed datasets) implement a standard interface for partitioning, iteration, and task scheduling. Parallel Operations and Task Scheduling: Spark creates tasks for each RDD partition and tries to schedule them based on data locality. It uses a technique called delay scheduling for efficiency. Handling of Shared Variables: Broadcast variables and accumulators are implemented with custom serialization formats to ensure efficient distribution and fault tolerance. Interpreter Integration: Spark integrates with the Scala interpreter, allowing interactive processing of large datasets. Modifications were made to ensure that closures and state are correctly serialized and distributed to worker nodes.</p> <p>5 Results Logistic Regression Performance: Spark significantly outperforms Hadoop in iterative machine learning workloads, with up to 10x faster performance due to data caching. Alternating Least Squares (ALS) Performance: The use of broadcast variables for distributing the ratings matrix results in substantial performance improvements in the ALS job. Interactive Spark Usage: Spark enables interactive querying of a large dataset with sub-second response times after initial data loading, providing a much faster and more interactive experience than Hadoop.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### MR ML model training
- it may scale great, but overhead is high: ML training just not well suited for stateless, deterministic functions
##### Spark better

##### Parameter Servers
1. Shared Memory for Parameters
2. Atomic Operations on Parameters
3. Efficient Communication via RPCs
4. Avoid Repartitioning Input Data

###### Sync? 
1. MR Approach: Requires strict synchronization (barrier synchronization) at the end of each stage, ensuring consistency but potentially leading to idle time as all nodes must wait for the slowest one.
2. Parameter Server Approach: Offers flexibility. Synchronous mode ensures consistency but might slow down the training due to waiting times. Asynchronous mode can significantly speed up training as it allows nodes to compute and update independently, but it introduces some level of noise in the updates, which the training process must tolerate.

</code></pre></div></div> <p>Parameter Servers</p> <p>Sequential (BSP - Bulk Synchronous Parallel):</p> <p>Traditional distributed computing, like in Spark, utilizes synchronous communication. Each iteration requires all tasks to be completed before moving to the next. Advantages: Broad applicability and high convergence quality per iteration. Disadvantages: Each iteration waits for the slowest task, leading to longer overall task computation times. Bounded Delay (SSP - Staleness Synchronous Parallel):</p> <p>Sets a maximum delay time, known as the staleness value, allowing a certain degree of inconsistency in task progress. When staleness = 0, it equates to the Sequential consistency model; when staleness = ∞, it becomes the Eventual consistency model. Advantages: Reduces waiting time between tasks to a certain extent, offering faster computation speed and allowing developers to balance between algorithm convergence rate and system performance. Disadvantages: The convergence quality per iteration may not be as high as BSP, potentially requiring more iterations to achieve similar convergence; not as broadly applicable as BSP, with some algorithms not suitable. Eventual (ASP - Asynchronous Parallel):</p> <p>Asynchronous communication, where tasks do not need to wait for each other. Tasks that complete first can proceed to the next iteration immediately. Advantages: No waiting for other tasks, leading to fast computation speed. Disadvantages: Poor applicability, potential decrease in convergence rate, or even non-convergence. The synchronization restrictions of the three consistency models progressively relax to pursue faster computation speeds. In practice, the consistency model and related parameters should be adjusted based on the changes in metrics to balance convergence and computational speed.</p> <p>User-defined Filters:</p> <p>The system supports user-defined filters to filter out certain entries, thus reducing network bandwidth. Common filters include the significantly modified filter, which only pushes entries that have changed beyond a certain threshold, and the KKT filter, which uses conditions from optimization problems to filter out entries that have minimal impact on weights.</p> <p>Vector Clock: The parameter server uses a range vector clock to record the timestamp of each node’s parameters. This helps in tracking the state of data and avoiding the resending of data. Since parameters are pushed and pulled in ranges, parameters within the same key range can share a timestamp. This approach compresses the traditional vector clock, reducing memory and network bandwidth overhead.</p> <p>Messages: Messages sent between nodes consist of a range vector clock and (key, value) pairs. Message Compression: Due to frequent updates of model parameters, the parameter server employs two methods to compress messages to reduce network bandwidth overhead:</p> <p>Key Compression: Since training data typically doesn’t change during iterations, it’s unnecessary for workers to send the same key lists each time. The server can cache the key lists upon the first reception. Subsequently, only the hash values of the key lists need to be sent for matching.</p> <p>Value Compression: Some parameter updates are not significant for final optimization, so users can define filter rules to discard unnecessary parameters. For instance, a large number of values being 0 or very small gradients can be inefficient in gradient descent and can be filtered out.</p> <p>Consistency and Replication: The parameter server uses consistent hashing to map keys and servers onto a ring according to a certain hash algorithm. Each server manages the key range from its insertion point counter-clockwise to another server. The server closest in the clockwise direction to a key on the ring is known as the primary server for that key range. Each server also backs up key ranges counter-clockwise, and these servers are known as backup servers for that key range. A physical server is often represented as multiple virtual servers to improve load balancing and fault tolerance.</p> <p>Chain Replication vs. Replication after Aggregation: Chain Replication: As shown in the left diagram, worker 1 updates x, server 1 processes the data with a custom function f(x), and then backs up f(x) to server 2. The push is completed only after worker 1 receives an acknowledgment. This backup method can cause significant network bandwidth overhead for algorithms requiring frequent parameter updates.</p> <p>Replication after Aggregation: As shown in the right diagram, the server aggregates updates from all workers before backing up and then sends acknowledgments to workers. Waiting for aggregation can introduce latency in pulling updates, but this can be mitigated by relaxing the consistency model.</p> <p>Server Management: Adding a Server: The server manager assigns a key range to the new server, and other servers adjust their key ranges accordingly. The new server acquires the key range it will maintain as the primary server and the key ranges it will back up as a backup server. The server manager broadcasts the changes to nodes. Removing a Server: When the server manager detects a server failure through heartbeat signals, it assigns the key range of the failed server to a new server and removes the failed server. Worker Management: Adding a Worker: The task scheduler assigns data to the new worker. The new worker loads training data and then fetches parameters from the server. The task scheduler broadcasts node changes, which may cause other workers to release some training data. Removing a Worker: Losing a small portion of training data typically doesn’t affect the training results. Moreover, restoring a worker requires more overhead than restoring a server. Therefore, removing a worker is usually done by simply disregarding the node. This can be used to terminate the slowest worker, mitigating the performance impact of stragglers. However, users can also choose to replace the lost worker with a new one.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Back to Blog Chapters](#blog-chapters)



## ** Chapter 5: Cloud Storage ** &lt;a name="topic-1"&gt;&lt;/a&gt;
##### Types of cs
</code></pre></div></div> <hr/> <p>blob arbitrary-sized “files” with simplified interface limited interface and semantics Example: AWS S3 Minimal promises re: sharing/concurrency, interrupted writes</p> <ul> <li>a big server</li> <li>a scalable service: aggregation of disk-equipped machines</li> </ul> <p>But with additional opportunities for simplification Object independence simplifies scaling (reduced coordination) Disallowing (or discouraging) object updates simplifies code paths, caching, use of space-saving encodings, etc.</p> <p>Non-FS interface also separates from legacy/kernel code • Standardized DFS implementations persist for a long time</p> <hr/> <p>Block stores (virtual disks) separate but attachable to any VM instance Guest OS running in a VM has code for FSs on disks So, give it a “disk” to use Virtual disk looks to guest OS just like real disk Most cloud infrastructures have this option AWS Elastic Block Store (EBS), OpenStack Cinder</p> <p>VDs often implemented as files A file is a sequence of bytes can hold a sequence of fixed-sized blocks</p> <p>Thin provisioning: Promise more, Allocate based on need Performance interference – Each VM may have a virtual disk</p> <h2 id="ioflow-a-software-defined-storage-architecture">IOFlow: a Software-Defined Storage Architecture.</h2> <p>“Local disk” as part of VM instance exists for lifetime of instance Key difference from #2-style block stores: visibility – visible only to the instance it comes with • makes sense, but can’t be attached to a different VM instance – exists only as long as the VM instance • can’t be attached to a different instance later ——————————— “Traditional” distributed FS (DIY or aaS)</p> <p>set up long-running instance(s) to be the DFS server(s) with block stores (#2) or storage-enhanced instance(s) (#3) running traditional DFS server software on the instance(s) DFS services can be provided by CSP or third party charge for file service CSP can implement a scalable file service and sell access</p> <hr/> <p>Provide a “union” filesystem on each client</p> <p>Make a single FS view from multiple FSs Implemented by a layer atop the individual FSs Each operation accesses “unioned” FSs as appropriate For read-only: Look in “first FS” first, then “second FS” if needed, … For creates/writes: put into “first” non-read-only FS</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

##### DFS in Cloud 

</code></pre></div></div> <p>```</p> <p><a href="#blog-chapters">Back to Blog Chapters</a></p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Advanced Cloud Computing Notes - SCS]]></summary></entry><entry><title type="html">Information Security - Currently Updating…</title><link href="https://ychen884.github.io/blog/2023/14741/" rel="alternate" type="text/html" title="Information Security - Currently Updating…"/><published>2023-08-29T11:00:00+00:00</published><updated>2023-08-29T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2023/14741</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/14741/"><![CDATA[<h3 id="course-evaluation-final-grade-a">Course Evaluation (Final grade: A)</h3> <p>This course is structured for those who have minimal or some previous experience in computer security. It covers a broad spectrum of topics and also delves deeply into each area. The exams present a considerable challenge, and the lab requires effort, depending on the student’s prior knowledge in security. Despite having studied information security and software security during my undergraduate studies, I found the labs in this course particularly engaging. Some topics, like cryptocurrency, privacy, and security management, were especially intriguing and somewhat new to me, adding a fresh perspective to my learning experience.</p> <h3 id="reference-book">Reference Book</h3> <h4 id="security-engineering-a-guide-to-building-dependable-systems-by-ross-j-anderson">Security Engineering: A Guide to Building Dependable Systems, by Ross J. Anderson</h4>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Info Sec - INI]]></summary></entry><entry><title type="html">Computer Systems - Currently Updating…</title><link href="https://ychen884.github.io/blog/2023/15513/" rel="alternate" type="text/html" title="Computer Systems - Currently Updating…"/><published>2023-08-29T11:00:00+00:00</published><updated>2023-08-29T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2023/15513</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/15513/"><![CDATA[<h3 id="course-evaluation-final-grade-a">Course Evaluation (Final grade: A)</h3> <p>This course is exceptional, thoroughly covering computer systems with engaging labs. The material is comprehensive yet manageable, ensuring exams and labs are not overwhelming. Highly recommended.</p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Computer Science - 213/513]]></summary></entry><entry><title type="html">Fundamentals of Telecommunication Networks - Currently Upating…</title><link href="https://ychen884.github.io/blog/2023/14740/" rel="alternate" type="text/html" title="Fundamentals of Telecommunication Networks - Currently Upating…"/><published>2023-08-28T11:00:00+00:00</published><updated>2023-08-28T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2023/14740</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/14740/"><![CDATA[<h3 id="course-evaluation-final-grade-a">Course Evaluation (Final grade: A)</h3> <p>This course is designed for individuals with minimal or no prior knowledge of computer networks. It delves into the intricate details of various protocols. The exams are generally easy to manage, but the labs may demand a significant amount of effort. Students should be aware that they will be sharing server resources with many others, leading to potential issues with resource contention. It also includes reviews of some state-of-the-art papers in the field.</p> <h3 id="paper-reviews">Paper reviews</h3> <p>I will upload my paper reviews here later.</p> <h3 id="reference-book">Reference Book</h3> <h4 id="computer-networking-7th-edition-james-kurose-and-keith-ross">Computer Networking 7th Edition, James Kurose, and Keith Ross</h4>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[computer networks - INI]]></summary></entry><entry><title type="html">Link to our Game Development Project - TimeOut</title><link href="https://ychen884.github.io/blog/2023/gameDev/" rel="alternate" type="text/html" title="Link to our Game Development Project - TimeOut"/><published>2023-08-27T18:37:00+00:00</published><updated>2023-08-27T18:37:00+00:00</updated><id>https://ychen884.github.io/blog/2023/gameDev</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/gameDev/"><![CDATA[<p>Check this out: https://rod233.itch.io/timeout I worked as a member of the SE team - Graphics.</p>]]></content><author><name></name></author><category term="Study"/><category term="Dev"/><summary type="html"><![CDATA[:) An interesting game dev project.]]></summary></entry><entry><title type="html">My Courses &amp;amp; Grades in UW-Madison</title><link href="https://ychen884.github.io/blog/2023/undergradCourses/" rel="alternate" type="text/html" title="My Courses &amp;amp; Grades in UW-Madison"/><published>2023-08-20T18:37:00+00:00</published><updated>2023-08-20T18:37:00+00:00</updated><id>https://ychen884.github.io/blog/2023/undergradCourses</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/undergradCourses/"><![CDATA[ <p>The University of Wisconsin-Madison offers an exceptional Computer Science track for undergraduate students, enabling them to delve into various fields within the discipline. During my undergraduate studies, I concentrated on system courses, which allowed me to gain valuable insights into this specialized area.</p> <p>Below is a breakdown of the courses I undertook at UW-Madison. The advanced courses have been highlighted at the beginning of the list for emphasis. I am profoundly grateful to my instructors, whose dedication and passion have significantly shaped my academic journey. Their guidance, coupled with my relentless hard work and consistent efforts, has truly made a difference in my education.</p> <p>The courses at UW-Madison are categorized into three distinct levels:</p> <p>Elementary (E): Typically, entry-level courses designed for freshmen and sophomores. Intermediate (I): These courses are aimed at students in their sophomore to junior years. Advanced (A): Targeted at students in their junior to senior years, these courses focus on more specialized and complex topics. The education I received at UW-Madison has not only broadened my knowledge but also laid a solid foundation for my future pursuits in the field of Computer Science.</p> <p>Above 100%: Extra credits given in that course</p> <table data-toggle="table" data-url="/assets/json/table_data.json"> <thead> <tr> <th data-field="Courses">Courses</th> <th data-field="Description">Description</th> <th data-field="Final Grade">Final Grade</th> </tr> </thead> </table> <p></p> ]]></content><author><name></name></author><category term="Study"/><category term="UW-Madison"/><summary type="html"><![CDATA[:) I enjoy all the courses I took in UW-Madison.]]></summary></entry></feed>