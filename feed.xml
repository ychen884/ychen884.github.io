<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ychen884.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ychen884.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-14T13:12:37+00:00</updated><id>https://ychen884.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Compiler Design Notes - Currently Upating…</title><link href="https://ychen884.github.io/blog/2024/15661/" rel="alternate" type="text/html" title="Compiler Design Notes - Currently Upating…"/><published>2024-01-30T11:00:00+00:00</published><updated>2024-01-30T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2024/15661</id><content type="html" xml:base="https://ychen884.github.io/blog/2024/15661/"><![CDATA[ <h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h3 id="paper-reviews">Paper reviews</h3> <p>I will upload my paper reviews here later.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 1: Overview of Compiler Design</a></li> <li><a href="#topic-1.1">Chapter 2: Instruction Selection</a></li> <li><a href="#topic-2">Chapter 3: Register Allocation</a></li> <li><a href="#topic-3">Chapter 4: Elaboration(after par/lex..)</a></li> <li><a href="#topic-4">Chapter 5: Static Semantics</a></li> <li><a href="#topic-5">Chapter 6: Grammars</a></li> <li><a href="#topic-6">Chapter 7: Lexical Analysis</a></li> <li><a href="#topic-7">Chapter 8: Function calls</a></li> <li><a href="#topic-8">Chapter 9: SSA</a></li> <li><a href="#topic-9">Chapter 10: Dynamic Semantics</a></li> <li><a href="#topic-10">Chapter 11: Mutable</a></li> <li><a href="#topic-11">Chapter 12: Dataflow Analysis</a></li> </ol> <h2 id="chapter-1-overview-of-compiler-design--">**Chapter 1: Overview of Compiler Design ** <a name="topic-1"></a></h2> <h3 id="what-makes-a-good-compiler-metrics">What makes a good Compiler: metrics</h3> <ul> <li>correctness</li> <li>code quality: compiled code runs fast</li> <li>efficiency: compilation runs fast</li> <li>usability: provides errors/warnings, …</li> </ul> <h3 id="compiler-design">Compiler Design</h3> <ul> <li>structure compilers</li> <li>applied alg &amp; data structures</li> <li>focus on sequential imperative programming languages</li> <li> <ul> <li>not functional, parallel, distributed, OOP…</li> </ul> </li> <li>code generation and optimization</li> </ul> <h3 id="organizing-a-compiler">Organizing a compiler</h3> <h4 id="front">Front</h4> <ul> <li>split work into different phases</li> <li>Lexical analysis -&gt; Token stream</li> <li>Parsing -&gt; Abstract syntax tree (mark body of while loop…)</li> <li>Sementic analysis (type check, variable initialization)</li> </ul> <h4 id="middle">Middle</h4> <ul> <li>IR(intermediate representation) Generation -&gt; Intermediate representations</li> <li>Optimize (most challenging)</li> </ul> <h4 id="back">Back</h4> <ul> <li>Instruction selection -&gt; Abstract assembly</li> <li>Register allocation -&gt; ASM Middle and Back has unclear distinctions</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-11-instruction-selection--">**Chapter 1.1: Instruction Selection ** <a name="topic-1.1"></a></h2> <ul> <li>Compiler phase</li> <li>IR tree -&gt; abstract assembly</li> </ul> <p>Example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = 5
return x+x+x*2

-&gt;&gt;&gt; Instruction selection
x = 5
temp1 = x + x 
temp2 = x * 2
ret_reg = t1 + t2
ret
</code></pre></div></div> <h5 id="ir-tree-more-expressions-statements">IR tree (more expressions, statements..)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Programs p ::= s1,...sn (sequence of statements)
statements s ::= x = e 
                return e

Expressions:
e ::= c int const
      x variable
      e1 ⊕ e2 binary OP (nested)
      ⊕ ::= +1 * 1 / 1 ...
</code></pre></div></div> <h5 id="abstract-assembly-flat">Abstract Assembly (flat)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Program: p ::= i1, ... in (instructions)
Instructions i::= d &lt;- s move
                = d &lt;- s1 ⊕ s2 bin op (sometimes one of the source works as dst)
                = ret return
Operands:
    d,s ::= r register (usually* finite numbers as defined)
          = c int const
          = t temps (variables)
          = x var

</code></pre></div></div> <h5 id="translations-expr">Translations Expr</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>translate(e1 ⊕ e2) = translate(e1); translate(e2);
res1 ⊕ res2?
Better: 
trans(d,e): seq of instructions that stores value of e in destination d. 


e           trans(d,e)
x           d &lt;- x
c           d &lt;- c
e1 ⊕ e2     trans(t1, e1), trans(t2, e2), d&lt;-t1⊕t2, (t1 and t2 are fresh temps)


</code></pre></div></div> <h5 id="translate-statements">Translate statements</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>trans'(s): seq of instru that inlements s
s           trans'(s)
x = e       trans(x,e)
return e    trans(ret,e) return (ret: return register)
</code></pre></div></div> <h5 id="example">Example</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IR prog: 
z = (x + 1) * (y * 4)
return z

trans'(p) 
= trans'(z = (x + 1) * (y * 4)), trans'(return z)
= trans(z,(x + 1) * (y * 4)),trans(ret, z), return
= trans(t1,x+1), trans(t2,y * 4), z&lt;- t1 * t2, ret&lt;-z, return
= t3 &lt;- x, t4 &lt;- 1, t1 &lt;- t3 + t4, t5 &lt;- y, t6 &lt;- 4, t2 &lt;- t5 * t6, z &lt;- t1*t2, ret&lt;-z, return
Optimize? directly use x and y instead of moving them to temps

</code></pre></div></div> <h5 id="how-to-improve">How to improve</h5> <ol> <li>Add special cases: for example c ⊕ e2</li> <li>Optimization pass after the first pass of translation (common approach)</li> <li>Different translation</li> </ol> <h5 id="constant-propagation">Constant propagation</h5> <ul> <li>goal: eliminate move t &lt;- c, p by replacing t with c in p</li> <li>But: stop replacing t if it’s written again <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example: 
t &lt;- 4
x &lt;- t+1   ---&gt; x &lt;- 4+1 
t &lt;- 5
ret &lt;- t   --NO--&gt; ret &lt;- 4
return 
</code></pre></div> </div> </li> </ul> <h5 id="copy-propagation">Copy propagation</h5> <ul> <li>goal: elim move d &lt;- t,p by replacing d with t in p, But: step replacing if d is written or if t is written</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

t &lt;- 5+1
d &lt;- t 
x &lt;- d+1 ----&gt; x &lt;- t+1
t &lt;- 5+2
ret &lt;- d+1 ---No---&gt; ret &lt;- t + 1
ret
</code></pre></div></div> <h5 id="static-single-assignment-form">Static Single Assignment Form</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- every temp is assigned at most once
- don't have to check "writes" but can replace all occurances in propagations
- Conversion to SSA -&gt; user version nums

t &lt;- 5+1
d &lt;- t 
x &lt;- d+1 ----&gt; x &lt;- t+1
t &lt;- 5+2
ret &lt;- d+1 ----No---&gt; ret &lt;- t + 1
ret

-----&gt;&gt;

t0 &lt;- 5+1
d0 &lt;- t0
x0 &lt;- d0+1
t1 &lt;- 5+2
...
</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-2-register-allocation--">**Chapter 2: Register Allocation ** <a name="topic-2"></a></h2> <ul> <li>Goal: assign registers and stack locations to temps <h5 id="x86-64-16-registers-no-temps">X86-64: 16 registers, no temps</h5> </li> <li>stack locations, when keeping track of more variables than registers <h5 id="strategy">Strategy</h5> <ol> <li>Store all temps on the stack (CON: inefficient, still need registers for efficiency)</li> </ol> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>IR trees(simplified syntax tree) 
--&gt; Instruction selection --&gt; ASM 
--&gt; reg alloc --&gt; ASM
--&gt; x86 asm


Example: 
d &lt;- s1 ⊕ s2
-&gt; reg alloc

rlld &lt;- exd * 4(rsp)
</code></pre></div></div> <h5 id="difficulty-x86-has-15-gen-purpose-registers">Difficulty: x86 has 15 gen purpose registers</h5> <ul> <li>Goal: assign each variable a register</li> <li>may have to use stack locations and clever use of registers for variables</li> </ul> <h5 id="interference">Interference:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x &lt;- 14
y &lt;- 15 
z &lt;- x+y

x &lt;- 14
y &lt;- 15 + x 
ret &lt;- 4+y
ret
if x is not used again, we can use overwrite the register for y
</code></pre></div></div> <h5 id="rigth-ir-for-reg-alloc">Rigth IR for reg alloc?</h5> <h5 id="3-addr-abs-asm">3 addr abs asm</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d &lt;- s1 + s2
d &lt;- s1 / s2
...
</code></pre></div></div> <h5 id="2-addr-abs-asm">2 addr abs asm</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d &lt;- s1
d &lt;- d + s2

d &lt;- s1
d &lt;- d / s2
...

</code></pre></div></div> <h5 id="abstract-x86">abstract x86</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MOVL S1,d
ADDL S2,d
...
IDIVL
edx:eax / s2 -&gt; eax

MOVL s1, %eax
CLTD (sign-extends eax into edx:eax.)
----
https://stackoverflow.com/questions/17170388/trying-to-understand-the-assembly-instruction-cltd-on-x86
What this means in practice is that edx is filled with the most significant bit of eax (the sign bit). For example, if eax is 0x7F000000 edx would become 0x00000000 after cdq. And if eax is 0x80000000 edx would become 0xFFFFFFFF.
----
IDIVL s2 (edx:eax / s2)
MOVL %eax, d

</code></pre></div></div> <h5 id="reg-alloc-at-3-addr-assem">Reg Alloc at 3-Addr Assem</h5> <ul> <li>leave one register unassigned for later conversion (r11d)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For example

d &lt;- s1 ⊕ s2
--&gt; 
MOVL s1, %r11d
ADDL s2, %r11d
MOVL %r11d, d
(one of them has to be a register
this will always work, but may not be the optimzied solution)

</code></pre></div></div> <h5 id="reg-alloc-at-2-addr-assem">Reg Alloc at 2-Addr Assem</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>For example

d &lt;- s1 + s2
--&gt; 
d &lt;- s1
d &lt;- s2 + d
</code></pre></div></div> <h5 id="how-to-allocate-registers">How to allocate registers?</h5> <h6 id="goal-minimize-spilling-to-the-stack">Goal: minimize spilling to the stack</h6> <h6 id="one-method-graph-based-greedy-register-allocation">One method: graph-based, greedy register allocation</h6> <h6 id="build-interference-graph">Build interference graph</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Vertices are registers and temps
Edges indicate interference (need diff registers)

Example: 
t1  -- t2 -- t3 -- eax
        |           |
         -----------

Complexity pf deciding if two temps interfere? Undecidable
Why? Reduction to the halting problem
x &lt;- 5
y &lt;- 6
complex code (doesn't use x and y)
ret&lt;- x + y
return

x and y interfere iff complex code terminates


Approximate Interference with liveness
Liveness: temp t is live at line l if t might be used in the 
following computation

For L1: work backwards
temp t is live at l if either of them is valid:
- t is read at l
- t is live at l+1 and not written at line l

Example: 
x1 &lt;- 1            -
x2 &lt;- 1           x1
x3 &lt;- x1 + x2     x2,x1
x4 &lt;- x3 + x2     x3,x2
x5 &lt;- x4 + x3     x3,x4
ret &lt;- x5         x5
return ret        ret register

life range of t1: 2-3
life range of x2: 3-4
..


t1 &lt;- 1     -
t1 &lt;- t1+1  t1
t2 &lt;- t1    t1

Construct interf graph:
Option 1: Add edge between t1 and t2 if they have overlapping live ranges
Option 2: rule 1: For every instruction d &lt;- s1 ⊕ s2
                        add an edge {d,t} if t is  live at the next instruction
          rule 2: For every move d &lt;- s, add an edge {d,t} 
                  that t not in {s,d}, and is live at the next instruction


</code></pre></div></div> <h5 id="find-coloring-with-as-few-colors-as-possible-adjacent-vertices-have-different-colors">Find coloring with as few colors as possible: Adjacent vertices have different colors</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Given: Graph G = (V,E) and k colors
Question: Is there a coloring of V with k colors such that adjacent verticies have different colors
Complexity: NP complete for k &gt;= 3 

What folllows for reg allocation? 

For a Turing-comp lang
It's undecidable for a prog if there's an equiv prog that uses k registers (and no tem)

</code></pre></div></div> <h5 id="assign-colors-to-registers-and-stack-locations">Assign colors to registers and stack locations</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Greedy graph coloring: get a minimal coloring for most programs

Assume colors are number 1,2,3...

Let N(v) be the nbs of v
Input: G(V,E) and ordering V=v1,...,vn
Output: Coloring of V: col:V-&gt;{1,..k}

- Order matters

x5 

x4-x3-x2-x1

Order 1: x5,x4,x3,x2,x1,ret

Order 2: x1,x4,x3,x2,x5,ret


</code></pre></div></div> <h5 id="th-there-exists-an-ordering-that-produces-an-optimal-coloring">Th: There exists an ordering that produces an optimal coloring</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>How to find optimal ordering for chordal graphs?

Def: A graph is chordal if every cycle with &gt;= 4 vertices has a chord 
(edge between two verticies in the cycle that is not in the cycle)

Example

a - b
|   |
c - d
not chordal

a - b
| / |
c - d
chordal

a - b
| x |
c - d
chordal


Intuition: how to get long cycle w/o chord?
a over lap with b and c
we want d to be not overlapping with a


</code></pre></div></div> <h5 id="maximum-cardinality-search">Maximum cardinality search</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: G, Op: ordering of v
for each vertex in V, set wt(v) &lt;- 0

For all v ∈ V set wt(v) ← 0
Let W ← V
For i ← 1 to n do
      Let v be a node of maximal weight in W
      Set vi ← v
      For all u ∈ W ∩ N(v) set wt(u) ← wt(u) + 1
      Set W ← W \ {v}


MCS Ordering returns: simplicial elimination ordering if G is chordal
Def: v is simplicial in G in N(v) is a clique
A simplicial vertex is one whose neighbors form a clique: every two neighbors are adjacent. 


Def: A simpl elim ordering is an ord v1, ... vn st vi is simplicial in
Gv1,..vi &lt;- subgraph induced by v1, .. vi (picked)
</code></pre></div></div> <h5 id="theorem-1-the-graph-is-chordal-iff-it-has-an-simplicial-elim-ordering">Theorem 1: The graph is chordal iff it has an simplicial elim ordering</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Proof..
</code></pre></div></div> <h5 id="theorem-2-the-greedy-coloring-alg-finds-an-opt-coloring-if-run-with-a-simpl-elim-ordering">Theorem 2: The greedy coloring alg finds an opt coloring if run with a simpl elim ordering</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Proof: Let k be the number of colors used
Observations: 
- k &lt;= max|Ni(vi)| + 1
- #min colors &gt;= max|Ni(vi)| + 1
</code></pre></div></div> <h5 id="--theorem-mcs-returns-a-sompl-elem-ordering-iff-g-is-chordal">-&gt; Theorem: MCS returns a sompl elem ordering iff G is chordal</h5> <h5 id="spill-assign-remaining-colors-to-stack-loc">Spill: assign remaining colors to stack loc.</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>strategies: 
1. number of uses of temps in code, higher use freq get the registers
2. incorp loop nestings
3. highest colors (high color may be used in less time, approx for 1)

</code></pre></div></div> <h4 id="summary">Summary</h4> <ol> <li>Build the interference graph (different ways)</li> <li>Order the vertices with MCS</li> <li>Color with greedy alg</li> <li>Spill if # colors &gt; 13</li> </ol> <h4 id="liveness-analysis--interence-rules">Liveness analysis &amp; interence rules</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>live(l,x) ~ x i live at line l
d != u
l: d &lt;- x ⊕ y live(l+1, u)
--------------------------
      live(l, u)


l: d&lt;- x ⊕ y
------------------
    live(l, x)
    live(l, y)

l: return
------------------
   live(l, ret)


l: x &lt;- c
live(l+1, u), x!=u
-------------------
      live(l,u)

l: x&lt;-y, u!=x
live(l+1, u)
--------------
live(l,u)


l:x&lt;-y
-------
live(l,y)

Using derivation tree to prove if x live at line l

</code></pre></div></div> <h5 id="general-saturation-alg">General Saturation Alg</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Can be used for all predicates
1: start with empty set of facts &lt;- derived predicates
2: pick arg (here: l and t) and a rule with live(l,t) in the conclusion
so that the premises are already facts
3: Repeat until no facts can be derived

Will always stop
</code></pre></div></div> <h5 id="refactoring-liveness-rules">Refactoring liveness rules</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use(l,x)
---------
live(l,x)

*: l' is a possible succesor
live(l'x) succ(l,l') ~def(l,x) 
---------------------------------
      live(l,x)

</code></pre></div></div> <h5 id="need-to-define">Need to define:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use(l,x) ~ x is used at l 
def(l,x) ~ x is def at l
succ(l,l') ~ l' can be a succ of l
</code></pre></div></div> <h5 id="example-1">Example</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>l: d &lt;- x ⊕ y
-------------
use(l,x)
use(l,y)
def(l,d)
succ(l,l+1)
</code></pre></div></div> <h5 id="adding-loops-and-conditionals">Adding loops and conditionals:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>i = l: d &lt;- x ⊕ y
.
.
.
l: goto l' (uncond jump)
l: if(x?c) then lt then lt else lf(cond jump)

New rules:

l: goto l'
-----------
succ(l,l')

l: if(x'c) then lt
else lf
-------------------
succ(l,lt)
succ(l,lf)
use(l,x)


Keep doing iterations until we cannot add more new to the liveness sets
pass 1, pass 2, ...

</code></pre></div></div> <h5 id="interference-graph">Interference graph</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Overlapping live ranges

live(l,x) live(l,y)
-------------------
inter(x,y)

But doesn't work in the presence of that code dead code

Example: 
a &lt;- 1
b &lt;- 2
ret &lt;- a
return


2. Assignment based
More sparse

l: x &lt;- y ⊕ z live(l+1, u), u!=x
---------------------------------
inter(x,u)

l:x&lt;-y, y!=u
live(l+1, u) x!=u
------------------
inter(x,u)

l: x&lt;-c, u!=x
live(l+1, u)
--------------
inter(x,u)
</code></pre></div></div> <h2 id="chapter-3-elaboration-">Chapter 3: Elaboration <a name="topic-3"></a></h2> <h5 id="goal-minimal-repr-and-clear-of-progr">Goal: minimal repr and clear of progr</h5> <ul> <li>remove syntactic sugar</li> <li>make scope explicit</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parsing -&gt; Parse tree -&gt; elaboration -&gt; AST(&lt;-Semantic analysis)

Example: 
turn for lops into while loops

for(int x=4, x&lt;8128, x++){
      y = y+x
}
------&gt; elab
use declaration(x,int,s)
{int x=4; while(x&lt;8128)..}
decl(x, int, while(x&lt;8218){..})

// Before elaboration
for (int x = 4; x &lt; 30; x++) { y = y + x }


// After elaboration
{
int x = 4;
while (x &lt; 30) { y = y + x; x += 1 }
}

abstract syntax

declare(x, int,seq(assign(x, 4),
                  while(x &lt; 30,
                  seq(assign(y, y + x), assign(x, x + 1)))))

The extra scope is necessary
Rather than continuing to perform manipulations on surface syntax, we introduce the BNF of an elaborated abstract syntax


Expressions e ::= n | x | e1 ⊕ e2 | e1 /o e2 | f(e1, . . . , en)
| e1 ? e2 | !e | e1 &amp;&amp; e2 | e1 || e2

/0:  potentially effectful operators (such as division or shift, which could raise an exception)

⊕ for effect-free operators
? for comparison operators returning a boolean, !, &amp;&amp;, and || for logical negation, conjunction, and disjunction, respectively

Statements s ::= 
declare(x, τ, s) | 
assign(x, e) | 
if(e, s1, s2) | 
while(e, s)| 
return(e) | 
nop | 
seq(s1, s2)


</code></pre></div></div> <h5 id="inf-rule">Inf rule</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;tp&gt; τ
&lt;ident&gt; x
&lt;exp1&gt; e1
&lt;exp2&gt; e2
&lt;stmt1&gt; s1
&lt;stmt2&gt; s2
-----------------------------------------------------
for (&lt;tp&gt; &lt;ident&gt; = &lt;exp1&gt;; &lt;exp2&gt;; &lt;stmt1&gt;) &lt;stmt2&gt;
 declare(x, τ,while(e1,seq(s2, s1)))

When we want to translate a for loop that
matches the pattern at the bottom, we have to do six things: 
elaborate the type,
elaborate the identifier, 
elaborate both expressions, 
elaborate the afterthought, and
then elaborate the loop body. 
x++ -&gt; assign(x, x + 1)
make sure any nested for loops in &lt;stmt2&gt;, the body of our for loop, are elaborated

</code></pre></div></div> <h5 id="why-ir-tree">Why IR tree</h5> <h5 id="isolate-potentially-effectful-expressions-making-their-order-of-execution-explicitsimplifies-instruction-selection-and-also-means-that-the-remaining-pure-expressions-can-be-optimized-much-more-effectively">isolate potentially effectful expressions, making their order of execution explicit.simplifies instruction selection and also means that the remaining pure expressions can be optimized much more effectively.</h5> <h5 id="make-the-control-flow-explicit-in-the-form-of-conditional-or-unconditional-branches-which-is-closer-to-the-assembly-language-target-and-allows-us-to-apply-standard-program-analyses-based-on-an-explicit-control-flow-graph">make the control flow explicit in the form of conditional or unconditional branches, which is closer to the assembly language target and allows us to apply standard program analyses based on an explicit control flow graph.</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pure Expressions p ::= n | x | p1 ⊕ p2

Commands c 
::= 
x ← p
| x ← p1 /0 p2
| x ← f(p1, . . . , pn)
| if (p1 ? p2) then lt else lf
| goto l
| l :
| return(p)
Programs r ::= c1 ; . . . ; cn


</code></pre></div></div> <h5 id="translating-expressions">Translating Expressions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tr(e) = &lt;ˇe, eˆ&gt;
where eˇ is a sequence of commands r that we need to write down to compute the
effects of e and eˆ is a pure expression p that we can use to compute the value of e back up. 


tr(n) = &lt;·, n&gt;
tr(x) = &lt;·, x&gt;
tr(e1 ⊕ e2) = &lt;(ˇe1 ; ˇe2), eˆ1 ⊕ eˆ2&gt;?


</code></pre></div></div> <h5 id="translating-statements">Translating Statements</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tr(assign(x, e)) = ˇe ; x ← eˆ
tr(return(e)) = ˇe ; return(ˆe)
tr(nop) = ·
tr(seq(s1, s2)) = ˇs1 ; ˇs2


tr(if(e, s1, s2)) = ˇe ;
if (ˆe != 0) then l1 else l2 ;
l1 : ˇs1 ;
goto l3 ;
l2 : ˇs2 ;
l3 :



tr(while(e, s)) = l1 : ˇe;
if (ˆe != 0) then l2 else l3 ;
l2 : ˇs ; goto l1;
l3 : 
</code></pre></div></div> <h5 id="translating-boolean-expressions">Translating Boolean Expressions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cp(e1 ? e2, l, l') = 
ˇe1 ; ˇe2 ;
if (ˆe1 ? eˆ2) then l else l'


cp(!e, l, l') = cp(e, l', l)

cp(e1 &amp;&amp; e2, l, l') = 
cp(e1, l2, l');
l2 : cp(e2, l, l')


cp(e1 || e2, l, l') = left to the reader

cp(0, l, l') = goto l'

cp(1, l, l') = goto l

cp(e, l, l') = 
ˇe ;
if (ˆe != 0) then l else l'


tr(if(b, s1, s2)) = cp(b, l1, l2)
l1 : tr(s1) ; goto l3
l2 : tr(s2) ; goto l3
l3 :


tr(e) = &lt;cp(e, l1, l2);
l1 : t ← 1 ; goto l3
l2 : t ← 0 ; goto l3
l3 :
, t&gt;
</code></pre></div></div> <h5 id="extended-basic-blocks">Extended basic blocks</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> instead of basic blocks being sequences of commands ending
in a jump, they are trees of commands that branch at conditional statements and
have unconditional jumps (goto or return) as their leaves
</code></pre></div></div> <h2 id="chapter-4-static-semantics-">Chapter 4: Static Semantics <a name="topic-4"></a></h2> <h5 id="abstract-syntax">abstract syntax</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>After lexing and parsing, a compiler will usually apply elaboration to translate the parse tree to a high-level intermediate form often called abstract syntax. Then we verify that the abstract syntax satisfies the requirements of the static semantics.


C0:
• Initialization: variables must be defined before they are used.
• Proper returns: functions that return a value must have an explicit return statement on every control flow path starting at the beginning of the function.
• Types: the program must be well-typed.
</code></pre></div></div> <h5 id="abstract-syntax-as-defined-in-last-chap">Abstract Syntax as defined in last chap</h5> <h5 id="def">Def</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>we would like to raise an error if there is a possibility that
an unitialized variable may be used. 

may-property
use(e1/2, x)
-----------
use(e1 &amp;&amp; e2, x)

use(e1/2, x)
-----------
use(e1 ⊕ e2, x)


must-property
def(s, x) if the execution of statement s will define x.


--------------------
def(assign(x, e), x)



def(s1, x) def(s2, x)
-----------------------
def(if(e, s1, s2), x)

A conditional only defines a variable if is it defined along
 both branches, and a while loop does not define any variable 
 (since the body may never be executed).

def(s1/2, x)
-----------------
def(seq(s1, s2), x)

def(s, x) y != x
---------------------
def(decl(y, τ, s), x)
x is declared at most once on a control-flow
path. 

--------------------
def(return(e), x)
Since a return statement will never
pass the control flow to the next instruction, any subsequent statements are unreachable. It is therefore permissible to claim that all variables currently in scope have been defined. 
</code></pre></div></div> <h6 id="liveness">Liveness</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
We observe that liveness is indeed a may-property, since a variable is live in a conditional if is used in the condition or live in one or more of the branches.


use(e, x)
----------------
live(assign(y, e), x)


use(e, x)
----------------
live(if(e, s1, s2), x)


use(s1, x)
----------------
live(if(e, s1, s2), x)


use(s2, x)
----------------
live(if(e, s1, s2), x)

use(e, x)
----------------
live(while(e, s), x)

live(s, x)
----------------
live(while(e, s), x)

use(e, x)
----------------
live(return(e), x)

no rule for
live(nop, x)

live(x, s) y != x
----------------
live(decl(y, τ, s), x)


live(s1, x)
----------------
live(seq(s1, s2), x)

¬def(s1, x) live(s2, x)
----------------
live(seq(s1, s2), x)

</code></pre></div></div> <h6 id="initialization">Initialization</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>captures the general condition.


should be read from the premises
to the conclusion.
decl(x, τ, s) in p   live(s, x)
------------------------------
error


from the conclusion to the premises


------------------------------
init(nop)


init(s1) init(s2)
------------------------------
init(seq(s1, s2))


init(s) ¬live(s, x)
------------------------------
init(decl(x, τ, s))
</code></pre></div></div> <h5 id="from-judgments-to-functions">From Judgments to Functions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>init : stm → bool
init(nop) = T
init(seq(s1, s2)) = init(s1) ∧ init(s2)
init(decl(x, τ, s)) = init(s) ∧ ¬live(s, x)

live(nop, x) = ⊥
live(seq(s1, s2), x) = live(s1, x) ∨ (¬def(s1, x) ∧ live(s2, x))
live(decl(y, τ, s), x) = y != x ∧ live(x, s)
. . .

...
init(δ, s, δ'): 
assuming all the variables in δ are defined when s is reached, 
no uninitialized variable will be referenced and after its execution 
all the variables in δ' will be defined.

use(δ, e): e will only reference variables defined in δ.

δ |- s ⇒ δ' for init(δ, s, δ').
δ |- e for use(δ, e).

δ ⊢ s1 ⇒ δ1
δ1 ⊢ s2 ⇒ δ2
______________
δ ⊢ seq(s1, s2) ⇒ δ2


δ ⊢ e
_________
δ ⊢ assign(x, e) ⇒ δ ∪ {x}


δ ⊢ e
δ ⊢ s1 ⇒ δ1
δ ⊢ s2 ⇒ δ2
_________________
δ ⊢ if(e, s1, s2) ⇒ δ1 ∩ δ2


δ ⊢ e
δ ⊢ s ⇒ δ'
_________
δ ⊢ while(e, s) ⇒ δ



In particular, declare(x, τ, s) means that the variable x is declared (only) within the statement s.
δ ⊢ s ⇒ δ'
___________
δ ⊢ decl(y, τ, s) ⇒ δ' - {y}


δ ⊢ e
_________
δ ⊢ return(e) ⇒ {x | x in scope}



Typing Judgment for Statements:
The notation Γ ⊢ s : [τ] is a typing judgment for statements,
where Γ is the type environment (context), 
s is a statement, 
and τ is the type. 
This notation signifies that the statement s is well-typed within the 
context Γ and ultimately returns a value of type τ
</code></pre></div></div> <h2 id="chapter-5-grammars--">**Chapter 5: Grammars ** <a name="topic-5"></a></h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Grammars are a general way to describe languages.
For a given grammar G with start symbol S, a derivation in G is a sequence of rewritings
S → γ1 → · · · → γn = w

A context-free grammar consists of a set of productions of the form X −→ γ, where X is a non-terminal symbol and γ is a potentially mixed sequence of terminal and non-terminal symbols.

While the parse tree removes some ambiguity, it turns out that the example grammar is ambiguous in other, more important, ways.

we can add that string specifically as the new base case
It is important that programming languages be unambiguous in practice. 
 
We can usually rewrite grammars to remove ambiguity, but sometimes we extend the language of context-free grammars to resolve ambiguity. One example is explicitly stating precedence and associativity as a way of resolving ambiguities.

</code></pre></div></div> <h5 id="cyk-parsing">CYK Parsing</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CYK Parsing

The Cocke-Younger-Kasami (CYK) parsing algorithm is a method used to check if a given string can be generated by a context-free grammar (CFG). 
It's especially useful in computational linguistics and computer science for parsing strings. The algorithm requires the CFG to be in Chomsky Normal Form (CNF), 
where production rules are either a non-terminal producing two non-terminals, or a non-terminal producing a terminal.

How CYK Works
Input and Initialization: The algorithm takes a string w of length n and a CFG G in CNF. It initializes a table to store which grammar symbols can generate which substrings of w.

Filling the Table: The algorithm fills a table P, where P[i,j] holds non-terminal symbols that can generate the substring w[i,j]. It starts with the smallest substrings 
(single characters) and combines them to form larger substrings.

For each terminal in w, it adds corresponding non-terminals to P as per the grammar rules.
For each possible split of substrings, it checks if their combination can be generated by any non-terminals as per the grammar rules.
Iterating Over Substrings: It considers all possible substrings of w by their length, from shortest to longest, and checks all possible splits for each substring, looking 
for non-terminals that could generate these parts.

Checking for Acceptance: After filling the table, it checks if the start symbol S is in the top-right cell of the table (P[1,n]). 
If S is present, the string w is accepted by the grammar.

Complexity
The CYK algorithm has a time complexity of O(n^3 * |G|), where n is the length of the string and |G| is the size of the grammar. This complexity comes from considering all substrings, 
all possible splits of each substring, and the grammar size.

Applications
The CYK algorithm is used in syntax checking, natural language processing, and compiler design, where it's important to determine if strings follow specific grammatical rules.

By using dynamic programming, the CYK algorithm efficiently parses strings and determines their grammatical structure under a given CFG.

https://www.youtube.com/watch?v=VTH1k-xiswM
</code></pre></div></div> <h5 id="shift-reduce-parsing">Shift-reduce parsing</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Deductive parsing involves two primary rules originally used in the CYK parsing algorithm, but when we interpret these for shift-reduce parsing, 
we make some modifications.

Rule D1: This is an assertion rule where for any terminal symbol 'a' found in the input string, we can deduce that 'a' derives itself. This is represented as:

a : a


Rule D2: This is a combination rule that states if you have a production in the grammar where a non-terminal X leads to a sequence of symbols (γ1 ... γn), 
and you have established that for substrings w1 through wn of your input string w0, each wi derives γi, 
then you can conclude that the concatenated string w1...wn is derived from X. This rule is written as:

[r] X → γ1 ... γn
w1 : γ1 ... wn : γn
------------------------
w1 ... wn : X


Certainly! Let's put the explanation of the rules for deductive shift-reduce parsing in plain text form:

Deductive parsing involves two primary rules originally used in the CYK parsing algorithm, but when we interpret these for shift-reduce parsing, we make some modifications.

Rule D1: This is an assertion rule where for any terminal symbol 'a' found in the input string, we can deduce that 'a' derives itself. This is represented as:


a : a
Rule D2: This is a combination rule that states if you have a production in the grammar where a non-terminal X leads to a sequence of symbols (γ1 ... γn), 
and you have established that for substrings w1 through wn of your input string w0, each wi derives γi, 
then you can conclude that the concatenated string w1...wn is derived from X. This rule is written as:


[r] X → γ1 ... γn
w1 : γ1 ... wn : γn
------------------------
w1 ... wn : X

When we adapt these rules for shift-reduce parsing, we modify the form of the facts used:

Instead of w : γ, where γ is a sequence of terminals and non-terminals, we look to conclude facts of the form w : β, 
where β is a possibly empty series of terminals and non-terminals.

We then have two operations: shift and reduce, focusing on the rightmost elements of w or β. The rules are:

Start Rule:
This is the base case for parsing, where you start with an empty string deriving an empty string:
ε : ε

Shift Rule:
If you have a fact w : β and 'a' is a terminal symbol next in the input string, you can append 'a' to both w and β:
w : β
w a : β a


Reduce Rule:
If there's a production where X leads to α and you have a string w followed by α in β, you can reduce this by recognizing α as a unit derived from X:

[r] X → α
w : β α
------------
w : β X

The reduce rule is the key to recognizing higher-level structures. Each time you apply it, you replace a sequence of symbols 
(which matches the right-hand side of a production rule) with the non-terminal symbol from the left-hand side of that production rule.

If a grammar is unambiguous, this means
that, as we apply a series of rules to try to derive w : S from ε : ε, there is at most one rule we can apply that will lead us to success.

If we can successfully predict what the next step should be at every point, then we can implement this proof search with a stack 
holding the terminals and nonterminals (on the left) and a queue or array index tracking the unprocessed tokens (on the right).

What would we need to know how to always make the right decision for the grammar above?



We will begin constructing a parse table, where the columns
correspond to the next unshifted token and the rows correspond to patterns that we match against the stack β.
</code></pre></div></div> <h5 id="parse-table">Parse Table</h5> <h5 id="conflicts">Conflicts</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Cases where we can make different decisions and still successfully parse the string are called conflicts.

Example:
we have decided to resolve the conflicts by giving a precedence to the operators and declaring both of them to be left-associative

It is also possible to have reduce/reduce conflicts

For many parser generators, the default behavior of a shift/reduce conflict is to shift, and for a reduce/reduce conflict to apply the textually first production in the grammar.
</code></pre></div></div> <h5 id="lr1-and-more">LR(1) and more?</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
The set of languages that we can parse with shift-reduce parsers that have lookahead 1 is called LR(1). But even though a language may be 
describable with an LR(1) grammar, it’s not necessarily the case that every grammar for an LR(1) language can be parsed with a shift-reduce parser. 


Even though the grammar is unambiguous, to parse it correctly, we’d need arbitrary lookahead – we’d need to look over an arbitrary 
number of b tokens to find whether they were followed by a c or a d.
</code></pre></div></div> <h2 id="chapter-6-lexical-analysis--">**Chapter 6: Lexical Analysis ** <a name="topic-6"></a></h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>turn a raw byte or character input stream coming from the source file into a token stream by chopping the input into pieces and skipping over irrelevant details.

classify input tokens into types like INTEGER or IDENTIFIER or WHILE-keyword or OPENINGBRACKET...

Lexers are specified by regular expressions. Classically, however, they are implemented by finite automata.


deterministic finite automaton (DFA). At every state and every input there is at most one edge enabling a transition.


But in general, finite automata can be nondeterministic finite automata (NFA). That is, for the same input, one path may lead to
an accepting state while another attempt fails.


</code></pre></div></div> <h5 id="regular-expressions---nondeterministic-finite-automata">Regular Expressions -&gt; Nondeterministic Finite Automata</h5> <h5 id="nondeterministic-finite-automata---deterministic-finite-automata">Nondeterministic Finite Automata -&gt; Deterministic Finite Automata</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> instead of a single state, we now consider the set of states in which we could be.


Another operation that is often done by lexer generator tools is to minimize the resulting DFA by merging states and reducing the number of states and transitions in the automaton.

Lexical analysis reduces the complexity of subsequent syntactical analysis by first dividing the raw input stream up into a shorter sequence of tokens, 
each classified by its type (INT, IDENTIFIER, REAL, IF, ...). 
The lexer also filters out irrelevant whitespace and comments from the input stream so that the parser does not have to deal with that anymore. 
The steps for generating a lexer are


1. Specify the token types to be recognized from the input stream by a sequence of regular expressions

2. Bear in mind that the longest possible match rule applies and the first production that matches longest takes precedence.

3. Lexical analysis is implemented by DFA.

4. Convert the regular expressions into NFAs (or directly into DFAs using derivatives).

5. Join them into a master NFA that chooses between the NFAs for each regular expression by a spontaneous ε-transition

6. Determinize the NFA into a DFA

7. Optional: minimize the DFA for space

8. Implement the DFA for a recognizer. Respect the longest possible match rule by storing the last accepted token and 
backtracking the input to this one if the DFA run cannot otherwise complete.

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-8-function-calls--">**Chapter 8: Function Calls ** <a name="topic-7"></a></h2> <h5 id="ir">IR</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>two lower-level forms of intermediate representation for function calls. 
1. d ← f(s1, . . . , sn)
2. call f

</code></pre></div></div> <h5 id="calling-conventions">calling conventions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameters
first six arguments are passed in registers, the remaining arguments
are passed on the stack
all arguments take 8 bytes of space on the stack, even if the type of
argument would indicate that only 4 bytes need to be passed

callee, should set up its stack frame, reserving space for local variables, spilled temps that could not be assigned to registers, 
and arguments passed to functions it calls in turn

It is recommended to calculate the total space needed statically and then decrementing the stack pointer %rsp by the appropriate amount only one time within the function

%rsp should be aligned 0 mod 16 before another function is called,
and may be assumed to be aligned 8 mod 16 on function entry.


Return
The result is returned in a specific return register %rax.





...
n + 16(%rsp) argument 8
n + 8(%rsp) argument 7
n + (%rsp) return address
local variables         Callee
(%rsp) end of frame     Callee


</code></pre></div></div> <h5 id="registers-convention">Registers convention</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Abstract x86-64 
form Register Usage function Preserved accross calls
res0 %rax return value∗ No
arg1 %rdi argument 1 No
arg2 %rsi argument 2 No
arg3 %rdx argument 3 No
arg4 %rcx argument 4 No
arg5 %r8 argument 5 No
arg6 %r9 argument 6 No
ler 7 %r10 caller-saved No
ler 8 %r11 caller-saved No

lee9 %rbx callee-saved Yes
lee10 %rbp callee-saved∗ Yes
lee11 %r12 callee-saved Yes
lee12 %r13 callee-saved Yes
lee13 %r14 callee-saved Yes
lee14 %r15 callee-saved Yes
%rsp stack pointer Yes


</code></pre></div></div> <h5 id="typical-calling-sequence">Typical Calling Sequence</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>d ← f(s1, s2, s3):

arg3 ← s3
arg2 ← s2
arg1 ← s1
call f
d ← res0


l : call f
caller-save(r)
---------------- J'8
def(l, r)

caller-save(r) is true of register r among %rax, %rdi, %rsi, %rdx, %rcx, %r8, %r9, %r10, and %r11.

Now if a temp t is live after a function call, we have to add an infererence edge connecting t with any of the fixed registers 


Callee-saved Registers
%rbx, %rbp, %r12, %r13, %r14 and %r15
The standard approach is to save those that are needed onto the stack in the function prologue and restore them from the stack in the function epilogue, just before returning.
**: saving and restoring them all is safe, but may be overkill
for small functions

If we need more than the available number of caller-saved registers, we assign callee-save registers before we resort to spilling,
but make sure the save them at the beginning of a function and restore them at the end. 


f :
t1 ← lee9
t2 ← lee10
· · ·
function body
· · ·
lee10 ← t2
lee9 ← t1
ret


</code></pre></div></div> <h5 id="liveness-about-calling-convention">Liveness about calling convention</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. During a call, res0, arg1-6 and ler7, ler8 are defined.
2. For each line l and each temp t defined at l, we create an edge between t and any variable live in the successor.



All precolored registers implicitly interfere with each other, so we don’t include that in the interference graph.


</code></pre></div></div> <h2 id="chapter-9-ssa--">**Chapter 9: SSA ** <a name="topic-8"></a></h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>relabel variables in the code so that each variable is defined
only once in the program text. If the program has this form, called static single assignment (SSA),


then we can perform constant propagation immediately
There are other program analyses and optimizations for which it is convenient to have this property..



</code></pre></div></div> <h5 id="basic-blocks">Basic blocks</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>int dist(int x, int y) {
x = x * x;
y = y * y;
return isqrt(x+y);
}

dist(x0,y0):
x1 &lt;- x0 * x0
y1 &lt;- y0 * y0
t0 &lt;- x1 + y1
t1 &lt;- isqrt(t0)
return t1



</code></pre></div></div> <h5 id="loops">Loops</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>int pow(int b, int e)
//@requires e &gt;= 0;
{
int r = 1;
while (e &gt; 0)
//@loop_invariant e &gt;= 0;
// r*b^e remains invariant
{
r = r * b;
e = e - 1;
}
return r;
}

pow(b0,e0):
      r0 &lt;- 1
      goto loop(b0,e0,r0)
loop(b1,e1,r1):
      if (e1 &gt; 0)
            then body(b1,e1,r1)
            else done(b1,e1,r1)
body(b2,e2,r2):
      r3 &lt;- r2 * b2
      e3 &lt;- e2 - 1
      goto loop(b2,e3,r3)
done(b3,e4,r4):
      return r4

</code></pre></div></div> <h5 id="minimize-ssa-form">Minimize SSA Form</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>φ Function form to minimized form.
It’s quite simple: repeatedly remove φ-functions of the form
ti = φ(tx1, tx2, . . . txk)


pow(b0,e0):
r0 &lt;- 1
      goto loop
loop:
      b1 &lt;- phi(b0,b2)
      e1 &lt;- phi(e0,e3)
      r1 &lt;- phi(r0,r3)
      if (e1 &gt; 0) then body else done
body:
      b2 &lt;- phi(b1)
      e2 &lt;- phi(e1)
      r2 &lt;- phi(r1)
      r3 &lt;- r2 * b2
      e3 &lt;- e2 - 1
      goto loop

done:
      b3 &lt;- phi(b1)
      e4 &lt;- phi(e1)
      r4 &lt;- phi(e1)
      return r4

-------------------------------------------
pow(b0,e0):
      r0 &lt;- 1
      goto loop
loop:
      e1 &lt;- phi(e0,e3)
      r1 &lt;- phi(r0,r3)
      if (e1 &gt; 0) then body else done
body:
      r3 &lt;- r1 * b0
      e3 &lt;- e1 - 1
      goto loop
done:
      return r1


The new form on the right is, of course, no longer in SSA form. Therefore one cannot apply any SSA-based optimization. Conversion out of SSA should therefore be one of the last steps before code emission. 

At this point register allocation, possibly with register coalescing, can do a good job of eliminating redundant moves.



</code></pre></div></div> <h2 id="chapter-10-dynamics--">**Chapter 10: Dynamics ** <a name="topic-9"></a></h2> <h5 id="denotational-semantics">Denotational Semantics</h5> <ul> <li>Each part of a program is associated with a denotation</li> </ul> <h5 id="axiomatic-semantics">Axiomatic Semantics</h5> <ul> <li>Strongly related to program logic</li> <li>Gives meaning to phrases using logical axioms <h5 id="operational-semantics">Operational Semantics</h5> </li> <li>Related to interpreters and abstract machines</li> <li>Most popular and flexible form of semantics</li> </ul> <h5 id="expressions">Expressions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>• Many different styles
- Natural semantics (or big-step semantics or evaluation dynamics)
- Structural operational semantics
- Substructural operational semantics
- Abstract machine (or small-step with continuation)

abstract machine:
Very general
Low-level and elaborate

e &gt; K
valuate expression e and pass the result to the K

Binary operations
With effects

Constant and the empty continuation, stop

Boolean, short cutting

Variable: env that maps variables to values
Never changes when evaluating expressions




</code></pre></div></div> <h5 id="executing-statements">Executing Statements</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Don't pass values to the continuation
Usually have effects on the env
</code></pre></div></div> <h5 id="function-calls">Function Calls</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What needs to happen at a function call?
• Evaluate the arguments in left-to-right order
• Save the environment of the caller to continue the execution after the function call
• Save the continuation of the caller
• Execute the body of the callee in a new environment that maps the
formal parameters to the argument values
• Pass the return value to the environment of the caller

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-11-mutable--">**Chapter 11: Mutable ** <a name="topic-10"></a></h2> <h5 id="pointers-and-arrays">Pointers and Arrays</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>• Static semantics of pointers
Extend:
types with pointer types
expressions with alloc, deref, and null ptr
use an indefinite (polymorphic) type any* for synthesis for NULL
We can compare two pointers using p==q if the have the same type


• Dynamic semantics of pointers
value of a type ptr* is an address that stores a value of type
(or a special address 0)
Allocations return fresh (unused) addresses
Dereferencing retrieves a stored value
Need heap that maps addresses to values


• Static semantics of arrays
type[]
expression has: alloc_array(type, e) | e1[e2]
destination: d[e]

• Dynamic semantics of arrays
Array Evaluation: Access

H ; S ; η ⊢ e1[e2] ▷ K
   ⟶ H ; S ; η ⊢ e1 ▷ (_[e2], K)

H ; S ; η ⊢ a ▷ (_[e2], K)
   ⟶ H ; S ; η ⊢ e2 ▷ (a[_], K)
   // Need types.

H ; S ; η ⊢ i ▷ (a[_], K)
   ⟶ H ; S ; η ⊢ H(a + i*τ|τ) ▷ K
   // a ≠ 0, 0 ≤ i &lt; length(a), a : τ[]
   // Need array sizes.

H ; S ; η ⊢ i ▷ (a[_], K)
   ⟶ exception(mem)
   // a = 0 or i &lt; 0 or i ≥ length(a)


Default Values of Array Type
We also need a default value for array types
• We will just use 0 as the default value again
• It represents an array of length 0
• We can never legally access an array element in the default array
• Warning: Arrays can be compared with equality
• Make sure that alloc_array(t,0) returns a fresh address different from 0
• If arrays have address a=0 then you should not access M[a-8]

* We cannot translate d1[e2] += e3 to d1[e2] = d1[e2] + e3
Effects of e2 and d1 would be evaluated twice

</code></pre></div></div> <h5 id="heap">Heap</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>H : (N U {next}) -&gt; Val


Memory exceptions -&gt; SIGUSR2
better for debugging

</code></pre></div></div> <h5 id="array-len">Array len</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Alternative 1: Add length at the front, array address points to the start
Alternative 2: Array address points to the first element
Simplifies address arithmetic
Allows to pass pointers to C
</code></pre></div></div> <h5 id="struct">Struct</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Decl:
struct s;

Defn:
struct s {t1 f1; ... tn fn; };


Arrays are represented with pointers (but cannot be dereferenced)
-&gt; they can be compared and stored in registers

Structs are usually also pointers but they can be dereferenced

Structs are large types that do not fit in registers
</code></pre></div></div> <h5 id="c0-restrictions">C0 restrictions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Local variables, function parameters, and return values must have
small type

Left and right-hand sides of assignments must have small type

Conditional expressions must have small type

Equality and disequality must compare expressions of small type

Expressions used as statements must have small type
</code></pre></div></div> <h5 id="struct-static-semantics-rules">Struct static semantics rules</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Field names occupy their own namespace: allowed to overlap with
variable, function, or type names (but they must be distinct from
keywords)
• The same field names can be used in different struct definitions
• In a given struct definition, all field names must be distinct
• A struct may be defined at most once

Types struct s that have not (yet) been defined may be referenced as
long as their size is irrelevant

Size is relevant for
- alloc(struct s)
- alloc_array(struct s,e)
- definitions of structs if structs are types of fields

• Struct declarations are optional
‣ An occurrence of struct s in a context where its size is irrelevant
serves as an implicit declaration of the type struct s.

Types:
Extend types with struct types

Expr:
Field access

Lval

Define:
Elab

struct s* x = alloc(struct s)
e-&gt;f = (*e).f
must be defined before access

</code></pre></div></div> <h5 id="struct-sizes">Struct sizes</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Struct sizes determined by laying out the fields from left to right
Ints/bools aligned by 4
Pointers aligned by 8
Struct aligned by most restrictive fields


- Need to pick the right instructions (movl vs movq, cmpl vs cmpq)
- Could always use 8 bytes for spilling.
- Maintain size information in IRs!
- It is a good idea to keep temp/registers of different sizes separate
- If you want moves from small to large temps then make conversion
explicit
zeroextend s^32
signextend s^32

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="chapter-12-dataflow-analysis--">**Chapter 12: Dataflow analysis ** <a name="topic-11"></a></h2> <ol> <li>Liveness analysis -&gt; interfere?</li> <li>Neededness analysis -&gt; dead code?</li> <li>reaching definitions -&gt; const/copy prop</li> </ol> <h5 id="recap-liveness">Recap liveness</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>use(l,x) =&gt; instr at line l uses x 
def(l,x) =&gt; instr at line l defines x
succ(l,l') =&gt; line l' is a successor of l
live(l,x) =&gt; temp x is live at line l

use(l,x)
----------
live(l,x)

propagate backwards
only if not defined in l' 
use(l',u), succ(l,l'), -def(l', u)
----------------------------------
live(l,u)

</code></pre></div></div> <h5 id="memory-instr-in-ir">Memory instr in IR</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>M[y] &lt;- x =&gt; store x at addr y
x &lt;- M[y] =&gt; load addr y'


we use the src and dest
l: M[y] &lt;- x
-------------
use(l,x)
use(l,y)
succ(l, l+1)


we define dst as x
l: x &lt;- M[y]
-------------
def(l,x)
use(l,y)
succ(l, l+1)
</code></pre></div></div> <h5 id="dead-code-elimination">Dead code elimination</h5> <h5 id="dead-code-operations-that-dont-influence-the-result-of-progrfunction">Dead code: operations that don’t influence the result of progr/function</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Result of progr
- mem effects
- return val
- errors
- non-terminate ? 
Can exist in either source code
or translated ASM,... 
Usually we run it multiple times 




</code></pre></div></div> <h5 id="example-remove-dead-code-in-code-factorial-using-liveness-info">Example: Remove dead code in code Factorial using liveness info</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>l1: p &lt;- 1
l2: p &lt;- p * x
l3: z &lt;- p + 1 ----------- dead
l4: x &lt;- x-1
l5: if(x &gt; 0) then l2 else l6
l6: return p

Liv anal


l1: p &lt;- 1                          x
l2: p &lt;- p * x                      p,x
l3: z &lt;- p + 1                      p,x
l4: x &lt;- x-1                        p,x   
l5: if(x &gt; 0) then l2 else l6       p
l6: return p                        p      

z is not live at l4 --&gt; can replace l3 with nop
- only if op at l3 has no effects
- like memory operations, division

but if we change that line:
because we propagate liveness of z, we can't use this rule to remove though 
z &lt;- z + 1 is dead code
l1: p &lt;- 1                          x,z
l2: p &lt;- p * x                      p,x,z
l3: z &lt;- z + 1                      p,x,z
l4: x &lt;- x-1                        p,x,z
l5: if(x &gt; 0) then l2 else l6       p,x,z
l6: return p                        p


Conclusion: Liveness does not help much in some cases for removing dead codes.
</code></pre></div></div> <h5 id="neededness-analysis">Neededness Analysis</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----Goal: remove ops that----
(a) have no effect and 
(b) don't have a "needed" result

nec(l,x) =&gt; x is necessary at l 

l: x&lt;- y effect ops z
-----------------------
nec(l,y)
nec(l,z)


l: return x
-----------------------
nec(l,x)


l: M[y] &lt;- x
-----------------------
nec(l,y)
nec(l,z)


c0 no need to worry about this:
l: x &lt;- M[y]
-----------------------
nec(l,y)


l: if (x?c) then l' else l"
---------------------------
nec(l,x)


~~~~~~~~~~ Like liveness ~~~~~~~~~~

nec(l,x)
----------
needed(l,x)


needed(l',x), -def(l,x), succ(l,l')
-------------------------------
needed(l,x)



A more complex rule: 
l: x &lt;- y
.
.
l': return x 

needed(l',x), succ(l, l'), def(x, l)
--------------------------------------
needed(l,x)



We would not accidentally end non-termination:
For infinite loop:

f:
l1: goto f
l2: ret 0

</code></pre></div></div> <h5 id="complexity-ovars--lines">Complexity O(#vars * #lines)</h5> <h5 id="look-at-this-problematic-version-again-using-neededness">Look at this problematic version again using neededness:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>z &lt;- z + 1 is dead code

First pass, we start at l6, propagate
                                    Needed
l0L z &lt;- 1                          
l1: p &lt;- 1                          x,
l2: p &lt;- p * x                      p,x (the added complex rule)
l3: z &lt;- z + 1                      p
l4: x &lt;- x-1                        p
l5: if(x &gt; 0) then l2 else l6       p
l6: return p                        p


Second pass, we start at l5, propagate
                                    Needed
l0L z &lt;- 1                          
l1: p &lt;- 1                          x,
l2: p &lt;- p * x                      p,x (the added complex rule)
l3: z &lt;- z + 1                      p,x
l4: x &lt;- x-1                        p,x
l5: if(x &gt; 0) then l2 else l6       p,x
l6: return p                        p


?? TODO: Maybe merge in one pass? 

</code></pre></div></div> <h5 id="example-for-optimization">Example for optimization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>s =&gt; elem size
a =&gt; addr of array
n =&gt; length of array

Step 1:
l1: i &lt;- 0
l2: if ( 0 &lt;0) then error else l3 -------&gt; const propagation ----&gt; const folding---&gt; jump l3 ----&gt; one more pass ---&gt; (nop)
l3: if ( 0 &gt;=n) then error else l4
l4: t &lt;- 0 * s ----------------------&gt; t &lt;- 0 --------&gt; then maybe t is not needed, so removed
l5: u &lt;- a + t ----------------------&gt; copy propagation, u &lt;- a
l6: x &lt;- M[u]

what if 
l7: i &lt;- i+1
l7: if (i&lt;n) then l2 else l9
l9: return x 

Then problem is this is not in SSA form, that i is defined twice
defs in l1 and l7 each l2
Cannot do constant propagation, so we are not sure i is 0

predicate we want to define: 

reach(l,x,l') 
complexity: O(#line * #line), much larger than O(#var * #line)







</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Compiler Design Notes - SCS]]></summary></entry><entry><title type="html">Distributed Systems Notes - Currently Upating…</title><link href="https://ychen884.github.io/blog/2024/15640/" rel="alternate" type="text/html" title="Distributed Systems Notes - Currently Upating…"/><published>2024-01-18T11:00:00+00:00</published><updated>2024-01-18T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2024/15640</id><content type="html" xml:base="https://ychen884.github.io/blog/2024/15640/"><![CDATA[<h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h3 id="paper-reviews">Paper reviews</h3> <p>I will upload my paper reviews here later.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 1: Remote Procedure Call (RPC)</a></li> <li><a href="#topic-2">Chapter 2: Caching</a></li> <li><a href="#topic-3">Chapter 3: Scalability</a></li> <li><a href="#topic-4">Chapter 4: </a></li> <li><a href="#topic-5">Chapter 5: </a></li> <li><a href="#topic-6">Chapter 6: </a></li> <li><a href="#topic-7">Chapter 7: </a></li> </ol> <h2 id="-chapter-1-remote-procedure-call-rpc--">** Chapter 1: Remote Procedure Call (RPC) ** <a name="topic-1"></a></h2> <ul> <li>Try to fake procedure call to local programming</li> <li>Why? bring down programming complexity for distributed systems</li> <li>client-server model, per interface</li> <li>two aspects: control flow, invocation syntax</li> <li>with network delays (theoretically best at speed of light)</li> </ul> <h5 id="limitations-of-rpc">limitations of RPC</h5> <ul> <li>No address space sharing between client and server, can’t sharing pointers(call by reference), can’t share global data..</li> <li>Delayed binding in RPC</li> </ul> <h5 id="failure-independence">Failure independence</h5> <ul> <li>caller and callee live and die together in local setup</li> <li>we can witness failure case but hard to do in local</li> <li>failure handling consider visibility of failure</li> <li>Security: different domains</li> </ul> <h5 id="typical-rpc">Typical RPC</h5> <ul> <li>client: makerpc(request_packet, &amp;reply_packet): blocks until reply or failure</li> <li>server: getrequest(&amp;request_packet) blocks until receives request, sendresponse(reply_packet)</li> </ul> <h5 id="stub-routines">Stub routines</h5> <ul> <li>generated by stub generator</li> <li>sit between high level purpose and low level network packing/unpacking send/recv..</li> </ul> <h5 id="procedure">procedure:</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
--: network communication

        Client                                             Server
App -&gt; Stub -&gt; Transport --Network Com--- Transport -&gt; Stub -&gt; App

App calls Stub
Stub pack/unpack
Transport transmit/receive
Server App do actual work then return
Packing and unpacking is usually not elastic in dev env
Correctness and API design is more important

</code></pre></div></div> <ul> <li>Marshalling/Unmarshalling, Serialization/De-serialization</li> </ul> <h5 id="rpc-packet-format">RPC packet format</h5> <ul> <li>Network transport header(Ethernet, IP, Transport(TCP/UDP))</li> <li>RPC header <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RPC Version ID
Opcode (Stub)
Flags
parameters + Len
</code></pre></div> </div> </li> </ul> <h5 id="stub-with-dynamically-allocated-buffer">Stub with dynamically allocated buffer</h5> <ul> <li>variable to indicate how many bytes in coming</li> <li>malloc for new memories</li> </ul> <h5 id="failure-independence-of-clients--servers-adds-complexity">Failure independence of clients &amp; servers adds complexity</h5> <h6 id="outsourcing-for-example-relying-on-tcp-tcp-guarantees-reliable-in-order-unlimited-delivery">Outsourcing (for example, relying on TCP). TCP guarantees reliable, in-order, unlimited delivery.</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pain:
- no preservation of write() boundaries
- data is re-framed in transit
- read may return fewer than number of bytes requested

</code></pre></div></div> <ol> <li>The buck has to stop somewhere. Do it yourself. <ul> <li>Retransmission</li> <li>duplicate delivery/execution violates RPC semantics, sequence numbering to eliminate</li> <li>based on UDP</li> </ul> </li> </ol> <h5 id="timeout-values-in-distributed-systems">Timeout values in distributed systems</h5> <ul> <li>statistics -&gt; reasonable</li> <li>no matter what, could be too soon</li> </ul> <h5 id="what-does-server-do-when-receiving-duplication-happens">What does server do when receiving duplication happens?</h5> <ul> <li>indicates one of these happening: <ol> <li>reply lost</li> <li>reply crossed retransmitted request</li> <li>compute time was excessive</li> <li>client too impatient.</li> <li>…</li> </ol> </li> </ul> <h5 id="knowlege-at-server-is-always-stale-relative-to-client-and-vice-versa">Knowlege at server is always stale relative to client, and vice versa</h5> <ul> <li>processing time/network transmit time, indifferent to client..</li> <li>best for server to do is retrans</li> <li>should preserve reply, not re-compute, because computation can be substantial</li> <li>*my question: what if replies too large? maybe best effort</li> </ul> <h5 id="exactly-once-semantics">Exactly-once semantics</h5> <ul> <li>How long to keep old replies and sequence numbers</li> <li>Rigorous interpretation of “RPC” -&gt; forever</li> <li>Server crashes:</li> <li> <ul> <li>saved in non-volatile</li> </ul> </li> <li> <ul> <li>server response has to be after non-volatile write</li> </ul> </li> <li> <ul> <li>disk/flash latency per RPC</li> </ul> </li> <li> <ul> <li>clean undo of partial computations before crash</li> </ul> </li> <li>exactly-once RPC:</li> <li> <ul> <li>success return -&gt; call exected exactly once</li> </ul> </li> <li> <ul> <li>call blocks indefinitely, no failure return</li> </ul> </li> </ul> <h5 id="in-practice-for-rpc-package-at-most-once-semantics">In practice for RPC package: At-most-once semantics</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Avoid indefinite blocking
Declare timeout beyond certain delay: success or not? 
many timeout reasons...
</code></pre></div></div> <h5 id="slow-servers--long-running-calls">Slow Servers &amp; Long-Running Calls</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Solution: 
probes to check server health during long calls
server responds busy while working
essentially a keepalive mechanism
</code></pre></div></div> <h5 id="orphaned-computations">Orphaned Computations</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>network failure, then server continues, unaware its work is useless
Orphan detection and extermination are difficult - but important
</code></pre></div></div> <h5 id="at-least-once-semantics-strongest">At-least-once semantics (strongest)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Even simpler to implement
Requires operation idempotency
Idempotency is a property of certain operations or API requests, which guarantees that performing the operation multiple times will yield the same result as if it was executed only once.
- for example, read request on locked object or read-only object, current_stock_price(MSFT)
</code></pre></div></div> <h6 id="choice-of-semantics-less-strong">Choice of semantics (less strong)</h6> <ul> <li>Achieving exactly-once semantics ``` not provided in any real RPC package requrie application-level dup elim built on top of at-most-once RPC</li> <li>have to write on disk before replying ```</li> <li>At-most-once (mostly provided by RPC packages) ``` avoids:</li> <li>transactional storage</li> <li>non-volatile storageo of replies and sequence #s</li> <li>indefinite storage of replies</li> </ul> <p>if crashed, just crash without executing anything exactly once: have to undo, and do again using instruction in non-vol</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
##### Safety and liveness properties
</code></pre></div></div> <p>safety: correct functionalty (“At most one entity can execute in a critical section”)</p> <ul> <li>bad things never happen</li> </ul> <p>liveness: characterizes timely execution progress (e.g., “This code is deadlock-free”)</p> <ul> <li>good things will eventually happen ```</li> <li>“Exactly-once semantics” -&gt; safety property</li> <li>Existence of timeout in at-most-once RPC -&gt; liveness property</li> </ul> <h5 id="placement-of-functionality---what-are-you-promising-vs-deliver">Placement of Functionality - what are you promising vs deliver</h5> <ul> <li>Protocol layering</li> </ul> <h5 id="tcp-for-rpc">TCP for RPC</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TCP timeout -&gt; reconnect
- new connection is unaware of old
- server need to do dup elim
- orphans still possible
- exactly-once RPC no easier with TCP

TCO simplifies at-most-once RPC
TCP hurts since it has independent acks in each direction


User TCP rather UDP:
- not simplify exactly-once imple
- worse performance in best case
- simplify at-most once impl

End-to-end argument:
For a given functionality:
- correctness is expressed relative to two endpoints (safety)
- implementation requires support of those two end points
- support below end points cannot suffice (may improve performance(liveness))

</code></pre></div></div> <h5 id="critical-question-where-to-place-function-in-a-distributed-system">Critical question: Where to place function in a distributed system?</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>What guarantee promising
end point 
package
simplified implementation
guarantee properties
</code></pre></div></div> <h5 id="performance-of-distributed-system-delay">Performance of distributed system: delay</h5> <ol> <li>Processing delay</li> <li>Queueing delay &lt;- dominate</li> <li>Tranmission &lt;- usuallly small unless very large files</li> </ol> <h5 id="queueing">Queueing</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Any shared resources
Arrival pattern: uniform, poisson, ...

Service time also varies

---- without considering real world complications:----
1. Util
2. Latency
3. Freedom (how constrained of arrival discipline..) 
We can optimize at most 2 out of 3




</code></pre></div></div> <h5 id="latency-is-the-killer">Latency is the killer</h5> <h2 id="chapter-2-caching-">Chapter 2: Caching <a name="topic-2"></a></h2> <h5 id="metrics">Metrics</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Miss Ratio = Misses/References
Hit Ratio = Hits/References = (1 − Miss Ratio)
Expected cost of a reference = (Miss Ratio * cost of miss) + (Hit Ratio * cost of hit)
Cache Advantage = (Cost of Miss / Cost of Hit)
</code></pre></div></div> <h5 id="fopcus-distributed-file-systems">Fopcus: distributed file systems</h5> <h6 id="fetch-policy">Fetch policy</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Full Replication?
-Storage for entire subtree consumed on every replica
-Significant update traffic on hot spots
-Machines receive updates whether they care or not
Coarse-grain, non-selective management of data 

A Much Better Approach:
on-demand caching 
- requires operating system modifications
+ total application transparency
+ enable demand caching

Multi-OS On-Demand Caching 


</code></pre></div></div> <ol> <li>Update propagation policy</li> <li>Cache replacement policy ((prefetching))</li> </ol> <h5 id="caching-in-the-real-world">Caching in the Real World</h5> <ol> <li>Cost of remote data access often not uniform</li> <li>spatial locality</li> <li>Remote data more coarsely addressable than local Fetch more than you need on miss</li> </ol> <h5 id="spatial--temporal-locality">Spatial &amp; Temporal Locality</h5> <h5 id="update-propagation">Update Propagation</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>One-copy semantics
• there are no externally observable functional differences
• relative to same system without caching (or replication)

This model aims to provide the illusion that although data may be replicated across multiple servers or nodes (to improve reliability, performance, and fault tolerance), users interact with the data as if there is only one copy.

Challenges:
Physical master copy may not exist
Network may break between some users and master copy
Intense read- and write-sharing across sites
(The benefits of caching (reducing access time, decreasing bandwidth usage) are undermined in this scenario. The caches might spend more time synchronizing than serving the actual read and write requests, making them "effectively useless")
</code></pre></div></div> <h5 id="cache-consistency-strategies">Cache Consistency Strategies</h5> <h6 id="broadcast-invalidations">Broadcast Invalidations</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Notification to All Caching Sites:every caching site in the network is notified, regardless of whether they actually have the cached object.

Handling at Cache Sites:
Upon receiving the invalidation notice, if a cache has the invalidated object, it will mark it as invalid.

Strengths
- Strict One-Copy Semantics:
- Race Condition Prevention:
If the updating process is blocked until all caches have invalidated the item, it prevents race conditions, ensuring that no stale reads occur.
- Simplicity



Limitations
- Wasted Traffic:
- Blocking Updating Process:
- Scalability Issues:
As the number of nodes in the system increases, the overhead of sending invalidations to every node and the corresponding acknowledgments becomes impractical. 
</code></pre></div></div> <h6 id="check-on-use">Check on Use:</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Reader checks master copy before each use
Has to be done at coarse granularity (e.g. entire file or large block)
Whole file granularity-&gt; “session semantics”

Advantages
• strict consistency at coarse granularity
• easy to implement, no server state
• servers don’t need to know of caching sites

"session semantics at open-close granularity," where changes made to a file during a session (from open to close) are not visible to other clients until the session ends (the file is closed).
principled weakening of strict one-copy semantics


Strict One-Copy Semantics With Write-Sharing
any write operation performed by any client or process in a distributed system is immediately visible to all other clients or processes. 
</code></pre></div></div> <h5 id="sharing-taxonomy">Sharing Taxonomy</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Multiple concurrent read-only sessions 
 “read sharing”

Multiple concurrent read-only sessions + one read-write session
 “read-write sharing”

Multiple concurrent read-write sessions “write-write sharing”
1. “Last Close After Write Wins”: AFS
2. “Raise Conflict Exception”: Coda File System
3. “Live Happily and Be Blissfully Unaware”:  NFS, Google Docs, DropBox, etc.
None of these approaches prevents concurrent updates on server


3: How to do?


“Last Close After Write Wins”
Mechanism: This approach resolves write-write conflicts by accepting the changes made by the process that closes the file last. All previous writes to the file during the conflict period are overwritten by the last writer's data.
Pros: Simplifies conflict resolution by enforcing a clear rule.
Cons: Can lead to data loss for earlier writes, as changes made by all but the last writer are discarded.
Use Case: AFS uses this method, prioritizing simplicity and predictability over the preservation of every change


“Raise Conflict Exception”
Mechanism: When a write-write conflict is detected, the system raises an exception, alerting the involved parties (applications or users) of the conflict.
Pros: Prevents data loss by not automatically overwriting any data. It requires intervention to resolve the conflict, which can ensure that important data isn't inadvertently lost or overwritten.
Cons: Requires additional mechanisms for conflict resolution and may interrupt the user workflow.
Use Case: The Coda File System employs this strategy to maintain high data integrity, especially in environments where data consistency is critical.


“Live Happily and Be Blissfully Unaware”
Mechanism: This approach allows concurrent writes to proceed without immediate conflict resolution. Systems employing this strategy might merge changes automatically, keep all versions of the file, or simply ignore the conflict entirely.
Pros: Enhances user experience by avoiding interruptions. Systems like Google Docs merge changes in real-time, allowing seamless collaboration.
Cons: Can lead to inconsistencies or unexpected results if automatic merging is not possible or if changes are incompatible.
Use Case: NFS (Network File System) traditionally does not handle write-write conflicts explicitly at the file system level. Google Docs and Dropbox provide user-friendly collaboration features, allowing multiple users to edit documents simultaneously, with changes reflected in real-time or through version history.

</code></pre></div></div> <h5 id="check-on-open-diadv">Check-on-open diadv</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>slows read access on loaded servers &amp; high-latency networks
check is almost always success: frivolous traffic
load on network and server
</code></pre></div></div> <h5 id="callback">Callback</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>targeted notification of caching sites
on update, all sites with cached data notified (“callback”)

Advantages:
        excellent scalability for Unix workloads (often involve a mix of read and write operations on files)
        zero network traffic for read of cached-valid objects 
        precursor to caching for disconnected operation
        biases read performance in favor of write-performance

Disadvantages:
        sizable state on server
        complexity of tracking cached state on clients
        NAT networks with masquerading firewalls
        
        Clients may not be able to distinguish between a network failure that prevents 
        callbacks from reaching them and a situation where no data changes have occurred


        Periodic “Keepalive” Probes: To mitigate some of the issues with lost callbacks and ambiguous silence, systems might implement periodic "keepalive" probes. These probes allow clients to verify their connection to the server and the validity of their cached data. However, this approach introduces additional network traffic and can only partially address the problem, as data could still become stale between probes.


</code></pre></div></div> <h5 id="prevents-concurrent-updates-on-server">Prevents concurrent updates on server</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Method 4:
Caching site obtains finite-duration control from master copy
few seconds
multiple sites can obtain read lease; only one can get write lease

lease duration = 0: Check on Use
lease duration = ∞: (Targeted Notification)

Advantages
• generalizes the check on use and callback schemes
• lease duration can be tuned to adapt to mutation rate
Lease duration is a clean tuning knob for design flexibility
For data that changes frequently, shorter leases ensure that caches are updated or invalidated promptly, maintaining data consistency. For more stable data, longer leases can reduce the overhead of lease renewals and data checks, improving system efficiency. 
• conceptually simple yet flexible

Key Assumption
Clocks tick at the same rate everywhere
• clocks do NOT have to be synchronized
• absolute time does not matter
• only relative time (i.e., clock tick rate) matters
Time becomes a hidden communication channel



Disadvantages
• lease-holder has total autonomy during lease; revocation?
• writers delayed while read lease holders complete their leases
• more traffic than callback (but less than check on use)
keepalives for callback only one per server, not per lease


</code></pre></div></div> <h5 id="what-to-do-when-there-is-intense-write-sharing">What To Do When There is Intense Write-Sharing?</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>5. Skip Scary Parts
• When write-sharing detected, turn off caching everywhere
All references go directly master copy
• Resume caching when write-sharing ends

Original Use
• Sprite (circa 1987) (in conjunction with check on use)

Advantages
• Precise single-copy semantics (even at byte-level consistency)
• Excellent fallback position
• Good adaptation of caching aggressiveness to workload
Disadvantages
• Server maintains state
• Server aware of every use of data (open)

</code></pre></div></div> <h5 id="even-weaker-safety-property">Even weaker safety property?</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
6. Faith-Based Caching
Basic Idea
• blindly assume cached data is valid for a while
• periodically check (based on time since last check)
• no communication needed during trust period
Original use
• Sun NFSv3 file system
cached file blocks assumed current for X seconds
X = 3 for files, 30 for directories
• small variant is a TTL field for each object
used in web caching, gives content creator modicum of control

Imprecise and weak approximation to one-copy semantics
• not just in the presence of network failures (partitions)
• no clear basis for reasoning about state of system
Methods 1-5 used controlled and precise approximations
• e.g., session semantics at open-close granularity
• offered clear basis for reasoning

Advantages
• Simple implementation
• Server is stateless

Disadvantages
• User-visible inconsistencies sometimes seen (make)
• Blind faith sometimes misplaced!
• Not as efficient as callback-based schemes



7. Pass the Buck
Basic Idea
• Let the user trigger cache revalidation (hit “reload”)
• otherwise, all cached copies assumed valid forever
• Equivalent to infinite-TTL faith-based caching
• Arose in the context of the Web

Advantages
• trivial to implement, no server changes
• avoids frivolous cache maintenance traffic

Disadvantages
• places burden on user
user may be clueless about level of consistency needed
• assumes existence of user
• pain for write scripts/programs



</code></pre></div></div> <h5 id="what-old-data-do-you-throw-out-to-free-up-space">What old data do you throw out to free up space?</h5> <h6 id="cache-replacement-policy">Cache replacement policy</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Ideal victim: large object that is not needed for a long time, and cheap to refetch

Non-Serviceable Miss: The pain of evicting an object that cannot be easily refetched due to failures or disconnected operation is also a vital factor. This aspect is particularly relevant in environments with unreliable connectivity or when dealing with unique or hard-to-replace data.

Uniform vs. Non-Uniform Fetch Cost: While the assumption of uniform fetch cost might hold near the hardware level (e.g., when all data resides on the same disk or in the same data center), it becomes less tenable at higher system layers. At these layers, factors such as variable object sizes, the physical characteristics of storage media (like rotational and seek delays in disks), and differing network qualities to different servers introduce non-uniform fetch costs.

Abstract Problem Formulation: 
The problem can be abstracted to managing a set of 
equal-sized data containers (frames) and 
a large set of equal-sized, equal-importance data objects (pages), 
with access patterns represented by a sequence of integers (reference string). 
The primary metric of interest in this model is the miss ratio.

</code></pre></div></div> <h5 id="predict-distance-to-next-ref">Predict distance to next ref</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>optimal replacement algorithm
predict is hard
• LRU is often a good approximation to OPT (not always)
assumes recent past is a good predictor of the near future

</code></pre></div></div> <h5 id="lru">LRU</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loop (good for LRU)
- while (some test)
Sequential scan (bad for LRU)
- memset (start address, 0, many bytes)


Stack Property: “Adding cache space never hurts”
LRU has this property
Other cache replacement algorithms may not
FIFO exhibits “Belady’s anomaly


When is LRU Ineffective?
• purely sequential access (aka “scan”)
caching cannot help at all; only adds overhead

• purely random access
ratio of cache size to total data size is all that matters

Examples:
sequential scan of large files &amp; databases in data mining

video/audio playback (“streaming data”)

hash-based data structures cause accesses to be “spread out”


Example: small tight loop accessing huge array sequentially
• code shows high locality, but data does not
• the sequential data access will “pollute” an LRU-based cache
• even code (which shows locality) will be flushed out of the cache
</code></pre></div></div> <h5 id="working-set">Working Set</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Given a time interval T,
WorkingSet(T) is the set of distinct data objects accessed in T
This captures adequacy of cache size relative to program behavior
• small working set, small cache is enough,high locality
• large working set, poor locality
size and pages in working set may change over time
</code></pre></div></div> <h5 id="queueing-theory">queueing theory</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>It improves utilization by reducing the load on critical resources, 
thereby preventing bottlenecks and allowing for more efficient resource usage. 
Caching also directly reduces latency
it provides greater flexibility in handling different arrival disciplines, 
ensuring the system can cope with varying patterns of demand.

utilization vs peak

</code></pre></div></div> <h5 id="the-curse-of-uncacheability">The Curse of Uncacheability</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Businesses want to know your every click
Client caching hides this knowledge from the server
hurts response time, network load &amp; server load


Can businesses benefit from caching without giving up control?

Content Distribution Networks
A Weak Solution
Effectively third-party caching sites trusted by businesses
Late binding of parasitic content by caching site
Pioneered by Akamai in the late 1990s
• many examples now
• e.g., CloudFront (Google), Windows Azure CDN, Streamzilla

Not as effective as client caching
But better than no caching at all, and better monetizes cached resources
</code></pre></div></div> <h2 id="-chapter-3-scalabilty--">** Chapter 3: Scalabilty ** <a name="topic-3"></a></h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scalability:

1. Load Scalability
Scale with more users.
How? Can't as simple as just buy more infras..
VIRTUALIZATION
- VMs
- SDN
- SDS

+: 
(1): elasticity
(2): Monitoring, fault tolerance to local 
(3): Multiple OSes
(4): Isolation, Sandboxing



VM: perfect software abstraction of
OS-visible hardware

Software Abstraction
• Behaves just like hardware
• Allows multiple OSes

Properties of VMs:

Isolation
- Fault isolation, performance isolation, software isolation

Encapsulation and portability
- Cleanly capture all VM state, Enables VM snapshots, clones
- Independent of physical hardware
- Enables migration of live, running VMs

Interposition
- Transformations on instructions, memory, I/O
- Enables encryption, compression, …


VMM (hypersivor):
• transparant between OS and hardware
• multiplex hardware among multiple VMs

+ Fidelity: provides an environment for programs which is essentially identical with the original machine

+ Performance: programs run in this environment show at worst only
minor decreases in speed

+ Safety and isolation: VMM is in complete control of system resources






Virtualization can transform CAPEX into OPEX
capital expenses -&gt; operational expenses
Elasticity: check from advanced cloud computing topic 


</code></pre></div></div> <h5 id="types-of-system-virtualization">Types of System Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Type 1: Native/Bare Metal Hypervisors
Type 1 hypervisors run directly on the host's hardware to control the hardware and to manage guest operating systems. This type of hypervisor is also known as a "bare metal" hypervisor because it does not need an underlying operating system to function. The hypervisor acts as the operating system itself and has direct access to physical resources, which contributes to higher performance and efficiency.

Advantages:
+ Higher Performance: Direct access to physical hardware without going through an additional operating system layer allows for better performance.
+ Isolation: Provides strong isolation between virtual machines, improving security.

Examples:
VMWare ESX/ESXi
KVM (Kernel-based Virtual Machine)
Xen
Microsoft Hyper-V (when installed as a stand-alone hypervisor)


Type 2: Hosted Hypervisors
Type 2 hypervisors run on a conventional operating system just like other computer programs. This type of hypervisor is also known as a "hosted" hypervisor because it is hosted by an operating system. The hypervisor creates and runs virtual machines (VMs) that are one level removed from the physical hardware, relying on the host operating system to manage calls to the CPU, memory, and other hardware resources.

Advantages:
+ Ease of Use: Generally easier to install and use compared to Type 1 hypervisors. Suitable for development, testing, and educational purposes.
+ Cost-effective: Often less expensive and can be more suited for smaller scale or personal use.

Disadvantages:
- Higher Latency: The additional layer (the host OS) can introduce latency and reduce performance compared to Type 1 hypervisors.
- Dependence on Host OS: Relies on the host operating system's drivers and kernel, which can affect stability and performance.

Examples:
VMware Workstation
Oracle VirtualBox
Parallels Desktop
</code></pre></div></div> <h5 id="cpu-virtualization">CPU Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Privileged instructions (e.g., IO requests, Update CPU state, Manipulate page table)


Non-privileged instructions (e.g., Load from mem)


For OS:
Privileged instructions from user mode: System calls Trap to OS and executed from kernel mode
Non-privileged instructions: Run directly from user mode

For virtualization:
Privileged instructions from user mode: Trap to VMM
Non-privileged instructions: Run directly on native CPU
User Mode: Privileged Ins -&gt; Trap -&gt; Kernel Mode -&gt; VMM(emulate, update to VCPU) -&gt; return result to user mode
Trap and Emulate → Full Control for VMM




</code></pre></div></div> <h5 id="memory-virtualization">Memory Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OS assumes that it has full control over memory
- Management: Assumes it owns it all
- Mapping: Assumes it can map any Virtual→ Physical 

However, VMM partitions memory among VMs
- VMM needs to assign hardware pages to VMs
- VMM needs to control mapping for isolation
-&gt; Cannot allow OS to map any Virtual ⇒ hardware page


Logical pages (process address space in a VM)
-&gt; physical pages of process (abstraction of hardware memory, guest OS)
-&gt; machine pages (physically, Managed by VMM)
</code></pre></div></div> <h5 id="live-migration-of-vms">Live Migration of VMs</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Running guest VMs can be moved between systems, without interrupting user access to the apps
- Supported by type 1 and type 2 hypervisors
- Very useful for
-- resource management/efficiency
-- no downtime for upgrades/maintenance, etc.
- Key enabler: Encapsulation


A migrate to B

A: running guest src
A-&gt;B: establish
B: Create guest target
A: send R/O pages
A: send R/W pages
A: send dirty pages(repeat)
B-&gt;A: run guest target
A: terminate guest source


</code></pre></div></div> <h5 id="containers-vs-virtualization">Containers vs Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Containers provide isolation not virtualization
+ less overhead (operations, boost..)
+ high density (large number per machine)
+ no CPU support
-: No encap, interposition, cannot migrate, state leak to OS


• Multiple isolated instances of programs
• Running in user-space (shared OS/kernel); no VMM
• Instances see only resources (files, devices) assigned to their container
</code></pre></div></div> <h5 id="scaling">Scaling?</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scale Up:
no app change, no new failure nodes, latency concern..
more expensive
hits limit

Scale Out
application change
complex failure modes, latency
more economical

What can be scaled? 
- physical machine
- VM
- container
- process

• all threads in a process share same virtual address space
• all processes in a VM share same local file system
• all physical machines in cluster share a distributed file system


When to Scale Out?
- !response time gets longer in nonlinear way

Latency comes from
- Queueing time 
average arrival rate
average service rate

avg server load (=fraction of time server is busy)
load = avg arrival rate / avg service rate


why non linear? why latency if load &lt; 1
- avg!


If arrival rate  &gt; service rate, the latency goes to infinity

</code></pre></div></div> <h5 id="we-do-not-want">We do not want</h5> <ol> <li>Latency to spike up.</li> <li>Overprovision the system, that latency is very low with very low server load</li> </ol> <h5 id="timing-of-scaling">Timing of scaling</h5> <ol> <li>Scaling too late: response time</li> <li>Scaling too soon: overhead, underutilized resources</li> </ol> <h5 id="sweet-spot">Sweet spot?</h5> <ol> <li>Good heuristic: queue len <ul> <li>as the server queue builds up, use a threshold to trigger scale out</li> <li>when load drops, we shrink scale. (like empty queues)</li> <li>Hyteresis: gap to avoid wasterful oscillations</li> <li>*CautionL often queue len is not sufficient</li> </ul> </li> </ol> <h5 id="scale-out-front-end-servers">Scale out front end servers</h5> <ul> <li>Connection termination</li> <li>TLS termination</li> <li>Open to probing from the internet</li> <li>Handling connections</li> </ul> <h5 id="scale-out-back-end-servers-app-server">Scale out back end servers (APP server)</h5> <ul> <li>different layer design</li> <li>scale independently</li> </ul> <h5 id="what-to-do-about-db">What to do about DB</h5> <ul> <li>very difficult to sclae out</li> </ul> <h5 id="reliability">Reliability</h5> <ul> <li>Failures are likely</li> <li>if stateless, we restart</li> <li>storage layer? this is not simple as restart</li> <li>logging, replications.</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <p><a href="#blog-chapters">Back to Blog Chapters</a></p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Distributed Systems Notes - SCS]]></summary></entry><entry><title type="html">Advanced Cloud Computing Notes - Currently Upating…</title><link href="https://ychen884.github.io/blog/2024/15719/" rel="alternate" type="text/html" title="Advanced Cloud Computing Notes - Currently Upating…"/><published>2024-01-16T11:00:00+00:00</published><updated>2024-01-16T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2024/15719</id><content type="html" xml:base="https://ychen884.github.io/blog/2024/15719/"><![CDATA[<h3 id="course-evaluation-final-grade-na">Course Evaluation (Final grade: N/A)</h3> <p>I will add my course evaluation at the end of this semester.</p> <h2 id="blog-chapters"><strong>Blog Chapters</strong></h2> <ol> <li><a href="#topic-1">Chapter 1: Overview of Cloud Computing</a></li> <li><a href="#topic-2">Chapter 2: Building a Cloud</a></li> <li><a href="#topic-3">Chapter 3: Encapsulation</a></li> <li><a href="#topic-4">Chapter 4: Programming Models and Frameworks</a></li> <li><a href="#topic-5">Chapter 5: Cloud Storage</a></li> <li><a href="#topic-6">Chapter 6: Scheduling Computation</a></li> <li><a href="#topic-7">Chapter 7: </a></li> </ol> <h2 id="-chapter-1-overview-of-cloud-computing--">** Chapter 1: Overview of Cloud Computing ** <a name="topic-1"></a></h2> <h4 id="definitions">Definitions</h4> <h5 id="properties">Properties:</h5> <ul> <li>Computing utility, always available, accessible through the networks</li> <li>Simplified interface</li> <li>Statistical multiplexing, sharing resources</li> <li>Economies of scale from consolidation, costs lower</li> <li>Capital costs converted to operating costs</li> <li>Rapid and easy variation of usage</li> <li>Appearance of infinite resources with small users</li> <li>Pay only for what you use</li> <li>Cost conservation: 1 unit for 1000 hours == 1000 units for 1 hour</li> </ul> <h5 id="consolidation-sharing-elasticity">Consolidation, sharing, elasticity</h5> <ul> <li>CLT theory</li> <li>users with widely varying needs apply a considerably less variable load on a huge provider, allowing providers to do less overprovisioning.</li> <li> <ul> <li>Because of CLT, it is predictable for the overall load which causes less overprovisioning.</li> </ul> </li> <li>Users perceive exactly what they need all the time, if their needs are “small”(so the accessed resources are appearing as infinite)</li> </ul> <h5 id="saas-paas-iaas">SaaS, PaaS, IaaS</h5> <ul> <li>SaaS: service as application (Salesforce)</li> <li> <ul> <li>consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage &amp; minimum deployed applications configurations settings.</li> </ul> </li> <li>PaaS: high-level programming model for cloud computer, Turing complete but resource management hidden. (Google AppEngine)</li> <li> <ul> <li>Only App and data are controlled by user</li> </ul> </li> <li> <ul> <li>consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage but with control over the deployed applications and possibly configuration settings for the application-hosting environment.</li> </ul> </li> <li>IaaS: low-level computing model for cloud computer (AWS)</li> <li> <ul> <li>The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed application</li> </ul> </li> <li> <ul> <li>Can manage App, Data, Runtime, Middleware, OS, Cannot manage Virtualization, servers, storage, networking</li> </ul> </li> </ul> <h5 id="xxx-as-a-service">XXX as a Service</h5> <ul> <li>Data as a Service, Network as a Service, Communication as a Service(No hardware private VOIP switching), IT as a Service(IT providing services)..</li> </ul> <h5 id="deployment-models">Deployment models</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>public cloud: provider sells computing to unrelated consumers
private cloud: largely unrelated components and divisions as consumers
community cloud: providers and consumers are different organizations with strong shared concerns
Hybrid cloud: private plus public resources combined by same consumer. Better availability, overflow from private to public, load balancing to increase elasticity
</code></pre></div></div> <h5 id="larry-ellisons-objection">Larry Ellison’s objection</h5> <ul> <li>definition is including too much </li> </ul> <h5 id="obstacles-of-cloud-computing">Obstacles of cloud computing</h5> <ul> <li>Privacy &amp; security</li> <li>Privacy in the world tends to rely on regulation</li> <li>Utility issues</li> <li>Physical utilities tend to rely on regulation</li> <li>High cost of networking combined with always remote</li> <li>Performance unpredictability &amp; in situ development/debugging</li> <li>Software licensing – $/yr/CPU is not elastic and pay as you go</li> </ul> <h5 id="load-balancing-approach">load balancing approach</h5> <h5 id="1-dns-load-balancing">1. DNS load balancing</h5> <ul> <li>DNS reorder the list for each client asking for translation of name</li> <li>PRO: easy to scale, unrelated to actual TCP/HTTP requests</li> <li>CON: new server may get less resources if scheduling more servers, dynamic changing is hard because it has to tell client for a binding(existing binding exists due to TTL, caching in network middleboxes).</li> </ul> <h5 id="2-router-distribute-tcp-connections">2. Router distribute TCP connections</h5> <ul> <li>Router do the mapping: (client_ip+client_port) &lt;-&gt; (server_ip+server_port)</li> <li> <ul> <li>for SYN packets</li> </ul> </li> <li> <ul> <li>not exposed to client about the ip address(like NAT)</li> </ul> </li> <li>PRO: router doesn’t have to think or remember too much, *it just selected the server with least connnection to schedule for the new connections.</li> <li>CON: traffic all go through the router(more difficult to scale), and decision takes time cuz it’s for the entire session</li> </ul> <h4 id="3-router-distribute-individual-quests-embedded-in-connections">3. Router distribute individual quests embedded in connections</h4> <ul> <li>PRO: most dynamic</li> <li>CON: requires the most processing and state in the router, CPU load and memory goes up due to intelligent routing decisions. <h4 id="elasticit-how-elasticity-controller">Elasticit: How? Elasticity controller</h4> </li> <li>Elasticity controller to adjust load capability based on current load status</li> <li>Monitoring: resource usage, request sequence(patterns)</li> <li>Triggering: (simple conditions like thresholds), schedule, complex model based on monitored instances</li> </ul> <h4 id="elasticity-scale-out-or-scale-up">Elasticity: Scale-out or Scale up</h4> <ul> <li>Horizontal scaling: adding more instances (Common approach)</li> <li>Vertical scaling: Resizing the resources(bdw, cpu cores, memory) allocated to an existing instances, challenging(different OS.) More challenging.</li> </ul> <h4 id="two-tier-services">Two-tier services</h4> <ul> <li>web-database</li> <li>web server easy to be in cloud. At beginning, order-taking is not in cloud.</li> <li>Elasticity in IaaS: database scaling is more difficult with state(consistency)</li> <li>PassS, P=Web Service. Built-in elastic load balancing and scheduled actions for containers, persistent key-value store (datastore) &amp; non-persistent memcache for simple database tier, Users can instantiate Backends, user code can request (actuate) horizontal scaling, running traditional database services, whose scaling is still hard.</li> </ul> <h5 id="load-balancing-method-affect-how-much-statistics-we-can-get">Load-balancing method affect how much statistics we can get.</h5> <ul> <li>Router-based load balancing: firewall, intrusion detection, accelerator</li> <li>scaling middleboxes: CPU intensive tasks. (OpenFlow, split flows)</li> <li>bdw allocation by sw/rt</li> </ul> <h5 id="service-parallelization-load-balancer">Service parallelization: Load Balancer</h5> <ul> <li>aws cloud watch</li> <li>Load balancer is not necessarily elastic</li> </ul> <h5 id="scalable-relational-database">Scalable relational database</h5> <ul> <li>Separate data at rest(distributed pay-for-use storage (HDFS)) from ongoing or recent access &amp; mutation</li> <li>Recent access &amp; mutation servers are elastic (called Owning Transaction Managers)</li> <li>Partitioned but all transactions restricted to one partition: transactions block on locks and bottleneck performance scaling</li> <li>Fault-tolerance of Elastic controller. Controller itself, reliability provided by replication. Can re-assigns partitions while server is down/start up.</li> </ul> <h5 id="elastras-architecture-scales-otm-machines">ElasTraS architecture scales OTM machines</h5> <ul> <li>Transactions are limited to interacting with data from only one partition to avoid the complexity of distributed transactions.</li> <li>TODO</li> </ul> <h3 id="paper-reading-notes">Paper reading notes</h3> <h5 id="armbrust2010">Armbrust2010</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Referring to http://doi.acm.org/10.1145/1721654.1721672
Cloud computing: what brings it? large capital outlays, overprovisioning/underprovisioning...

Definition: Refers to both the applications as services over the internet and hardware and systems software in the data center that provide those services. The services themselves: SaaS. Services being sold is utility computing. Cloud computing = SaaS + utility computing. It has to be large enough to be called cloud. Hardware provisioning/pricing: 1. inifinite computing resources available on demand; 3. elimination of an up-front commitment by cloud users, add resources when needed; 4. pay for use of computing resources temporarily. *Construction and operaition of extremely large-scale, commodify-computer data centers at low-cost locations was the key necessary enabler of cloud computing. It could offer services below the costs of a medium-sized data center and make profits.

Utility Computing classes: EC2 with low level control but less automatic scalability and failover(application may need to control the replication...). Google AppEngine(domain specific platforms) on the other hand. Azure is in between. 

Economics: Favor cloud computing over conventional: 1. demand of services changes over time 2. demand is unknown
usage based pricing economically benefits the buyer. 
Elasticity helps reduce the costs. Underprovisioning has a cost that is difficult to measure: the users may never come back. Scale-up elasticity is an operational requirement, and scale-down elasticity allowed the steady state expenditure to more closely match the steady-state workload. 


Obstacles for Cloud computing:

1-3(adoption):
1. Business Continuity and Service Availability (hard to ensure availability, single failure still exists for a service provider)
2. Data Lock-In (public+private sharing by sharing API)
3. Data Confidentiality/Auditability: from other user/provider 

4-8(growth):
4. Data Transfer Bottlenecks： Applications continue to become more data-intensive
5. Performance Unpredictability： I/O interference between virtual machines, concerns scheduling of virtual machines for some classes of batch processing programs, specifically for highperformance computing
6.Scalable Storage
7. Bugs in Large Scale Distributed Systems: bugs cannot be reproduced in smaller configurations
8. Scaling Quickly

9-10(policy and business):
9. Reputation Fate Sharing, legal liability(customer responsible-&gt;unexpected down)
10. Software Licensing



Opportunities: 
- improve architectures and operating systems to efficiently virtualize interrupts and I/O channels
- flash memory will decrease I/O interference.
- offer something like “gang scheduling” for cloud computing
- create a storage system that would not only meet existing programmer expectations,but combine them with the cloud advantages of scaling arbitrarily up and down on demand. 
- reliance on virtual machines in cloud computing.(7)
- automatically scale quickly up and down in response to load in order to save money
- create reputation-guarding services similar to the “trusted email” services 
</code></pre></div></div> <h5 id="referring-to-nistdef2011">Referring to NISTdef2011</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Characteristics:
- On-demand self-service
- Broad network access
- Rapid elasticity
- Resource pooling (multi-tenant)
</code></pre></div></div> <h5 id="vaquero11">Vaquero11</h5> <h5 id="dynamically-scaling-applications-in-the-cloud">Dynamically Scaling Applications in the Cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vertical scaling is hard: rebooting..
load balancers need to support

Server scalability: a per-tier controller, a single controller for the whole application (for all tiers)
How and When to Scale: Feeding Controller with Rules and Policies
- adding load balancers
- LB scalability requires the total time taken to forward each request to the corresponding server to be negligible for small loads and should grow no faster than O(p) 
-  CPU-intensive web app: a LB to split computation among many instances.
-  network intensive: CPU-powerful standalone instance to maximize throughput
-  more network intensive applications, it may be necessary to use DNS load balancing

Scaling the network:
- Virtualization: VLAN(L2)
-  periodically measure actual network usage per application use other applications’ - increase the utilization of the network by virtually “slicing”: “network as a service”
- statistical multiplexing


Scaling the platform:
PaaS cloud platforms
- Container Scalability: execution environment for user applications.scalability of the container layer is crucial as it must efficiently manage and distribute resources to meet the demands of potentially numerous applications running concurrently.

Multitenant containers and Isolation: requires strong isolation to prevent security issues
Individual Containers per User: simplifies isolation but requires effective management of numerous containers.
Horizontal Scaling via Container Replication: achieved through automatic scaling in IaaS systems or by inherent capabilities of the platform itself
Components should ideally be stateless to cope with the dynamic nature of container instantiation and disposal, support for stateful components is also necessary, requiring sophisticated load balancing (LB) and container management to handle session data.(soft state replication, distributed caching systems)

- Database Scalability: for example, implementing horizontal scaling strategies, such as replicating the database across multiple nodes to handle increased load and ensure data availability. (demands on PaaS platforms can often surpass the capacity of any single machine)
NoSQL Databases: These databases provide high scalability and availability, fitting well in cloud environments with high demand. However, they offer eventual consistency rather than immediate consistency, leading to limitations in transaction support and SQL functionalities.
Replication Mechanisms: In-core Solutions, Middleware Solutions

Ideal Elastic Cloud: scale through VM or container replication, reconfiguration, and dynamic load balancing, possibly using DNS for the latter. allow dynamic allocation of network resources. 
Ideal PaaS Platform Features: capable of instantiating or releasing instances of user components based on demand changes and distributing the load transparently among them. Implementing session concepts and supporting transparent data replication are essential. Access to traditional relational databases with ACID transaction support is crucial. However, the system must address the increased latency due to consistency maintenance across replicas, especially under high demand.
</code></pre></div></div> <ul> <li>Infrastructure-as-Code tools provide a high-level software interface (e.g., Python / Ruby or JSON / YAML) that allows developers to specify their infrastructure requirements, software dependencies, and the process for building the infrastructure and deploying it to the cloud</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="-chapter-2-building-a-cmu-cloud--">** Chapter 2: Building a CMU Cloud ** <a name="topic-2"></a></h2> <h4 id="model">Model</h4> <h5 id="aim-for-less-costs">aim for less costs</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Client -&gt;
App
OS
Instance(s)
--------------------
Hypervisor (shared)
Building Blocks
Usage Monitor
Hardware (shared)
(shared Amazon EC2)

</code></pre></div></div> <h5 id="problem-used-mostly-during-ddl-2-methods-renting">Problem: used mostly during ddl? 2 methods, renting..</h5> <h5 id="build-a-cloud">Build a cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. user would rent, so provisioner(resources,monitoring,... -&gt; assignment of users to machines)
- Bin-packing, NP-hard. Can do with assumptions.. -&gt; less complexity
- Migration, also costs
2. Scheduler: which user jobs/processes to run: Prioritization(pay more), Oversubscription, Workload constraints
3. Encapsulation: compute, storage, networking and data
4. Virtualization
5. Fault tolerance: 
    why scale of data center matters? Economy of scale, cost of operations
    state replication, logging, storage replic

Storage services - Scalable, fault tolerant
Tools - Programming models, frameworks
Automation - Reactive systems &amp; elastic scaling

Elastic scaling - based on monitoring and diagnosis
traditional: provision for peaks

</code></pre></div></div> <ul> <li>Cloud users &amp; Services</li> <li> <ul> <li>App user: availability, performance, no interfaces exposed by cloud service provider (app provider did that)</li> </ul> </li> <li> <ul> <li>Application Deployment User: Dashboard, management interfaces exposed</li> </ul> </li> <li> <ul> <li>Admin: Management…</li> </ul> </li> <li>Orchestration: automatic deployment for user</li> </ul> <h5 id="openstack">OpenStack</h5> <ul> <li>independent parts,6 core services</li> <li>communicate through public and well-defined APIs</li> <li>Identity -&gt; Dashboard -&gt; Compute(?)/Network -&gt; Image -&gt; Object storage(get the image for VM) -&gt; Block Storage(volume) -&gt; initiate VM</li> <li>Distributed storage to accelerate image initialization takes Azure many years to fix</li> </ul> <h5 id="referring-to-sotomayor2009">Referring to sotomayor2009</h5> <h5 id="virtual-infrastructure-management-in-private--hybrid-clouds">Virtual Infrastructure Management in Private &amp; Hybrid Clouds</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Virtual Infrastructure (VI) management in the context of private/hybrid cloud environments: Uniform Resource View, Full Lifecycle Management of VMs, Configurable Resource Allocation Policies, Adaptability to Changing Resource Needs. provides primitives to schedule and manage VMs across multiple physical hosts


OpenNebula: This is a VI manager that allows organizations to deploy and manage VMs, either individually or in groups. It automates the VM setup process (including preparing disk images and setting up networking) and is compatible with various virtualization layers (Xen, KVM, VMware) and external clouds (EC2, ElasticHosts).

Haizea: This acts as a lease manager and can serve as a scheduling backend for OpenNebula. It introduces leasing capabilities not present in other cloud systems, such as advance reservations and resource preemption, which are particularly valuable for private cloud environments.



Traditional VI Management Tools: lack certain features necessary for building IaaS clouds, such as public cloud-like interfaces and the ability to deploy VMs on external clouds.
Cloud toolkits can help transform existing infrastructure into an IaaS cloud with cloudlike interfaces. VI management capabilities are not as robust.
    -: Gap: There's a noticeable gap between cloud management and VI management. Cloud toolkits attempt to cover both areas but often fall short in delivering comprehensive VI management functionalities. Integrating cloud management solutions with VI managers is complex due to the lack of open, standard interfaces and certain key features in existing VI managers.
    
    OpenNebula: overcome these challenges, Scaling to external clouds. A flexible and open architecture for easy extension and integration with other software. A variety of placement policies and support for scheduling, deploying, and configuring groups of VMs.
    
    Integration of Haizea with OpenNebula: work as a scheduler for OpenNebula, This integration allows OpenNebula to offer resource leases as a fundamental provisioning abstraction, and Haizea to operate with real hardware through OpenNebula. This combination provides advanced features like advance reservation of capacity, which is not offered by other VI managers.

    The integration is particularly beneficial for private clouds with limited resources, enabling sophisticated VM placement strategies that support queues, priorities, and advance reservations (ARs).

OpenNebula: 
core:  image and storage technologies for preparing disk images for VMs, network fabric (such as Dynamic Host Configuration Protocol [DHCP] servers, firewalls, or switches) for providing VMs with a virtual network environment, hypervisors for creating and controlling VMs. Performs operations through pluggable drivers. 
A separate scheduler component makes VM placement decisions.
Management interfaces: libvirt API intergrate within other data center management tools, cloud interface exposed to external users.
Cloud drivers to interface with external clouds

The Haizea Lease Manager: Haizea is an open source resource lease manager and can act as a VM scheduler for OpenNebula.
Advance Reservation (AR) Leases: Resources are guaranteed to be available at a specific future time. This is beneficial for scenarios requiring resource certainty.
Best-Effort Leases: Resources are provisioned as soon as possible, with requests queued if immediate provisioning isn't possible.
Immediate Leases: Resources are provided immediately upon request or not at all, suitable for urgent needs without flexibility.
    
Haizea addresses the challenge of resource underutilization, often a downside of AR, by leveraging VMs. It allows for efficient support of ARs through resource preemption - suspending lower-priority VMs to free up resources for higher-priority needs and resuming them later.
It uses optimizations such as reusing disk images across leases to minimize the impact of preparation overhead and schedules runtime overheads (like suspending, resuming, and migrating VMs) efficiently.
Scheduling is based on a resource slot table, representing all physical nodes managed by Haizea over time.
Best-effort leases are managed using a first-come-first-serve queue with backfilling, optimizing queue-based systems.
AR leases utilize a greedy algorithm for selecting physical resources to minimize preemptions.

There are ongoing efforts to extend OpenNebula's capabilities, including the implementation of the libvirt interface, VM consolidation schedulers for minimizing energy consumption, and tools for service elasticity management, VM placement, public cloud interface support, and policy-driven dynamic placement optimization.

Haizea shows that VM-based approaches with suspend/resume capabilities can address utilization issues typically associated with advance reservation (AR) use.

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h5 id="xen-and-the-art-of-virtualization">Xen and the Art of Virtualization</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Xen, a high-performance, resource-managed VMM that effectively allows multiple commodity operating systems to share hardware resources safely and efficiently

Advantages: isolation of virtual machines to prevent adverse performance effects, accommodation of diverse operating systems, and minimization of performance overhead introduced by virtualization

- targeting existing application binary interfaces (ABIs) and supporting the virtualization of complex server configurations within a single guest OS instance. full multi-application operating systems, and the necessity of paravirtualization for high performance on complex architectures like x86.

 resource virtualization: ensures both correctness and performance by allowing guest operating systems visibility of real as well as virtual resources.  better support for time-sensitive tasks and optimal use of physical resources like superpages or page coloring. 

Memory Management:
Segmentation and Paging: Guest OSes are responsible for managing hardware page tables with minimal Xen involvement for safety. Xen resides in a 64MB section at the top of every address space to avoid TLB flush when switching to and from the hypervisor. Segmentation is virtualized by validating updates to hardware segment descriptor tables.
Guest OS Restrictions: Guest OSes cannot install fully-privileged segment descriptors and must not overlap with the top end of the linear address space. They have direct read access to hardware page tables but updates are batched and validated by Xen.


CPU:
Privilege Levels: Guest OSes run at a lower privilege level than Xen to protect the hypervisor from OS misbehavior and ensure domain isolation. x86's four privilege levels are leveraged, with guest OSes typically moved from ring 0 to ring 1.
Exceptions and System Calls: Guest OSes must register descriptor tables for exception handlers with Xen. They may install a ‘fast’ handler for system calls to avoid indirecting through Xen for every call. Page faults and system calls are the most performance-critical exceptions.

Device I/O:
Virtual Devices: Instead of emulating hardware devices, Xen introduces efficient and simple device abstractions. Data transfer is done using shared-memory, asynchronous buffer-descriptor rings, allowing Xen to perform validation checks.
Event System: Hardware interrupts are replaced with a lightweight event system. Xen supports a mechanism for sending asynchronous notifications to a domain, similar to hardware interrupts, but with more control and efficiency.

Porting Operating Systems to Xen: Code Modifications in architecture-independent sections, virtual network drivers, block-device drivers, and Xen-specific (non-driver) areas. Both operating systems required substantial alterations in their architecture-specific sections. 

Role of Domain0: A domain created at boot time, termed Domain0, is responsible for hosting application-level management software. It uses the control interface to manage the system, including creating and terminating other domains and controlling their resources.

Control Transfer: Hypercalls and Events:

Hypercalls: Synchronous calls from a domain to Xen, analogous to system calls in conventional OSes. Used for operations like requesting page-table updates.
Event Mechanism: Asynchronous notifications from Xen to domains, replacing traditional interrupt delivery mechanisms. Events are used for lightweight notifications like domain-termination requests or indicating that new data has been received over the network.
Data Transfer: I/O Rings:

Efficient data transfer mechanism that allows data to move vertically through the system with minimal overhead. The design focuses on resource management and event notification.
I/O rings are circular queues of descriptors allowing asynchronous, out-of-band data transfer between Xen and guest OSes. They support a variety of device paradigms and enable efficient zero-copy transfer.

Subsystem Virtualization:
CPU Scheduling: Xen employs the Borrowed Virtual Time (BVT) scheduling algorithm for domain scheduling, ensuring low-latency wake-up and fair resource sharing.
Time and Timers: Xen provides notions of real time, virtual time, and wall-clock time to guest OSes. Domains can program alarm timers for real and virtual time, with timeouts delivered using Xen's event mechanism.

Virtual Address Translation: Xen avoids the overhead of shadow page tables by allowing guest OSes to manage hardware page tables directly, with Xen involved only in updates validation. This approach minimizes the number of hypercalls required for page table management.

Physical Memory: Memory is statically partitioned between domains, providing strong isolation. Guest OSes can adjust their memory usage dynamically, with mechanisms like the balloon driver in XenoLinux facilitating this interaction.

Network: Xen introduces the concept of a virtual firewall-router (VFR), with each domain having virtual network interfaces (VIFs) attached to the VFR. Domains transmit packets via enqueueing buffer descriptors on I/O rings, and Xen handles packet reception efficiently by exchanging packets directly with page frames.
Disk: Only Domain0 has direct access to physical disks. Other domains access persistent storage through virtual block devices (VBDs). Disk requests are batched and serviced in a round-robin fashion by Xen, ensuring fair access and good throughput.

Advantages of Delegating Domain Construction to Domain0:
Reduced Hypervisor Complexity: Building a domain within Domain0 rather than entirely within Xen simplifies the hypervisor's design, focusing its functionality on core tasks and leaving the domain setup process to a more specialized and capable component.

Improved Robustness: The process of setting up a new domain involves numerous delicate operations. Performing these operations within Domain0 allows for better error checking and handling.

Metrics:
Performance Comparison with Other Virtualization Technologies
Efficient Data Transfer and Subsystem Virtualization
Performance Isolation
Scalability
Network Performance
Concurrent Virtual Machines
Microbenchmarks 

Key points:
Efficient Paravirtualization
Resource Management and Performance Isolation
Scalability
Minimal Performance Overhead
Generality of the Interface
Facilitation of Network-Centric Services
</code></pre></div></div> <h2 id="-chapter-3-encapsulation--">** Chapter 3: Encapsulation ** <a name="topic-3"></a></h2> <h5 id="options-available-for-encap">Options available for encap</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Isolation: others can't read/write your data
- - can't impact your performance (DOS)
- Processes may need memory sharing, get resource alloc, has own address space
</code></pre></div></div> <h5 id="1-bare-metal">1. Bare metal</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+: good isolation, performance, very good software freedom
-: granularity limitation
</code></pre></div></div> <h5 id="2-process-just-application">2. Process: just application</h5> <h5 id="3-containers-with-applicaiton--libraryfsetc-that-looks-like-an-os">3. Containers: with applicaiton &amp; library/fs/etc that looks like an OS</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+: performance, decent software freedom
-: security issues
</code></pre></div></div> <h5 id="4-virtual-machines-physical-machine-like-software-container">4. Virtual Machines: physical machine-like software container</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>HW &lt;-&gt; VMM &lt;-&gt; (OS, Library/fs/etc/, App)
+: isolation properties, good software freedom
-: performance overhead, imperfect performance isolation

</code></pre></div></div> <h5 id="comparison">Comparison</h5> <ul> <li>Process, Container, VM, BM</li> <li>Lower management &lt;-&gt; Better isolation/fidelity</li> </ul> <h5 id="interface-is-the-key">interface is the key</h5> <ul> <li>between APP/Library/OS/HW</li> </ul> <h5 id="linux-namespaces">Linux namespaces</h5> <ul> <li>pid, gid, … <h5 id="resouece-alloc-linux-groups">resouece alloc: linux groups</h5> </li> </ul> <h5 id="limitation-of-containers">Limitation of containers</h5> <ul> <li>share the same host OS, security + flexibility(have to work with this OS)</li> </ul> <h5 id="virtualization">Virtualization</h5> <ul> <li>VM: vm OS &lt;–virtual machine interface–&gt; VMM &lt;–machine interface–&gt; HW</li> <li>The interfaces can be different</li> </ul> <h5 id="hw-virtualization-principles">HW Virtualization Principles</h5> <ul> <li>Fidelity, software operation keeps identical</li> <li>Isolation, guest cannot affect other guest/VMM</li> <li>Performance, most operations execute natively</li> </ul> <h4 id="ecap-principle">Ecap Principle</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Basic: Execute VM softare in de-privileged mode
- - prevent privileged instr from escaping containment
Privilege Level
App User/Ring 3
Lib User/Ring 3
OS  Supervisor/Ring-o
HW


Option 1
Privilege Level
App User/Ring 3
Lib User/Ring 3
OS  User/Ring-3
VMM Supervisor/Ring-0
HW
Overhead - Isolation, VMM has to deal with operations from OS.. not ideal

Option 2
Privilege Level
App User/Ring 3
Lib User/Ring 3
OS  Ring-1
VMM Supervisor/Ring-0
HW

----&gt; later
Option 3
Privilege Level
App User/Ring 3*
Lib User/Ring 3*
OS  Ring-0* (don't have all power, cannot change fundamental page tables..)
VMM Supervisor/Ring-0
HW
</code></pre></div></div> <h5 id="priv-ops">Priv Ops</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Most executions run directly
- VNM needs to handle OS attemps that execute priv ops, Non-CPU devices..
- - popf, pushf...

https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/VMware_paravirtualization.pdf

Handle: 
Trap &amp; emulate: VMM handles
Static software re-writing/paravirtualization: rewrite guest OS to leverage VMM hypercalls (performance, sacrificing transparancy)
Dynamic software re-writing: VMM re-write guest's privileged code - coalescing traps, VMM complex, good performance


Referring to stackoverflow.com/questions/21462581
Paravirtualization is virtualization in which the guest operating system (the one being virtualized) is aware that it is a guest and accordingly has drivers that, instead of issuing hardware commands, simply issue commands directly to the host operating system. This also includes memory and thread management as well, which usually require unavailable privileged instructions in the processor.

Full Virtualization is virtualization in which the guest operating system is unaware that it is in a virtualized environment, and therefore hardware is virtualized by the host operating system so that the guest can issue commands to what it thinks is actual hardware, but really are just simulated hardware devices created by the host.

Hardware Assisted Virtualization is a type of Full Virtualization where the microprocessor architecture has special instructions to aid the virtualization of hardware. These instructions might allow a virtual context to be setup so that the guest can execute privileged instructions directly on the processor without affecting the host. Such a feature set is often called a Hypervisor. If said instructions do not exist, Full Virtualization is still possible, however it must be done via software techniques such as Dynamic Recompilation where the host recompiles on the fly privileged instructions in the guest to be able to run in a non-privileged way on the host.

There is also a combination of Para Virtualization and Full Virtualization called Hybrid Virtualization where parts of the guest operating system use paravirtualization for certain hardware drivers, and the host uses full virtualization for other features. This often produces superior performance on the guest without the need for the guest to be completely paravirtualized. An example of this: The guest uses full virtualization for privileged instructions in the kernel but paravirtualization for IO requests using a special driver in the guest. This way the guest operating system does not need to be fully paravirtualized, since this is sometimes not available, but can still enjoy some paravirtualized features by implementing special drivers for the guest.

</code></pre></div></div> <h5 id="handling-memory-process">Handling memory: process</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- OS: page mapping from virtual page to physical page
- User process: typically continuous address space


Mutiple OS:
Guest OSes manage: Mapping guest virtual to guest physical
VMM manages guest physical to host physical

other split: cores, time sharing..

SW: Shadow page tables
HW: Extended page tables(EPT)
</code></pre></div></div> <h5 id="virtualization-devices">Virtualization Devices</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Could be all virtualized (trap accesses, and emulate), but worse performance

For performance
1. Mapping, control given to a host (hw support)
2. Partition, (disk..)
3. Guest enhancement (special VM -&gt; VMM calls)
4. Virtualized-enhanced devices (NICs with VMDq)

</code></pre></div></div> <h5 id="security-issues-of-virtualizationcontainerization">Security issues of virtualization/containerization</h5> <ul> <li>meltdown</li> <li>bitflip(row hammer)</li> </ul> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h5 id="web-servers">Web servers</h5> <ul> <li>Online retail stores</li> <li>parallel programming: contention, failures, consistency(shared resources), load balancing,..</li> </ul> <h5 id="hpc">HPC</h5> <ul> <li>scaling for N processes (lower costs, faster computation)</li> <li>strong/weak: same process finishes faster on N processors?</li> <li>Bulk Synchronous Processing..</li> <li>MPI, resource allocators, schedulers, ft</li> <li>Very manual, deep learning curve, few commercial runaway successes <h5 id="grid-computing">Grid Computing</h5> </li> <li>easier to use</li> <li>emphasized geographical sharing</li> <li>jobs selected from batch queue, take over cluster</li> <li>workload diversity <h5 id="cloud-computing">Cloud Computing</h5> </li> <li>low initial cost</li> <li>workload diversity +</li> <li>may cost efficiency for easier programming</li> </ul> <h5 id="motivation-for-cc-programming-framework">Motivation for CC programming framework</h5> <ul> <li>using many commodity* machines</li> <li>tolerate failures</li> <li>locality(reduce cost of communication), parallelism</li> </ul> <h5 id="batch-processing-of-large-datasets-on-a-cluster-framework-offers">Batch processing of large datasets on a cluster: framework offers..</h5> <ul> <li>…</li> <li>Job orchestration</li> <li>Data staging and movement</li> </ul> <h5 id="mapreduce">MapReduce</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- read large data set
- ind process the input data in chunks
- shuffle and sort data (by key)
- reduce(merge)
- locality optimization for racks
- shuffle begins after first round of map (master responsible for scheduling this)

Split
RR(record reader) -&gt;(K,V) Map -&gt; (K', V') -&gt; 
Partitioner: hash(key) mod R -&gt; reducer
Sort (may need to wait for reduce finish)
Reuce

</code></pre></div></div> <h5 id="map-data-size-vs-shuffle-data-size">Map data size vs shuffle data size</h5> <ul> <li>ngram(small-&gt;large)</li> </ul> <h5 id="spark">Spark</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Less disk operations for iterative apps
RDD as memory storage(immutable), rerun instead of storing in persistent storage
transformations create new ones, can be lazy
action create value, recomputed when action comes

</code></pre></div></div> <h2 id="-chapter-4-programming-models-and-frameworks--">** Chapter 4: Programming Models and Frameworks ** <a name="topic-4"></a></h2> <h5 id="map-reduce">Map Reduce</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Core Concepts of MapReduce:

The MapReduce model involves two primary functions: the map function and the reduce function.
The map function takes key/value pairs as input and produces a set of intermediate key/value pairs.
The reduce function then merges all intermediate values associated with the same intermediate key.

Automatic Parallelization and Distribution:
MapReduce abstracts the complexity of parallelization, fault-tolerance, data distribution, and load balancing

Scalability and Ease of Use:
The MapReduce implementation can process vast amounts of data (many terabytes) across thousands of machines.
It has a user-friendly nature, evidenced by the extensive use within Google, with hundreds of MapReduce programs implemented and thousands of jobs executed daily.

Motivation and Design Philosophy:
The motivation behind MapReduce stemmed from the need to process large sets of raw data (like web documents, request logs) into computed forms (like inverted indices, summaries) efficiently.
MapReduce was designed to simplify the computations by abstracting the complex code required for parallelization, data distribution, and fault tolerance into a library.

Programming Model:
The computation in MapReduce takes input key/value pairs and produces output key/value pairs, expressed through Map and Reduce functions defined by the user.
It handles the grouping of intermediate values by keys and provides these to the Reduce function, which then merges the values per key.

Examples and Applications:
The abstract and subsequent sections provide practical examples, like counting the number of occurrences of each word in a document collection, showcasing the model's applicability to real-world tasks.
It also highlights the versatility of MapReduce in tasks such as distributed grep, counting URL access frequency, reversing web-link graphs, and computing term-vectors per host.
Inverted Index: A fundamental operation in document searching where the map function processes documents, emitting word and document ID pairs, and the reduce function groups these by word, creating a list of document IDs for each word.
Distributed Sort: A sorting operation where the map function extracts keys from records and the reduce function sorts these keys, relying on the system's partitioning and ordering capabilities.

Implementation and Execution Overview:
designed for large clusters of commodity PCs, considering the typical hardware specifications and the nature of the network and storage systems.
The execution process involves dividing the input data into splits, assigning map and reduce tasks to workers, processing the data through the user-defined map and reduce functions, handling intermediate data storage and retrieval, and finally producing the output after all tasks are completed.

Fault Tolerance:
Fault tolerance is a critical aspect, given the scale of operation and the likelihood of machine failures. The system handles worker failures by reassigning tasks and redoing work if necessary. The master node monitors worker status and orchestrates the reassignment of tasks as needed.
The system is designed to ensure that, despite failures, the output is consistent with what would be produced by a faultless, sequential execution, as long as the map and reduce functions are deterministic.

Master Data Structures:
The master node maintains data structures to keep track of the status of each task and the locations of intermediate data. This information is crucial for coordinating the work of map and reduce workers and for ensuring that data is correctly routed through the system.


Locality
Network Bandwidth Optimization:  importance of 
conserving network bandwidth, a relatively scarce resource in large computing environments.
Data Locality: The approach involves scheduling map tasks on machines that contain a replica of the input data, or at least are network-local to the data. This strategy significantly reduces network bandwidth usage as most data is read locally.

Granularity of Map and Reduce Phases: The map phase is subdivided into M pieces, and the reduce phase into R pieces. Ideally, M and R should be much larger than the number of worker machines.
Dynamic Load Balancing and Recovery: This setup improves dynamic load balancing and expedites recovery from worker failures, as the tasks a worker has completed can be redistributed across other workers.
Handling Stragglers: machines that take unusually long to complete tasks. Causes for stragglers include hardware issues, resource contention, or bugs.

Custom Partitioning: While a default partitioning function based on hashing is provided, users have the option to specify a custom partitioning function, allowing more control over how data is partitioned across reduce tasks.

Intermediate Key/Value Pair Ordering: The framework ensures that within a partition, intermediate key/value pairs are processed in increasing key order. This ordering guarantee is particularly beneficial when the output needs to be sorted or efficiently accessible by key.

Data Aggregation at Mapper Nodes: The Combiner function allows for partial merging of intermediate data on the mapper nodes before sending it over the network. This feature is especially useful when there's significant repetition in the intermediate keys, and the reduce function is commutative and associative.

Flexible Data Formats: The MapReduce library supports various formats for input data, providing flexibility to handle different types of data sources.
</code></pre></div></div> <h5 id="spark-1">Spark</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Spark, a new cluster computing framework designed to address the limitations of MapReduce and its variants in handling certain types of applications, particularly those involving iterative operations and interactive data analysis. 

- Limitations of MapReduce and Similar Systems:
they are primarily built around an acyclic data flow model. This model is not suitable for applications that need to reuse a working set of data across multiple parallel operations.

Iterative Jobs: Common in machine learning, these jobs require applying a function repeatedly to the same dataset. MapReduce's need to reload data from disk for each iteration causes significant performance penalties.

Interactive Analytics: Users often run ad-hoc queries on large datasets using SQL interfaces. With MapReduce, each query incurs high latency because it operates as a separate job and needs to read data from disk.

Spark aims to support applications with working sets while retaining the scalability and fault tolerance characteristics of MapReduce.
The key abstraction in Spark is the resilient distributed dataset (RDD), a read-only collection of objects partitioned across a set of machines. RDDs can be explicitly cached in memory and reused in multiple parallel operations, significantly improving performance for certain types of applications.


Fault Tolerance through Lineage:
RDDs are fault-tolerant, using a concept called lineage. If a partition of an RDD is lost, the system has sufficient information on how the RDD was derived from other RDDs to rebuild just the lost partition.

Implementation and Usability of Spark:
Spark is implemented in Scala, allowing for a high-level, statically typed, and functional programming interface. It can also be used interactively, which is a novel feature for a system of this kind, enabling users to process large datasets on a cluster interactively.

Spark can significantly outperform Hadoop in iterative machine learning workloads and provide interactive querying capabilities with sub-second latency for large datasets.



- Programming Model
Spark introduces two primary abstractions for parallel programming: resilient distributed datasets (RDDs) and parallel operations on these datasets, and shared variables.

RDDs are read-only collections of objects partitioned across machines, capable of being rebuilt if a partition is lost.
RDDs can be created directly from a file in a shared filesystem like HDFS, by parallelizing a collection in the driver program, or by transforming an existing RDD.
RDDs are lazy and ephemeral by default, meaning they are computed on demand and not stored persistently after use.
However, users can modify this behavior using two actions:
The cache action suggests keeping the dataset in memory after its initial computation for future reuse.
The save action evaluates and persists the dataset to a distributed filesystem, like HDFS.
This approach to persistence is a design choice in Spark to ensure continued operation under memory constraints or node failures, drawing a parallel to the concept of virtual memory.

Parallel Operations
Operations on RDDs:
Spark supports various parallel operations on RDDs, such as reduce (combining elements using a function), collect (sending all elements to the driver program), and foreach (applying a function to each element for its side effects).

Shared Variables
Handling Variables in Parallel Operations:
When parallel operations like map and filter are performed, the closures (functions) used can refer to variables in their creation scope. By default, these variables are copied to each worker node.

Types of Shared Variables:
Spark introduces two types of shared variables for common usage patterns:
Broadcast Variables: Used for distributing large, read-only pieces of data efficiently across workers.
Accumulators: These are "add-only" variables for workers and readable only by the driver, suitable for implementing counters and parallel sums.


-Text Search
The first example is a simple text search to count the number of lines containing "ERROR" in a large log file.
The process involves creating an RDD from the file, filtering lines containing "ERROR", mapping each line to 1, and then reducing by summing these ones.
This example illustrates Spark's lazy evaluation and in-memory data sharing capabilities, which allows for efficient data processing without materializing intermediate datasets.


- Logistic Regression
This program demonstrates an iterative machine learning algorithm, logistic regression, which benefits significantly from Spark's ability to cache data in memory across iterations.
The program reads points from a file, caches them, and then iteratively updates a vector w using a gradient computed in parallel across the points.
The use of accumulators for summing the gradient and the syntax of Spark make the code resemble an imperative serial program while being executed in parallel.

- Alternating Least Squares (ALS)

Implementation
The section details Spark's implementation, including its reliance on Mesos for cluster management, the structure of RDDs, task scheduling for parallel operations, handling of shared variables, and integration with the Scala interpreter.

Resilient Distributed Datasets (RDDs): RDDs are implemented as a chain of objects capturing their lineage, allowing efficient recomputation in case of node failures. Different types of RDDs (e.g., for files or transformed datasets) implement a standard interface for partitioning, iteration, and task scheduling.
Parallel Operations and Task Scheduling: Spark creates tasks for each RDD partition and tries to schedule them based on data locality. It uses a technique called delay scheduling for efficiency.
Handling of Shared Variables: Broadcast variables and accumulators are implemented with custom serialization formats to ensure efficient distribution and fault tolerance.
Interpreter Integration: Spark integrates with the Scala interpreter, allowing interactive processing of large datasets. Modifications were made to ensure that closures and state are correctly serialized and distributed to worker nodes.

5 Results
Logistic Regression Performance: Spark significantly outperforms Hadoop in iterative machine learning workloads, with up to 10x faster performance due to data caching.
Alternating Least Squares (ALS) Performance: The use of broadcast variables for distributing the ratings matrix results in substantial performance improvements in the ALS job.
Interactive Spark Usage: Spark enables interactive querying of a large dataset with sub-second response times after initial data loading, providing a much faster and more interactive experience than Hadoop.


</code></pre></div></div> <h5 id="mr-ml-model-training">MR ML model training</h5> <ul> <li>it may scale great, but overhead is high: ML training just not well suited for stateless, deterministic functions <h5 id="spark-better">Spark better</h5> </li> </ul> <h5 id="parameter-servers">Parameter Servers</h5> <ol> <li>Shared Memory for Parameters</li> <li>Atomic Operations on Parameters</li> <li>Efficient Communication via RPCs</li> <li>Avoid Repartitioning Input Data</li> </ol> <h6 id="sync">Sync?</h6> <ol> <li>MR Approach: Requires strict synchronization (barrier synchronization) at the end of each stage, ensuring consistency but potentially leading to idle time as all nodes must wait for the slowest one.</li> <li>Parameter Server Approach: Offers flexibility. Synchronous mode ensures consistency but might slow down the training due to waiting times. Asynchronous mode can significantly speed up training as it allows nodes to compute and update independently, but it introduces some level of noise in the updates, which the training process must tolerate.</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter Servers

Sequential (BSP - Bulk Synchronous Parallel):

Traditional distributed computing, like in Spark, utilizes synchronous communication. Each iteration requires all tasks to be completed before moving to the next.
Advantages: Broad applicability and high convergence quality per iteration.
Disadvantages: Each iteration waits for the slowest task, leading to longer overall task computation times.
Bounded Delay (SSP - Staleness Synchronous Parallel):

Sets a maximum delay time, known as the staleness value, allowing a certain degree of inconsistency in task progress. When staleness = 0, it equates to the Sequential consistency model; when staleness = ∞, it becomes the Eventual consistency model.
Advantages: Reduces waiting time between tasks to a certain extent, offering faster computation speed and allowing developers to balance between algorithm convergence rate and system performance.
Disadvantages: The convergence quality per iteration may not be as high as BSP, potentially requiring more iterations to achieve similar convergence; not as broadly applicable as BSP, with some algorithms not suitable.
Eventual (ASP - Asynchronous Parallel):

Asynchronous communication, where tasks do not need to wait for each other. Tasks that complete first can proceed to the next iteration immediately.
Advantages: No waiting for other tasks, leading to fast computation speed.
Disadvantages: Poor applicability, potential decrease in convergence rate, or even non-convergence.
The synchronization restrictions of the three consistency models progressively relax to pursue faster computation speeds. In practice, the consistency model and related parameters should be adjusted based on the changes in metrics to balance convergence and computational speed.

User-defined Filters:

The system supports user-defined filters to filter out certain entries, thus reducing network bandwidth. Common filters include the significantly modified filter, which only pushes entries that have changed beyond a certain threshold, and the KKT filter, which uses conditions from optimization problems to filter out entries that have minimal impact on weights.

Vector Clock:
The parameter server uses a range vector clock to record the timestamp of each node's parameters. 
This helps in tracking the state of data and avoiding the resending of data. Since parameters are pushed and pulled in ranges, parameters within the same key range can share a timestamp. 
This approach compresses the traditional vector clock, reducing memory and network bandwidth overhead.

Messages:
Messages sent between nodes consist of a range vector clock and (key, value) pairs.
Message Compression:
Due to frequent updates of model parameters, the parameter server employs two methods to compress messages to reduce network bandwidth overhead:

Key Compression:
Since training data typically doesn't change during iterations, it's unnecessary for workers to send the same key lists each time. 
The server can cache the key lists upon the first reception. Subsequently, only the hash values of the key lists need to be sent for matching.

Value Compression:
Some parameter updates are not significant for final optimization, so users can define filter rules to discard unnecessary parameters. 
For instance, a large number of values being 0 or very small gradients can be inefficient in gradient descent and can be filtered out.

Consistency and Replication:
The parameter server uses consistent hashing to map keys and servers onto a ring according to a certain hash algorithm. 
Each server manages the key range from its insertion point counter-clockwise to another server. The server closest in the clockwise direction to a key on the ring is known as the primary server for that key range. 
Each server also backs up key ranges counter-clockwise, and these servers are known as backup servers for that key range. 
A physical server is often represented as multiple virtual servers to improve load balancing and fault tolerance.

Chain Replication vs. Replication after Aggregation:
Chain Replication:
As shown in the left diagram, worker 1 updates x, server 1 processes the data with a custom function f(x), and then backs up f(x) to server 2. 
The push is completed only after worker 1 receives an acknowledgment. This backup method can cause significant network bandwidth overhead for algorithms requiring frequent parameter updates.

Replication after Aggregation:
As shown in the right diagram, the server aggregates updates from all workers before backing up and then sends acknowledgments to workers. 
Waiting for aggregation can introduce latency in pulling updates, but this can be mitigated by relaxing the consistency model.

Server Management:
Adding a Server:
The server manager assigns a key range to the new server, and other servers adjust their key ranges accordingly.
The new server acquires the key range it will maintain as the primary server and the key ranges it will back up as a backup server.
The server manager broadcasts the changes to nodes.
Removing a Server:
When the server manager detects a server failure through heartbeat signals, it assigns the key range of the failed server to a new server and removes the failed server.
Worker Management:
Adding a Worker:
The task scheduler assigns data to the new worker.
The new worker loads training data and then fetches parameters from the server.
The task scheduler broadcasts node changes, which may cause other workers to release some training data.
Removing a Worker:
Losing a small portion of training data typically doesn't affect the training results. Moreover, restoring a worker requires more overhead than restoring a server. 
Therefore, removing a worker is usually done by simply disregarding the node. This can be used to terminate the slowest worker, mitigating the performance impact of stragglers. 
However, users can also choose to replace the lost worker with a new one.




</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="-chapter-5-cloud-storage--">** Chapter 5: Cloud Storage ** <a name="topic-1"></a></h2> <h5 id="types-of-cs">Types of cs</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------
blob
arbitrary-sized “files” with simplified interface
limited interface and semantics
Example: AWS S3
Minimal promises re: sharing/concurrency, interrupted writes
- a big server
- a scalable service: aggregation of disk-equipped machines

But with additional opportunities for simplification
Object independence simplifies scaling (reduced coordination)
Disallowing (or discouraging) object updates simplifies code
paths, caching, use of space-saving encodings, etc.


Non-FS interface also separates from legacy/kernel code
• Standardized DFS implementations persist for a long time

---------------------------------
Block stores (virtual disks)
separate but attachable to any VM instance
Guest OS running in a VM has code for FSs on disks
So, give it a “disk” to use
Virtual disk looks to guest OS just like real disk
Most cloud infrastructures have this option
AWS Elastic Block Store (EBS), OpenStack Cinder

VDs often implemented as files
A file is a sequence of bytes
 can hold a sequence of fixed-sized blocks

Thin provisioning: Promise more, Allocate based on need
Performance interference
– Each VM may have a virtual disk

IOFlow: a Software-Defined Storage Architecture.
---------------------------------
“Local disk” as part of VM instance
exists for lifetime of instance
Key difference from #2-style block stores: visibility
– visible only to the instance it comes with
• makes sense, but can’t be attached to a different VM instance
– exists only as long as the VM instance
• can’t be attached to a different instance later
---------------------------------
“Traditional” distributed FS (DIY or aaS)

set up long-running instance(s) to be the DFS server(s)
with block stores (#2) or storage-enhanced instance(s) (#3)
running traditional DFS server software on the instance(s)
DFS services can be provided by CSP or third party
    charge for file service
CSP can implement a scalable file service and sell access

---------------------------------
Provide a “union” filesystem on each client

Make a single FS view from multiple FSs
Implemented by a layer atop the individual FSs
Each operation accesses “unioned” FSs as appropriate
    For read-only: Look in “first FS” first, then “second FS” if needed, …
    For creates/writes: put into “first” non-read-only FS
</code></pre></div></div> <h5 id="dfs-in-cloud">DFS in Cloud</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>

</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p> <h2 id="-chapter-6-scheduling-computation--">** Chapter 6: Scheduling Computation ** <a name="topic-6"></a></h2> <h5 id="cluster-resource-scheduler">Cluster resource scheduler</h5> <ul> <li>schedule between tasks and machines</li> </ul> <h5 id="start-by-assuming-each-machine-is-same-allocated--as-atomic-units">Start by assuming each machine is same, allocated as atomic units</h5> <h5 id="start-by-assuming-each-job-require-a-single-full-machine-extensive-user-effort-applieduser-willing-to-help">Start by assuming each job require a single full machine, extensive user effort applied(user willing to help)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Simplest case: machine checkout
User looks at list of available machines and pick one, checkout 
* Centralized component is critical to success (avail list)
</code></pre></div></div> <h5 id="scheduler-comes-into-play">Scheduler comes into play</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>instead of user, scheduler picks machine
can run jobs on the machine: 
send to VMM or to agent that executes on the mahine

VMM or agent tells the scheduler to free resource
Still requires a central "list"
</code></pre></div></div> <h5 id="resource-requested-comes-in-ram-mhz-or-cores-of-cpu">Resource requested comes in (RAM, MHz or cores of CPU)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scheduler picks a machine with enough resource
- need to keep track 

Assume resource request is sufficient to the need
pick machine's VMM or agent ensures allocation fractions
intererence among jobs ignored
ignore unused fractions of machine

</code></pre></div></div> <h6 id="users-resource-request-can-be-imperfect">User’s resource request can be imperfect</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Overcommit - moonitor and use
- assign more
- dealing with resources run out: kill, migrate VM, shrink allocation...
- Slack resources, time sharing
- Imagine that only 1/2 of the CPU has 


</code></pre></div></div> <h6 id="users-give-more-info-about-the-resource-request">Users give more info about the resource request</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Reservation: min amount that must be provided
- Limit: upper bound amount can used
- Share: relative importance of different jobs


</code></pre></div></div> <h6 id="machine-differs">Machine differs</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scheduler still works in largely the same way
- special features require pruning set of options considered
- Exposing or hiding features? 
</code></pre></div></div> <h6 id="heterogeneity-in-aws">Heterogeneity in AWS</h6> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>EC2 types...
</code></pre></div></div> <h5 id="changing-previous-decisions">Changing previous decisions</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scheduler has algorithm to change decisions of allocations

change is not free - comes with tradeoffs
Examples:
VM migration, shoot and restart, moving jobs between machines..
</code></pre></div></div> <h5 id="non-resource-constraints">Non-resource constraints</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>e.g., some resources are closely allocated, near with another for performance

VMware constraint examples:
- Affinity: identiifes VMs that would beneefit from being on same machine for faster commu
- Anti-affinity: when crash, two VMs are not crashed together
</code></pre></div></div> <h5 id="multi-machine-tasks">Multi-machine tasks</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Scheduler makes decision based on the whole requested resources
- some may give subset ASAP rather than waiting
- may also improve assignments after knowing the full set

-&gt; To hoard or not to hoard
- hold back resources for large requests
- large requests may wait forever

</code></pre></div></div> <h5 id="example-job-and-scheduling-in-mapreduce">Example: job and scheduling in MapReduce</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Data Center: ToR - EoR - Core

MR assumes tree structure of DC, bdw higher on the same rack than than off-rack
JobTracker -&gt; Master Node
TaskTrackers -&gt; One or many slaves, coordinate with JobTracker
Send heartbeat to jobtracker every 5 seconds, liveness, availability, progress of current tasks...
Jobtracker combines updates to produce a global view
- JobTracker adds to queue
- schedular pick up and initialize


TaskTracker
- Fixed # slots for M &amp; R tasks depending on resources
- When assigned a task, copy JAR from HDFS to local FS
- creates TaskRunner: launches child JVM, run taskm communicates progress to TaskTracker
- Then TaskTracker can tell JobTracker about what happens

Job Schedulr
- Fills Map slots before reduce slots


* Centralized job scheduler, default FIFO, Others are pluggable (Fair Scheduler, Capacity Scheduler)
* considers data-locality, failures, ...
* Reduce initiates the shuffle, Map will just store info at local disks


FIFO Default Job Scheduler:
- a job takes all cluster resources
- sch in order of submission
- only running job finishes, we go for new job
- staration with long-running jobs (small jobs are starved waiting for large...)
- No job preemption
- No evaluation of job size



</code></pre></div></div> <h5 id="fair-job-scheduler-facebook">Fair Job Scheduler (Facebook)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Each user is given a fair share of cluster capacity over time
Jobs in pools of equal size for each user
Supports preemption, kill tasks in pools running over capacity for pool not received enough

Resources are shared when a task from another user comes in
Depends on concurrent users

</code></pre></div></div> <h5 id="capacity-job-scheduler-yahoo">Capacity Job Scheduler (Yahoo!)</h5> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Defined for large clusters
- Multiple independent consumers

Job queues:
- each configured with number of slots(cap)
Designed preempt

Prioritization within Queue by default
</code></pre></div></div> <p><a href="#blog-chapters">Back to Blog Chapters</a></p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Advanced Cloud Computing Notes - SCS]]></summary></entry><entry><title type="html">Information Security - Currently Updating…</title><link href="https://ychen884.github.io/blog/2023/14741/" rel="alternate" type="text/html" title="Information Security - Currently Updating…"/><published>2023-08-29T11:00:00+00:00</published><updated>2023-08-29T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2023/14741</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/14741/"><![CDATA[<h3 id="course-evaluation-final-grade-a">Course Evaluation (Final grade: A)</h3> <p>This course is structured for those who have minimal or some previous experience in computer security. It covers a broad spectrum of topics and also delves deeply into each area. The exams present a considerable challenge, and the lab requires effort, depending on the student’s prior knowledge in security. Despite having studied information security and software security during my undergraduate studies, I found the labs in this course particularly engaging. Some topics, like cryptocurrency, privacy, and security management, were especially intriguing and somewhat new to me, adding a fresh perspective to my learning experience.</p> <h3 id="reference-book">Reference Book</h3> <h4 id="security-engineering-a-guide-to-building-dependable-systems-by-ross-j-anderson">Security Engineering: A Guide to Building Dependable Systems, by Ross J. Anderson</h4>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Info Sec - INI]]></summary></entry><entry><title type="html">Computer Systems - Currently Updating…</title><link href="https://ychen884.github.io/blog/2023/15513/" rel="alternate" type="text/html" title="Computer Systems - Currently Updating…"/><published>2023-08-29T11:00:00+00:00</published><updated>2023-08-29T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2023/15513</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/15513/"><![CDATA[<h3 id="course-evaluation-final-grade-a">Course Evaluation (Final grade: A)</h3> <p>This course is exceptional, thoroughly covering computer systems with engaging labs. The material is comprehensive yet manageable, ensuring exams and labs are not overwhelming. Highly recommended.</p>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[Computer Science - 213/513]]></summary></entry><entry><title type="html">Fundamentals of Telecommunication Networks - Currently Upating…</title><link href="https://ychen884.github.io/blog/2023/14740/" rel="alternate" type="text/html" title="Fundamentals of Telecommunication Networks - Currently Upating…"/><published>2023-08-28T11:00:00+00:00</published><updated>2023-08-28T11:00:00+00:00</updated><id>https://ychen884.github.io/blog/2023/14740</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/14740/"><![CDATA[<h3 id="course-evaluation-final-grade-a">Course Evaluation (Final grade: A)</h3> <p>This course is designed for individuals with minimal or no prior knowledge of computer networks. It delves into the intricate details of various protocols. The exams are generally easy to manage, but the labs may demand a significant amount of effort. Students should be aware that they will be sharing server resources with many others, leading to potential issues with resource contention. It also includes reviews of some state-of-the-art papers in the field.</p> <h3 id="paper-reviews">Paper reviews</h3> <p>I will upload my paper reviews here later.</p> <h3 id="reference-book">Reference Book</h3> <h4 id="computer-networking-7th-edition-james-kurose-and-keith-ross">Computer Networking 7th Edition, James Kurose, and Keith Ross</h4>]]></content><author><name></name></author><category term="Study"/><category term="CMU"/><summary type="html"><![CDATA[computer networks - INI]]></summary></entry><entry><title type="html">Link to our Game Development Project - TimeOut</title><link href="https://ychen884.github.io/blog/2023/gameDev/" rel="alternate" type="text/html" title="Link to our Game Development Project - TimeOut"/><published>2023-08-27T18:37:00+00:00</published><updated>2023-08-27T18:37:00+00:00</updated><id>https://ychen884.github.io/blog/2023/gameDev</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/gameDev/"><![CDATA[<p>Check this out: https://rod233.itch.io/timeout I worked as a member of the SE team - Graphics.</p>]]></content><author><name></name></author><category term="Study"/><category term="Dev"/><summary type="html"><![CDATA[:) An interesting game dev project.]]></summary></entry><entry><title type="html">My Courses &amp;amp; Grades in UW-Madison</title><link href="https://ychen884.github.io/blog/2023/undergradCourses/" rel="alternate" type="text/html" title="My Courses &amp;amp; Grades in UW-Madison"/><published>2023-08-20T18:37:00+00:00</published><updated>2023-08-20T18:37:00+00:00</updated><id>https://ychen884.github.io/blog/2023/undergradCourses</id><content type="html" xml:base="https://ychen884.github.io/blog/2023/undergradCourses/"><![CDATA[ <p>The University of Wisconsin-Madison offers an exceptional Computer Science track for undergraduate students, enabling them to delve into various fields within the discipline. During my undergraduate studies, I concentrated on system courses, which allowed me to gain valuable insights into this specialized area.</p> <p>Below is a breakdown of the courses I undertook at UW-Madison. The advanced courses have been highlighted at the beginning of the list for emphasis. I am profoundly grateful to my instructors, whose dedication and passion have significantly shaped my academic journey. Their guidance, coupled with my relentless hard work and consistent efforts, has truly made a difference in my education.</p> <p>The courses at UW-Madison are categorized into three distinct levels:</p> <p>Elementary (E): Typically, entry-level courses designed for freshmen and sophomores. Intermediate (I): These courses are aimed at students in their sophomore to junior years. Advanced (A): Targeted at students in their junior to senior years, these courses focus on more specialized and complex topics. The education I received at UW-Madison has not only broadened my knowledge but also laid a solid foundation for my future pursuits in the field of Computer Science.</p> <p>Above 100%: Extra credits given in that course</p> <table data-toggle="table" data-url="/assets/json/table_data.json"> <thead> <tr> <th data-field="Courses">Courses</th> <th data-field="Description">Description</th> <th data-field="Final Grade">Final Grade</th> </tr> </thead> </table> <p></p> ]]></content><author><name></name></author><category term="Study"/><category term="UW-Madison"/><summary type="html"><![CDATA[:) I enjoy all the courses I took in UW-Madison.]]></summary></entry></feed>